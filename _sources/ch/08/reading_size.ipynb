{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference: https://jupyterbook.org/interactive/hiding.html\n",
    "# Use {hide, remove}-{input, output, cell} tags to hiding content\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "import myst_nb\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('precision', 2)\n",
    "# This option stops scientific notation for pandas\n",
    "# pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def display_df(df, rows=pd.options.display.max_rows,\n",
    "               cols=pd.options.display.max_columns):\n",
    "    with pd.option_context('display.max_rows', rows,\n",
    "                           'display.max_columns', cols):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch:reading_filesize)=\n",
    "# File size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers have finite limits on computing power. You have likely\n",
    "encountered these limits firsthand if your computer has slowed down from having\n",
    "too many applications open at once. We want to make sure that we do not\n",
    "exceed the computer's limits while working with data, and we might examine a file differently, depending on its size. If we know that our data set is relatively small, then a text editor or a spreadsheet can be convenient to look at the data. On the other hand, for large datasets, a more programmatic exploration or even distributed computing tools may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations, we analyze datasets downloaded from the Internet. These\n",
    "files reside on the computer's **disk storage**. In order to use Python to\n",
    "explore and manipulate the data, we need to read the data into the computer's\n",
    "**memory**, also known as random access memory (RAM). All Python code requires\n",
    "the use of RAM, no matter how short the code is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer's RAM is typically much smaller than a computer's disk storage. For\n",
    "example, one computer model released in 2018 had 32 times more disk storage\n",
    "than RAM.  Unfortunately, this means that data files can often be much bigger\n",
    "than what is feasible to read into memory. \n",
    "Both disk storage and RAM capacity are measured in terms of **bytes**. Roughly\n",
    "speaking, each character in a text file adds one byte to the file's size. For\n",
    "example, the legends.csv file has 120 characters and takes up 120 bytes of\n",
    "disk space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, many of the datasets we work with today contain many characters. To\n",
    "succinctly describe the sizes of larger files, we use the prefixes as described\n",
    "in the following {numref}`byte-prefixes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{table} Prefixes for common filesizes.\n",
    ":name: byte-prefixes\n",
    "\n",
    "| Multiple | Notation | Number of Bytes |\n",
    "| -------- | -------- | --------------- |\n",
    "| Kibibyte | KiB      | 1024    |\n",
    "| Mebibyte | MiB      | 1024²   |\n",
    "| Gibibyte | GiB      | 1024³   |\n",
    "| Tebibyte | TiB      | 1024⁴   |\n",
    "| Pebibyte | PiB      | 1024⁵   |\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a file containing 52428800 characters takes up 52428800 bytes = 50\n",
    "mebibytes = 50 MiB on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use multiples of 1024 instead of simple multiples of 1000 for these\n",
    "prefixes?** This is a historical result of the fact that most computers\n",
    "use a binary number scheme where powers of 2 are simpler to represent. You will\n",
    "also see the typical SI prefixes used to describe size---kilobytes, megabytes,\n",
    "and gigabytes, for example. Unfortunately, these prefixes are used\n",
    "inconsistently. Sometimes a kilobyte refers to 1000 bytes; other times, a\n",
    "kilobyte refers to 1024 bytes. To avoid confusion, we will stick to kibi-,\n",
    "mebi-, and gibibytes which clearly represent multiples of 1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When is it safe to read in a file?** Many computers have much more disk\n",
    "storage than available memory. It is not uncommon to have a data file happily\n",
    "stored on a computer that will overflow the computer's memory if we attempt to\n",
    "manipulate it with a program, including Python programs. We often begin our\n",
    "data work by making sure the files we are of manageable size. To accomplish\n",
    "this, we use the tools in the `os` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File                      Size (KiB)\n",
      "data/inspections.csv      455.18\n",
      "data/co2_mm_mlo.txt       49.93\n",
      "data/violations.csv       3638.87\n",
      "data/DAWN-Data.txt        273531.10\n",
      "data/legend.csv           0.12\n",
      "data/businesses.csv       644.76\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "KiB = 1024\n",
    "\n",
    "print(\"File\".ljust(25), \"Size (KiB)\")\n",
    "for filename in glob.glob('data/*'):\n",
    "    print(filename.ljust(25), \"{:.2f}\".format((os.path.getsize(filename) / KiB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `businesses.csv` file takes up 645 KiB on disk, making it well within the\n",
    "memory capacities of most systems. Although the `violations.csv` file takes up\n",
    "3.6 MiB of disk storage, most machines can easily read it into a Pandas dataframe too. \n",
    "The DAWN-Data.txt, which contains the DAWN survey data, is much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAWN file takes up nearly 270 MiB of disk storage, and while some computers can work\n",
    "with this file in memory, it might slow down other systems. The approach we have taken\n",
    "for working with this particular dataset is to reduce the number of features in\n",
    "the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folder Sizes.** Sometimes we are interested in the total size of a folder\n",
    "instead of the size of individual files. For example, if we have one file of\n",
    "inspections for each month in a year, we might like to see whether we can\n",
    "combine all the data into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data folder is 271.80 MiB\n"
     ]
    }
   ],
   "source": [
    "MiB = 1024**2\n",
    "Folderpath = 'data'   \n",
    "size = 0\n",
    "\n",
    "for ele in os.scandir(Folderpath):\n",
    "    size+= (os.path.getsize(ele))/MiB\n",
    "\n",
    "print(\"The\", Folderpath, \"folder is\", \"{:.2f}\".format(size), \"MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Overhead.** As a rule of thumb, reading in a file using `pandas`\n",
    "usually requires at least double the available memory as the file size. That\n",
    "is, reading in a 1 GiB file will typically require at least 2 GiB of available\n",
    "memory.\n",
    "\n",
    "Note that memory is shared by all programs running on a computer, including the\n",
    "operating system, web browsers, and yes, Jupyter notebook itself. A computer\n",
    "with 4 GiB total RAM might have only 1 GiB available RAM with many applications\n",
    "running. With 1 GiB available RAM, it is unlikely that `pandas` will be able to\n",
    "read in a 1 GiB file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data are in a Pandas DataFrame, our next task is to get a handle on the table's shape and granularity. We need to understand what a row represents and the expected kind of values in a field before we can begin to check the quality of the data. However, before we address this topic, in the next section, we describe command line tools available in the Shell for carrying out these same tasks. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
