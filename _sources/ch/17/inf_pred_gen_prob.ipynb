{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference: https://jupyterbook.org/interactive/hiding.html\n",
    "# Use {hide, remove}-{input, output, cell} tags to hiding content\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:theory_probIntro)=\n",
    "# Probability for Inference and Prediction\n",
    "\n",
    "Hypothesis testing, confidence intervals, and prediction intervals rely on probability calculations computed from the sampling distribution and the data generation process. These probability frameworks also enable us to run simulation and bootstrap studies for a hypothetical survey, an experiment, or some other chance process in order to study its random behavior. For example, we found the sampling distribution for an average of ranks under the assumption that the treatment in a Wikipedia experiment was not effective. Using simulation, we quantified the typical deviations from the expected outcome and the distribution of the possible values for the summary statistic. The triptych in {numref}`Figure 1 %s <triptych>` provided a diagram to guide us in the process; it helped keep straight the differences between the population, probability, and sample and also showed their connections. In this section, we bring more mathematical rigor to these concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formally introduce the notions of expected value, standard deviation, and random variable, and we connect them to the concepts we have been using in this chapter for testing hypotheses and making confidence and prediction intervals. We begin with the specific example from the Wikipedia experiment, and then generalize. Along the way, we connect this formalism to the triptych that we have used as our guide throughout the chapter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} TriptychRank.png\n",
    "---\n",
    "name: triptychRank\n",
    "---\n",
    "\n",
    "This diagram shows the population, sampling, and sample distributions and their summaries from the Wikipedia example. In this example, the population is known to consist of the integers from 1 to 200, and the sample are the ranks of the observed post-productivity measurements for the treatment group. In the middle, the sampling distribution of the average rank is created from a simulation study. Notice it is normal in shape with a center that matches the population average. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing the theory for average rank statistics\n",
    "\n",
    "Recall in the Wikipedia experiment, we pooled the post-award productivity values from the treatment and control groups and converted them into ranks, $1, 2, 3, \\ldots, 200$ so the population is simply made up of the integers from 1 to 200. {numref}`Figure %s <triptychRank>` is a diagram that represents this specific situation. Notice that the population distribution is flat and ranges from 1 to 200 (leftside of {numref}`Figure %s <triptychRank>`). Also, the population summary (called *population parameter*) we use is the average rank:\n",
    "\n",
    "$$\n",
    "\\theta^* ~=~ Avg(pop) ~=~  \\frac{1}{200} \\Sigma_{k=1}^{200} k ~=~ 100.5.\n",
    "$$\n",
    "\n",
    "Another relevant summary is the spread about $\\theta^*$, defined as the population standard deviation:\n",
    "\n",
    "$$ \n",
    "SD(pop) ~=~ \\sqrt{\\frac {1}{200} \\Sigma_{k=1}^{200} (k - \\theta^*)^2} ~=~  \n",
    "\\sqrt{\\frac {1}{200} \\Sigma_{k=1}^{200} (k - 100.5)^2}\n",
    "~\\approx~ 57.7 \n",
    "$$\n",
    "\n",
    "The SD(pop) represents the typical deviation of a rank from the population average.   To calculate SD(pop) for this example takes some mathematical handiwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed sample consists of the integer ranks of the treatment group; we refer to these values as $k_1, k_2, \\ldots, k_{100}.$ The sample distribution appears on the right in {numref}`Figure %s <triptychRank>` (each of the 100 integers appears once). \n",
    "\n",
    "The parallel to the population average is the sample average, which is our statistic of interest:\n",
    "\n",
    "$$\n",
    "Avg(sample) ~=~  \\frac{1}{100} \\Sigma_{i=1}^{100} k_i ~=~ \\bar{k} ~=~113.7.\n",
    "$$\n",
    "\n",
    "The $Avg(sample)$ is the observed value for $\\hat{\\theta}$. Similarly, the spread about $Avg(sample)$, called the standard deviation of the sample, represents the typical deviation of a rank in the sample from the sample average:\n",
    "\n",
    "$$ \n",
    "SD(sample) ~=~ \\sqrt{\\frac {1}{100} \\Sigma_{i=1}^{100} (k_i - \\bar{k})^2} ~=~ 55.3.\n",
    "$$\n",
    "\n",
    "Notice the parallel between the definitions of the sample statistic and the population parameter, in the case where they are averages. The parallel between the two SDs is also note worthy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we turn to the data generation process: draw 100 marbles from the urn (with values $1, 2,\\ldots,200$), without replacement, to create the treatment ranks.  We represent the action of drawing the first marble from the urn and the integer that we get, by the capital letter $Z_1$. This $Z_1$ is called a *random variable*. It has a probability distribution determined by the urn model. That is, we can list all of the values that $Z_1$ might take and the probability associated with each:\n",
    "\n",
    "$$\n",
    "{\\mathbb{P}}(Z_1 = k) ~=~ \\frac{1}{200} ~~~~\\text{ for } k=1, \\ldots, 200.\n",
    "$$\n",
    "\n",
    "In this example, the probability distribution of $Z_1$ is determined by a simple formula because all of the integers are equally likely to be drawn from the urn. ({numref}`Chapter %s <ch:theory_datadesign>` first introduces the notion of a probability distribution).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often summarize the distribution of a random variable by its *expected value* and *standard deviation*. Like with the population and sample, these two quantities give us a sense of what to expect as an outcome and how far the actual value might be from what is expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, the expected value of $Z_1$ is simply, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[Z_1]\n",
    "&= 1 \\mathbb{P}(Z_1 = 1) + 2 \\mathbb{P}(Z_1 = 2) +  \\cdots + 200 \\mathbb{P}(Z_1 = 200) \\\\\n",
    "&= 1 \\times \\frac{1}{200} + 2 \\times \\frac{1}{200} + \\cdots + 200 \\times \\frac{1}{200} \\\\\n",
    "&= 100.5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that $\\mathbb{E}[Z_1] = \\theta^*$, the population average from the urn. The average value in a population and the expected value of a random variable that represents one draw at random  from an urn that contains the population are always the same. This is more easily seen by expressing the population average as a weighted average of the unique values in the population weighted by the fraction of units that have that value. The expected value of a random variable of a draw at random from the population urn uses the exact same weights because they match the chance of selecting the particular value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "The term *expected value* can be a bit confusing because it need not be a possible value of the random variable. For example, $\\mathbb{E}[Z_1] = 100.5$, but only integers are possible values for $Z_1$. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the variance of $Z_1$ is defined as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{V}(Z_1) &= \\mathbb{E}[Z_1 - \\mathbb{E}(Z_1)]^2 \\\\\n",
    "&= [1 - \\mathbb{E}(Z_1)]^2 \\mathbb{P}(Z_1 = 1) + \\cdots + [200 - \\mathbb{E}(Z_1)]^2  \\mathbb{P}(Z_1 = 200) \\\\\n",
    "&= (1 - 100.5)^2 \\times \\frac{1}{200} + \\cdots + (200 - 100.5)^2  \\times \\frac{1}{200} \\\\\n",
    "&= 3333.25\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Additionally,\n",
    "\n",
    "$$\n",
    "SD(Z_1) = \\sqrt{\\mathbb{V}(Z_1)} = 57.7\n",
    "$$ \n",
    "\n",
    "We again point out that the standard deviation of $Z_1 $ matches the $SD(pop)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe the entire data generation process in {numref}`Figure %s <triptychRank>`, we also define, $Z_2 , Z_3, \\ldots, Z_{100}$ as the result of the remaining 99 draws from the urn. By symmetry these random variables should all have the same probability distribution. That is, for any $k = 1, \\ldots, 200$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Z_1 = k) ~=~ \\mathbb{P}(Z_2 = k) ~=~ \\cdots ~=~ \\mathbb{P}(Z_{100} = k) ~=~ \\frac{1}{200}.\n",
    "$$\n",
    "\n",
    "This implies that each $Z_i$ has the same expected value, 100.5, and standard deviation, 57.7.  However, these random variables are not independent. For example, if you know that $Z_1 = 17$, then it is not possible for $Z_2 = 17$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the middle portion of {numref}`Figure %s <triptychRank>`, which involves the sampling distribution of $\\hat{\\theta}$, we express the average rank statistic as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\frac{1}{100} \\Sigma_{i=1}^{100} Z_i\n",
    "$$\n",
    "\n",
    "We can use the expected value and SD of $Z_1$ and our knowledge of the data generation process to find the expected value and SD of $\\hat{\\theta}$. However, we need some more information about how combinations of random variables behave so we first present the results and then circle back to explain why. We first find the expected value of $\\hat{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}(\\hat{\\theta}) ~&=~ \\mathbb{E}[\\frac{1}{100} \\Sigma_{i=1}^{100} Z_i]\\\\\n",
    "~&=~ \\frac{1}{100} \\Sigma_{i=1}^{100} \\mathbb{E}[Z_i] \\\\\n",
    "~&=~  100.5 \\\\\n",
    "~&=~ \\theta^*\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In other words, the expected value of the average of random draws from the population equals the population average. Below we provide formulas for the variance of the average in terms of the population variance, as well as the SD.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{V}(\\hat{\\theta}) ~&=~ \\mathbb{V}[\\frac{1}{100} \\Sigma_{i=1}^{100} Z_i]\\\\\n",
    " ~&=~ \\frac{200-100}{100-1} \\times \\frac{\\mathbb{V}(Z_i)}{100} \\\\\n",
    " ~&=~ 16.75 \\\\\n",
    " ~&~\\\\\n",
    " SD(\\hat{\\theta}) ~&=~ \\sqrt{\\frac{100}{199}} \\frac{SD(Z_1)}{10} \\\\\n",
    " ~&=~ 4.1 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These computations relied on several properties of expected value and variance of a random variable and sums of random variables. Next we describe several useful properties of sums and averages of random variables. These can be used to derive the formulas we just presented for the expected value and SD of the average from a population.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General properties of random variables\n",
    "\n",
    "In general, a *random variable* represents a numeric outcome of a probabilistic event. In this book, we use capital letters like $X$ or $Y$ or $Z$ to denote a random variable. The probability distribution for $X$ is the specification, $\\mathbb{P}(X = x) = p_x$ for all values $x$ that the random variable takes on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the expected value of $X$ is defined as:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{x} x p_x,$$\n",
    "\n",
    "the variance $X$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{V}(X) ~&=~ \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\\\\n",
    " ~&=~ \\sum_{x} [x - \\mathbb{E}(X)]^2  p_x,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and, the $SD(X)$ is the square-root of $\\mathbb{V}(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Although random variables can represent either discrete (such as, the number of children in a family drawn at random from a population) or continuous (such as, the air quality measured by an air monitor) quantities, we address only random variables with discrete outcomes in this book. Since most measurements are made to a certain degree of precision, this simplification doesn't limit us too much. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple formulas provide the expected value, variance, and standard deviation when we make scale and shift changes to random variables, such as $a + bX$ for constants $a$ and $b$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}(a + bX) ~&=~ a + b\\mathbb{E}(X)  \\\\\n",
    "\\mathbb{V}(a + bX) ~&=~ b^2\\mathbb{V}(X) \\\\\n",
    "SD(a + bX) ~&=~ |b|SD(X) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To convince yourself that these formulas make sense, think about how a distribution changes if you added a constant $a$ to each value or scaled each value by $b$. Adding $a$ to each value would simply shift the distribution, which in turn would shift the expected value but not change the size of the deviations about the expected value. On the other hand, scaling the values by, say, 2, would spread the distribution out and essentially double both the expected value and the deviations from the expected value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in the properties of the sum of two or more random variables. Let's consider two random variables, say $X$ and $Y$. Then,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(a + bX + cY) ~=~ a + b\\mathbb{E}(X) + c\\mathbb{E}(Y).\n",
    "$$\n",
    "\n",
    "\n",
    "But, to find the variance of $a + bX + cY$, we need to know how $X$ and $Y$ vary together, which is called the *joint distribution* of $X$ and $Y$. The joint distribution of $X$ and $Y$ assigns probabilities to combinations of their outcomes,\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}(X =x, Y=y) ~=~ p_{x,y} \n",
    "$$\n",
    "\n",
    "A summary of how $X$ and $Y$ vary together, called the covariance, is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Cov(X, Y) ~&=~ \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] \\\\\n",
    " ~&=~ \\mathbb{E}[(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)] \\\\\n",
    " ~&=~ \\Sigma{x,y}[(xy) - \\mathbb{E}(X)\\mathbb{E}(Y)]p_{x,y} \n",
    " \\end{aligned}\n",
    "$$\n",
    "\n",
    "The covariance enters into the calculation of $\\mathbf(a + bX + cY)$, as shown below: \n",
    "\n",
    "$$\n",
    "\\mathbb{V}(a + bX + cY) ~=~ b^2\\mathbb{V}(X) + 2bcCov(X,Y) + c^2\\mathbb{V}(Y)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In the special case where $X$ and $Y$ are independent, their joint distribution is simplified to  $p_{x,y} = p_x p_y$. And, in this case, $Cov(X,Y) = 0$ so \n",
    "\n",
    "$$\n",
    "\\mathbb{V}(a + bX + cY) ~=~ b^2\\mathbb{V}(X) + c^2\\mathbb{V}(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These properties can be used to show that for random variables, $X_1, X_2, \\ldots X_n$, that are independent with expected value $\\mu$ and standard deviation $\\sigma$, the average, $\\bar{X}$, has the following expected value, variance, and standard deviation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}(\\bar{X}) ~&=~ \\mu\\\\\n",
    "\\mathbb{V}(\\bar{X}) ~&=~ \\sigma^2 /n\\\\\n",
    "SD(\\bar{X}) ~&=~ \\sigma/\\sqrt{n}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This situation arises with the urn model where $X_1, \\ldots,X_n$ are the result of random draws with replacement. In this case, $\\mu$ represents the average of the urn and $\\sigma$ the standard deviation.\n",
    "\n",
    "\n",
    "However, when we make random draws from the urn without replacement, the $X_i$ are not independent. In this situation, $\\bar{X}$ has the following expected value and variance:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}(\\bar{X}) ~&=~ \\mu\\\\\n",
    "\\mathbb{V}(\\bar{X}) ~&=~ \\frac{N-n}{N-1} \\times \\frac{\\sigma^2}{n}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that while the expected value is the same as when the draws are without replacement, the variance and SD are smaller. These quantities are adjusted by $(N-n/(N-1)$, which is called the *finite population correction factor*.  We used this formula earlier to compute the $SD(\\hat{\\theta})$ in our Wikipedia example. \n",
    "\n",
    "Returning to {numref}`Figure %s <triptychRank>`, we see that the sampling distribution for $\\bar{X}$ in the center of the diagram has an expectation that matches the population average; the SD decreases like $1/\\sqrt{n}$ but even faster because we are drawing without replacement; and the distribution is shaped like a normal curve.  We saw these properties earlier in our simulation study.\n",
    "\n",
    "Now that we have outlined the general properties of random variables and their sums, we connect these ideas to testing, confidence, and prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability behind testing and intervals\n",
    "\n",
    "As mentioned at the beginning of this chapter, probability is the underpinning behind conducting a hypothesis test, providing a confidence interval for an estimator and a prediction interval for a future observation.  \n",
    "\n",
    "We now have the technical machinery to explain these concepts, which we have carefully defined in this chapter without the use of formal technicalities. This time we present the results in terms of random variables and their distributions. \n",
    "\n",
    "Recall that a hypothesis test relies on a null model which provides the probability distribution for the statistic, $\\hat{\\theta}$. The tests we carried out were essentially computing (sometimes approximately) the following probability: given the assumptions of the null distribution,\n",
    "\n",
    "$$ \\mathbb{P}(\\hat{\\theta} \\geq \\text{observed statistic}) $$\n",
    "\n",
    "Often times, the random variable is normalized to make these computations easier and standard:\n",
    "\n",
    "$$ \n",
    "\\mathbb{P}\\left( \\frac{\\hat{\\theta} - {\\theta}^*}{SD(\\hat{\\theta})} \\geq \\frac{\\text{observed stat}- \\theta^*}{SD(\\hat{\\theta})}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When, $SD(\\hat{\\theta})$ is not known, we have approximated it via simulation or, when we have a formula for $SD(\\hat{\\theta})$ in terms of $SD(pop)$, we substitute $SD(samp)$ in for $SD(pop)$. This normalization is popular because it simplifies the null distribution. For example, if $\\hat{\\theta}$ has an approximate normal distribution than the normalized version will have a standard normal distribution with center 0 and SD of 1. These approximations are useful if a lot of hypothesis tests are being carried out, such as with A/B testing, for there is no need to simulate for every statistic because we can just use the normal curve probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability statement behind a confidence interval is quite similar to the probability calculations used in testing. In particular, to create a 95% confidence interval where the sampling distribution of the estimator is roughly normal, we standardize and use the probability,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}\\left( \\frac{|\\hat{\\theta} - \\theta^*|}{SD(\\hat{\\theta})} \\leq 1.96 \\right) &~=~ \\mathbb{P}\\left(\\hat{\\theta} - 1.96SD(\\hat{\\theta}) \\leq \\theta^* \\leq \\hat{\\theta} + 1.96SD(\\hat{\\theta}) \\right) \\\\\n",
    "&~\\approx~ 0.95\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\hat{\\theta}$ is a random variable in the above probability statement and $\\theta^*$ is considered a fixed unknown parameter value. The confidence interval is created by substituting the observed statistic in for $\\hat\\theta$ and calling it a 95% confidence interval:\n",
    "\n",
    "$$\n",
    "\\left[\\text{observed stat} - 1.96SD(\\hat{\\theta}),~ \\text{observed stat} + 1.96SD(\\hat{\\theta}) \\right]\n",
    "$$\n",
    "\n",
    "Once the observed statistic is substituted in for the random variable, then we say that we are 95% confident that the interval we have created contains the true value $\\theta^*$. In other words, in 100 cases where we compute an interval in this way, we expect 95 of them to cover the population parameter that we are estimating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider prediction intervals. The basic notion is to provide an interval that denotes the expected variation of a future observation about the estimator. In the simple case, where the statistic is $\\bar{X}$ and we have a hypothetical new observation $X_0$ that has the same expected value, say $\\mu$, and standard deviation, say $\\sigma$, of the $X_i$, then we find the expected variation of the squared loss:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[(X_0 - \\bar{X})^2] ~&=~ \\mathbb{E}\\{[(X_0 - \\mu) - (\\bar{X} - \\mu)]^2\\} \\\\\n",
    " ~&=~  \\mathbb{V}(X_0) + \\mathbb{V}(\\bar{X}) \\\\\n",
    " ~&=~  \\sigma^2 + \\sigma^2/n \\\\\n",
    " ~&=~ \\sigma\\sqrt{1 + 1/n}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there are two parts to the variation: one due to the variation of $X_0$ and the other due to the approximation of $\\mathbb{E}(X_0)$ by $\\bar{X}$. \n",
    "\n",
    "In the case of more complex models, the variation in prediction also breaks down into two components: the inherent variation in the data about the model plus the variation in the sampling distribution due to the estimation of the model. Assuming the model is roughly correct,\n",
    "we can express it as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} ~=~ \\textbf{X}\\boldsymbol{\\theta}^{*} + \\boldsymbol{\\epsilon},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta}^*$ is a $(p+1) \\times 1$ column vector, $\\textbf{X}$ is a $n \\times (p+1)$ design matrix, and $\\boldsymbol{\\epsilon}$ consists of $n$ independent random variables that each has expected value 0 and variance $\\sigma^2$. In this equation, $\\mathbf{Y}$ is a \n",
    "vector of random variables, where the expected value of each variable is determined by the design matrix and the variance is $\\sigma^2$. That is, the variation about the line is constant in that it does not change with $\\mathbf{x}$.\n",
    "\n",
    "When we create prediction intervals in regression, they are given an $1 \\times (p+1)$ row vector of covariates, called $\\mathbf{x}_0$. the prediction is $\\mathbf{x}_0\\boldsymbol{\\hat{\\theta}}$, where $\\boldsymbol{\\hat{\\theta}}$ is the estimated parameter vector based on the original\n",
    "$\\mathbf{y}$ and design matrix $\\textbf{X}$. The expected squared error in this prediction is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[(Y_0 - \\mathbf{x_0} \\boldsymbol{\\hat{\\theta}})^2] ~&=~ \n",
    "\\mathbb{E}\\{[(Y_0 - \\mathbf{x_0\\boldsymbol{\\theta}^{*} }) - (\\mathbf{x_0}\\boldsymbol{\\hat{\\theta}}  - \\mathbf{x_0}\\boldsymbol{\\theta}^{*})]^2\\} \\\\\n",
    " ~&=~  \\mathbb{V}(\\epsilon) + \\mathbb{V}(\\mathbf{x_0}\\boldsymbol{\\hat{\\theta}}) \\\\\n",
    " ~&=~  \\sigma^2 [1 + \\mathbf{x}_0 (\\textbf{X}^\\top \\textbf{X})^{-1} \\mathbf{x}_0^\\top] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate the variance of $\\epsilon$ with the variance of the residuals from the least squares fit. \n",
    "\n",
    "The prediction intervals we create using the normal curve rely on the additional assumption that the distribution of the errors is approximately normal. This is a stronger assumption than we  make for the confidence intervals. With confidence intervals, the probability distribution of $X_i$ need not look normal for $\\bar{X}$ to have an approximate normal distribution. Similarly, the probability distribution of $\\boldsymbol{\\epsilon}$ in the linear model need not look normal for the estimator $\\hat{\\theta}$ to have an approximate normal distribution.  \n",
    "\n",
    "We also assume that the linear model is approximately correct when making these prediction intervals. In {numref}`Chapter %s <ch:risk>` we consider the case where the fitted model doesn't match the model that has produced the data. We now have the technical machinery to derive the model bias-variance tradeoff introduced in that chapter. It's very similar to the prediction interval derivation with a couple of small twists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Behind Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In {numref}`Chapter %s <ch:risk>` we introduced model under- and over-fitting with mean square error.\n",
    "We described a general set up where the data might be expressed as follows:\n",
    "\n",
    "$$ y = g(\\mathbf{x}) + {\\epsilon}.$$\n",
    "\n",
    "The $\\epsilon$) are assumed to behave like random errors that have no trends or patterns, constant variance, and are independent of one another. The *signal* in the model is the function, $g()$. The data are the $ (\\mathbf{x}_i, y_i) $ pairs, and we fit models by minimizing the MSE,\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\cal{F}} \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - f(\\mathbf{x}_i)^2,\n",
    "$$\n",
    "\n",
    "where $\\cal{F}$ is the collection of models over which we are minimizing. This collection might be all polynomials of degree $m$ or less, bent lines with a bend at point $k$, etc. Note that $g$ doesn't have to be in the collection of functions that we are using to fit a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in model selection is to land on a model that predicts a new observation well. For a new observation, we would like the expected loss to be small:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[ y_0 - f(\\mathbf{x}_0)]^2 \n",
    "$$\n",
    "\n",
    "This expectation is with respect to the distribution of possible $(\\mathbf{x}_0, y_0)$, and is called *risk*. Since we don't know the population distribution of $ (\\mathbf{x}_0 , y_0) $, we can't calculate the risk, but we can approximate it by the average loss over the data we have collected:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[ y_0 - f(\\mathbf{x}_0)]^2 & \\approx \n",
    " \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - f(\\mathbf{x}_i))^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This approximation goes by the name of *empirical risk*. But, hopefully you recognize it as mean square error (MSE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit models by minimizing the empirical risk (or MSE) over all possible models, $ \\cal{F} = \\{ f \\} $,\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\cal{F}} \\frac{1}{n} \\sum_{i = 1}^{n}  (y_i - f(\\mathbf{x}_i))^2 \n",
    "$$\n",
    "\n",
    "The fitted model is called $ \\hat{f} $, a slightly more general representation of the linear model $ \\textbf{X}\\hat{\\boldsymbol{\\theta}}$. This technique is aptly called *empirical risk minimization*. \n",
    "\n",
    "In {numref}`Chapter %s <ch:risk>`, we saw problems arise when we used the empirical risk to both fit a model and evaluate the risk for a new observation. Ideally, we want to estimate the risk (expected loss), \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[ (y_0 - \\hat{f}(\\mathbf{x}_0))^2 ],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the expected value is over both the new observation $ (\\mathbf{x}_0, y_0) $ and over $ {\\hat{f}} $ (which involves the original data $ \\textbf{x}_i$ and $ {y}_i $, $ i = 1, \\ldots, n $). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the problem, we decompose the risk for a new observation that is estimated by the least squares fit, $\\hat{f}$, into three parts representing the model bias, model variance, and the *irreducible error* from $ \\epsilon $.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E} [ y_0 -  \\hat{f}(x_0)]^2  &  = \\mathbb{E} [ g(x_0) + \\epsilon_0 -  \\hat{f}(x_0)]^2  & \\textrm{definition}~\\textrm{of} ~y_0\\\\\n",
    "  &  = \\mathbb{E} [ g(x_0) + \\epsilon_0 - \\mathbb{E}[\\hat{f}(x_0)] + \\mathbb{E}[\\hat{f}(x_0)] -  \\hat{f}(x_0)]^2 & \\textrm{adding}~ \\pm \\mathbb{E}[\\hat{f}(x_0)] \\\\\n",
    " &  = \\mathbb{E} [ g(x_0) - \\mathbb{E}[\\hat{f}(x_0)] -  (\\hat{f}(x_0) - \\mathbb{E}[\\hat{f}(x_0)]) + \\epsilon_0]^2  & \\textrm{rearranging}~\\textrm{terms}\\\\\n",
    " &  = [ g(x_0) - \\mathbb{E}[\\hat{f}(x_0)]]^2 + \\mathbb{E}[\\hat{f}(x_0) - \\mathbb{E}[\\hat{f}(x_0)]]^2 + \\sigma^2 \\\\\n",
    "% & =  ~~~\\textrm{model bias}^2 ~~~~~~~+~~~~ \\textrm{model variance}^2 ~+~ \\textrm{error}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the last equality from the previous line, we need to formally prove that the cross product terms in the expansion of the quadratic are all 0. This takes a bit of algebra and we don't present it here. But the main idea is that the terms $\\epsilon_0$ and $(\\hat{f}(x_0) - \\mathbb{E}[\\hat{f}(x_0])$ are independent and have expected value 0. \n",
    "\n",
    "+ The first of the three terms in the final equation is model bias (squared). When, the signal, $ g $, does not belong to the model space, we have model bias. If the model space can approximate $g$ well, then the bias is small. Note that this term is not present in our prediction intervals because we assumed that there is no (or minimal) model bias.  \n",
    "\n",
    "+ The second term represents the variability in the fitted model that comes from the data. We have seen in earlier examples that high degree polynomials can over fit, and so vary a lot from one set of data to the next. The more complex the model space, the greater the variability in the fitted model. \n",
    "\n",
    "+ Finally, the last term is the variability in the error, the $\\epsilon_0$, which is dubbed the \"irreducible error\". This error sticks around whether we have under fit with a simple model (high bias) or over fit with a complex model (high variance).\n",
    "\n",
    "This representation of the expected loss shows the bias-variance decomposition of a fitted model. Model selection aims to balance these two competing sources of error. The train-test split, cross-validation, and regularization introduced in {numref}`Chapter %s <ch:risk>` are techniques to either mimic the expected loss for a new observation or penalize a model from over fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have covered a lot of theory in this chapter, we have attempted to tie it to the basics of the urn model and the three distributions: population, sample, and sampling. We wrap up the chapter with a few cautions to keep in mind when performing hypothesis tests and when making confidence or prediction intervals. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
