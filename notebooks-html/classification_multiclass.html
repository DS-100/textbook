
<div id="ipython-notebook">
    <div class="buttons">
        <button class="interact-button js-nbinteract-widget">
            Show Widgets
        </button>
        <a class="interact-button" href="http://data100.datahub.berkeley.edu/user-redirect/git-pull?repo=https://github.com/DS-100/textbook&subPath=notebooks/ch17/classification_multiclass.ipynb">Open on DataHub</a></div>
    




<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="c1"># Clear previously defined variables</span>
<span class="o">%</span><span class="k">reset</span> -f

<span class="c1"># Set directory for data loading to work properly</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/notebooks/ch17&#39;</span><span class="p">))</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multiclass-Classification-(outline)">Multiclass Classification (outline)<a class="anchor-link" href="#Multiclass-Classification-(outline)">&#182;</a></h2><p>Should this be a separate section or just an addendum to an existing section?</p>
<ul>
<li>What is multiclass classification<ul>
<li>More than 2 labels that can be classified; versus a binary classification which is restricted to 2 labels</li>
</ul>
</li>
<li>A situation where multiclass classification might be used<ul>
<li>Preferably come up with an example in lebron.csv, but might have to use a different data set</li>
</ul>
</li>
<li>OvO vs OvR<ul>
<li>In logistic regression, multiclass classification comes down to a bunch of binary problems</li>
<li>OvO: Fit a binary classification problem to each pair of classes, then vote for class that wins the most "head-to-head" classifications</li>
<li>OvR/OvA: Fit a binary classification problem for each class versus the rest of the classes; result is class that has the highest probability</li>
</ul>
</li>
<li>Example walking through multiclass? sklearn's logistic regression supports OvR out of the box.</li>
</ul>
<h2 id="Other-types-of-classification-problems">Other types of classification problems<a class="anchor-link" href="#Other-types-of-classification-problems">&#182;</a></h2><ul>
<li>Multilabel: More than one classification problem (e.g. a document can be positive/negative, religion/not religion, political/apolitical, etc.)</li>
</ul></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multiclass-Classification">Multiclass Classification<a class="anchor-link" href="#Multiclass-Classification">&#182;</a></h2><p>So far we have discussed binary classification, in which our classifier determines whether an observation is part of the positive class or the negative class. However, many data science problems involve <strong>multiclass classification</strong>, in which we would like to classify observations as one of several different classes. For example, we may be interested in knowing whether an animal is a dog, a cat, a squirrel, or a raccoon. In practice, we use <strong>one-vs-rest (OvR) classification</strong> to solve these types of problems.</p>
<h3 id="One-Vs-Rest-Classification">One-Vs-Rest Classification<a class="anchor-link" href="#One-Vs-Rest-Classification">&#182;</a></h3><p>In OvR classification (also known as one-vs-all, or OvA), we decompose a multiclass classification problem into several different binary classification problems. Returning to the example of animal identification, each observation $X_i$ in the training data would be assigned a label $y_i$ that classifies the animal as either a dog, a cat, a squirrel, or a raccoon. Then for each unique label $k$, we construct a new label vector $z$ where $z=1$ if $y=k$ and $z=0$ if $y \neq k$. With our four new label vectors, we can then create four separate classification tasks that predicts whether an observation belongs to the positive class.</p>
<p>So far in binary classification, we have used the output of these predictors - for example, we were primarily interested in whether the observation was a dog or not a dog. However, in multiclass classification we use the probabilities that each classifier outputs, then we assign the label $k$ for which the respective classifier outputs the highest probability score. Suppose the four classifiers output the probability scores shown below:</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">probas</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.63</span><span class="p">,</span> <span class="mf">0.37</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.18</span><span class="p">,</span> <span class="mf">0.82</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.44</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">]],</span> 
                      <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Dog&#39;</span><span class="p">,</span> <span class="s1">&#39;Cat&#39;</span><span class="p">,</span> <span class="s1">&#39;Squirrel&#39;</span><span class="p">,</span> <span class="s1">&#39;Raccoon&#39;</span><span class="p">],</span>
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;P(y=k)&#39;</span><span class="p">,</span> <span class="s1">&#39;P(y!=k)&#39;</span><span class="p">])</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">probas</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">




<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>P(y=k)</th>
      <th>P(y!=k)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Dog</th>
      <td>0.75</td>
      <td>0.25</td>
    </tr>
    <tr>
      <th>Cat</th>
      <td>0.63</td>
      <td>0.37</td>
    </tr>
    <tr>
      <th>Squirrel</th>
      <td>0.18</td>
      <td>0.82</td>
    </tr>
    <tr>
      <th>Raccoon</th>
      <td>0.44</td>
      <td>0.56</td>
    </tr>
  </tbody>
</table></div></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the Dog classifier outputs the highest probability score, we predict that our observation is a dog.</p>
<h3 id="Iris-Dataset">Iris Dataset<a class="anchor-link" href="#Iris-Dataset">&#182;</a></h3></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">DESCR</span><span class="p">)</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">



<div class="output_subarea output_stream output_stdout output_text">
<pre>Iris Plants Database
====================

Notes
-----
Data Set Characteristics:
    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

This is a copy of UCI ML iris datasets.
http://archive.ics.uci.edu/ml/datasets/Iris

The famous Iris database, first used by Sir R.A Fisher

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher&#39;s paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

References
----------
   - Fisher,R.A. &#34;The use of multiple measurements in taxonomic problems&#34;
     Annual Eugenics, 7, Part II, 179-188 (1936); also in &#34;Contributions to
     Mathematical Statistics&#34; (John Wiley, NY, 1950).
   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) &#34;Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments&#34;.  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) &#34;The Reduced Nearest Neighbor Rule&#34;.  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&#34;s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...

</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multilabel-Classification">Multilabel Classification<a class="anchor-link" href="#Multilabel-Classification">&#182;</a></h2><p>Another type of classification problem is <strong>multilabel classification</strong>, in which our goal is to solve more than one classification problem for the same dataset. An example would be a document classification system: a document can be classified as having positive or negative sentiments, but it can also be distinguished between religious/nonreligious or political/apolitical. Multilabel problems can also be multiclass; we may want our document classification system to distinguish between a list of genres, or identify the language that the document is written in.</p>
<p>Because each set of labels in a multiclass classification is mutually exclusive, each problem can be solved independently of the others. We can then combine the outputs so that every observation is assigned a set of classification labels.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>(to-do)</p></div></div></div></div>
