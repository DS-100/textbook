
<div id="ipython-notebook">
    <div class="buttons">
        <button class="interact-button js-nbinteract-widget">
            Show Widgets
        </button>
        <a class="interact-button" href="http://data100.datahub.berkeley.edu/user-redirect/git-pull?repo=https://github.com/DS-100/textbook&subPath=notebooks/ch16/classification_cost_justification.ipynb">Open on DataHub</a></div>
    




<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="c1"># Clear previously defined variables</span>
<span class="o">%</span><span class="k">reset</span> -f

<span class="c1"># Set directory for data loading to work properly</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/notebooks/ch16&#39;</span><span class="p">))</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Approximating-the-Empirical-Probability-Distribution">Approximating the Empirical Probability Distribution<a class="anchor-link" href="#Approximating-the-Empirical-Probability-Distribution">&#182;</a></h2><p>In this section, we introduce <strong>KL divergence</strong> and demonstrate how minimizing average KL divergence in binary classification is equivalent to minimizing average cross-entropy cost.</p>
<p>Since logistic regression outputs probabilities, a logistic model produces a certain type of probability distribution. Specifically, it uses its parameters $ \hat \theta $ to estimate the probability that the label $ y $ is $ 1 $ for an example input $ x $.</p>
<p>For example, suppose that $ x $ is a scalar recording the forecasted chance of rain for the day and $ y = 1 $ means that Mr. Doe takes his umbrella with him to work. A logistic model predicts the probability that Mr. Doe takes his umbrella given a forecasted chance of rain: $ \hat{P_\theta}(y = 1 | x) $.</p>
<p>Collecting data on Mr. Doe's umbrella usage provides us with a method of constructing an empirical probability distribution $ P(y = 1 | x) $. For example, if there were five days where the chance of rain $ x = 0.60 $ and Mr. Doe only took his umbrella to work once, $ P(y = 1 | x = 0.60) = 0.20 $. We can compute a similar probability distribution for each value of $ x $ that appears in our data. Naturally, after fitting a logistic model we would like the distribution predicted by the model to be as close as possible to the empirical distribution from the dataset. That is, for all values of $ x $ that appear in our data, we want:</p>
<p>$$ \hat{P_\theta}(y = 1 | x) \approx P(y = 1 | x) $$</p>
<p>One commonly used metric to determine the "closeness" of two probability distributions is the Kullback–Leibler divergence, or KL divergence, which has its roots in information theory.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Defining-Average-KL-Divergence">Defining Average KL Divergence<a class="anchor-link" href="#Defining-Average-KL-Divergence">&#182;</a></h2><p>KL Divergence reveals how imprecise the probability distribution output by a logistic model is relative to the data. For example, revisiting the basketball data, suppose we want to determine the probability that LeBron makes a shot 15 ft from the basket. KL divergence quantifies the difference between the probability output by our logistic model and the actual probability based on the dataset. The KL divergence for binary classification between two distributions $P$ and $\hat{P_\theta}$ for a single data point $(x, y)$ is given by:</p>
<p>$$D(P || \hat{P_\theta}) = P(y = 0 | x) \ln \left(\frac{P(y = 0 | x)}{\hat{P_\theta}(y = 0 | x)}\right) + P(y = 1 | x) \ln \left(\frac{P(y = 1 | x)}{\hat{P_\theta}(y = 1 | x)}\right)$$</p>
<p>KL divergence is not symmetric: $$D(P || \hat{P_\theta}) \neq D(\hat{P_\theta} || P)$$ In other words, the divergence of $\hat{P_\theta}$ from $P$ is not the same as the divergence of $P$ from $\hat{P_\theta}$. Since we use $\hat{P_\theta}$ to approximate $P$, we are concerned with $ D(P || \hat{P_\theta}) $.</p>
<p>The best $\hat{\theta}$ minimizes the average KL divergence of the entire dataset of $n$ points:</p>
<p>$$\displaystyle\arg \min_{\substack{\theta}} \frac{1}{n} \sum_{i=1}^{n} \left(P(y_i = 0 | x_i) \ln \left(\frac{P(y_i = 0 | x_i)}{\hat{P_\theta}(y_i = 0 | x_i)}\right) + P(y_i = 1 | x_i) \ln \left(\frac{P(y_i = 1 | x_i)}{\hat{P_\theta}(y_i = 1 | x_i)}\right)\right)$$</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>KL divergence does not penalize mismatch for rare events with respect to $P$. If the model predicts a high probability for an event that is actually rare, then both $P(k)$ and $\ln \left(\frac{P(k)}{\hat{P}(k)}\right)$ are low so the divergence is also low. However, if the model predicts a low probability for an event that is actually common, then the divergence is high. We can deduce that a logistic model that accurately predicts common events has a lower divergence from $P$ than does a model that accurately predicts rare events but varies widely on common events.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deriving-Cross-Entropy-Cost-from-KL-Divergence">Deriving Cross-Entropy Cost from KL Divergence<a class="anchor-link" href="#Deriving-Cross-Entropy-Cost-from-KL-Divergence">&#182;</a></h2><p>The structure of the average KL divergence equation for binary classification hints back to our study of the cross-entropy cost equation. The most important takeaway of this section is that minimizing average KL divergence is equivalent to minimizing average cross-entropy cost. We will demonstrate this equivalence by deriving the cross-entropy cost from the KL divergence mathematically.</p>
<p>Using properties of logarithms, we can rewrite the weighted log ratio:
$$P(y_i = k | x_i) \ln \left(\frac{P(y_i = k | x_i)}{\hat{P_\theta}(y_i = k | x_i)}\right) = P(y_i = k | x_i) \ln P(y_i = k | x_i) - P(y_i = k | x_i) \ln \hat{P_\theta}(y_i = k | x_i)$$</p>
<p>Note that since the first term doesn't depend on $\theta$, it doesn't affect $\displaystyle\arg \min_{\substack{\theta}}$ and can be removed from the equation. The resulting equation is the cross-entropy cost of the model $\hat{P_\theta}$:</p>
<p>$$\displaystyle\arg \min_{\substack{\theta}} \frac{1}{n} \sum_{i=1}^{n} - P(y_i = 0 | x_i) \ln \hat{P_\theta}(y_i = 0 | x_i) - P(y_i = 1 | x_i) \ln \hat{P_\theta}(y_i = 1 | x_i)$$</p>
<p>Since the label $y_i$ is a known value, the probability that $y_i = 1$, $P(y_i = 1 | x_i)$, is equal to $y_i$ and $P(y_i = 0 | x_i)$ is equal to $1 - y_i$. The model's probability distribution $\hat{P_\theta}$ is given by the output of the sigmoid function discussed in the previous two sections. After making these substitutions, we arrive at the cross-entropy cost equation:</p>
<p>$$ \displaystyle\arg \min_{\substack{\theta}} \frac{1}{n} \sum_i \left(- y_i \ln (f_\hat{\theta}(x_i)) - (1 - y_i) \ln (1 - f_\hat{\theta}(x_i) \right) $$</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Statistical-justification-for-Cross-Entropy-Cost">Statistical justification for Cross-Entropy Cost<a class="anchor-link" href="#Statistical-justification-for-Cross-Entropy-Cost">&#182;</a></h2><p>The cross entropy cost also has fundamental underpinnings in statistics. Since the logistic regression model predicts probabilities, given a particular logistic model we can ask, "What is the probability that this model produced the set of observed outcomes $ y $?" We might naturally adjust the parameters of our model until the probability of drawing our dataset from the model is as high as possible. Although we will not prove it in this section, this procedure is equivalent to minimizing the cross entropy cost—this is the <em>maximum likelihood</em> statistical justification for the cross entropy cost.</p>
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>Average KL divergence can be interpreted as the average log difference between the two distributions $P$ and $\hat{P_\theta}$ weighted by $P$. Minimizing average KL divergence also minimizes average cross-entropy cost. We can reduce the divergence of logistic regression models by selecting features that accurately classify commonly occurring data.</p></div></div></div></div>
