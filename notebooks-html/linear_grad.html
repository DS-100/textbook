
<div id="ipython-notebook">
    <div class="buttons">
        <button class="interact-button js-nbinteract-widget">
            Show Widgets
        </button>
        <a class="interact-button" href="http://data100.datahub.berkeley.edu/user-redirect/git-pull?repo=https://github.com/DS-100/textbook&subPath=notebooks/ch13/linear_grad.ipynb">Open on DataHub</a></div>
    




<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="c1"># Clear previously defined variables</span>
<span class="o">%</span><span class="k">reset</span> -f

<span class="c1"># Set directory for data loading to work properly</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/notebooks/ch13&#39;</span><span class="p">))</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h1>Table of Contents<span class="tocSkip"></span></h1></p>
<div class="toc"><ul class="toc-item"><li><span><a href="#Fitting-a-Linear-Model-Using-Gradient-Descent" data-toc-modified-id="Fitting-a-Linear-Model-Using-Gradient-Descent-1">Fitting a Linear Model Using Gradient Descent</a></span></li><li><span><a href="#Derivative-of-the-MSE-cost" data-toc-modified-id="Derivative-of-the-MSE-cost-2">Derivative of the MSE cost</a></span></li><li><span><a href="#Running-Gradient-Descent" data-toc-modified-id="Running-Gradient-Descent-3">Running Gradient Descent</a></span></li></ul></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="k">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">tips</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;tips&#39;</span><span class="p">)</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="k">def</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">cost_fn</span><span class="p">,</span> <span class="n">grad_cost_fn</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Uses gradient descent to minimize cost_fn. Returns the minimizing value of</span>
<span class="sd">    theta once the cost changes less than 0.0001 between iterations.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">progress</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;theta: </span><span class="si">{theta}</span><span class="s1"> | cost: </span><span class="si">{cost}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_cost_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
        <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
        <span class="n">new_cost</span> <span class="o">=</span> <span class="n">cost_fn</span><span class="p">(</span><span class="n">new_theta</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_cost</span> <span class="o">-</span> <span class="n">cost</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.0001</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">new_theta</span>
        
        <span class="n">theta</span> <span class="o">=</span> <span class="n">new_theta</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">new_cost</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fitting-a-Linear-Model-Using-Gradient-Descent">Fitting a Linear Model Using Gradient Descent<a class="anchor-link" href="#Fitting-a-Linear-Model-Using-Gradient-Descent">&#182;</a></h2><p>We want to fit a linear model that predicts the tip amount based on the total bill of the table:</p>
$$
f_\hat{\theta} (x) = \hat{\theta_1} x + \hat{\theta_0}
$$<p>In order to find $ \hat{\theta_1} $ and $ \hat{\theta_0} $, we need to first choose a cost function. We will choose the mean squared error cost function:</p>
$$
\begin{aligned}
L(\hat{\theta}, x, y)
&amp;= \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\hat{\theta} (x_i))^2\\
\end{aligned}
$$<p>Note that we have modified our loss function to reflect the addition of an explanatory variable in our new model. Now, $ x $ is a vector containing the individual total bills, $ y $ is a vector containing the individual tip amounts, and $ \hat{\theta} $ is a vector: $ \theta = [ \hat{\theta_1}, \hat{\theta_0} ] $.</p>
<p>Using a linear model with the squared error also goes by the name of least-squares linear regression. We can use gradient descent to find the $ \theta $ that minimizes the cost.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>An Aside on Using Correlation</strong></p>
<p>If you have seen least-squares linear regression before, you may recognize that we can compute the correlation coefficient and use it to determine $ \hat{\theta_1} $ and $ \hat{\theta_0} $. This is simpler and faster to compute than using gradient descent for this specific problem, similar to how computing the mean was simpler than using gradient descent to fit a constant model. We will use gradient descent anyway because it is a general-purpose method of cost minimization that still works when we later introduce models that do not have analytic solutions. In fact, in many real-world scenarios we will use gradient descent even when an analytic solution exists because computing the analytic solution can take longer than gradient descent, especially on large datasets.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Derivative-of-the-MSE-cost">Derivative of the MSE cost<a class="anchor-link" href="#Derivative-of-the-MSE-cost">&#182;</a></h2><p>In order to use gradient descent, we have to compute the derivative of the MSE cost with respect to $ \hat{\theta} $. Now that $ \hat{\theta} $ is a vector of length 2 instead of a scalar, $ \nabla_{\hat\theta} L(\hat{\theta}, x, y) $ will also be a vector of length 2.</p>
$$
\begin{aligned}
\nabla_{\hat\theta} L(\hat{\theta}, x, y)
&amp;= \nabla_{\hat\theta} \left[ \frac{1}{n} \sum_{i = 1}^{n}(y_i - f_\hat{\theta} (x_i))^2 \right] \\
&amp;= \frac{1}{n} \sum_{i = 1}^{n}2 (y_i - f_\hat{\theta} (x_i))(- \nabla_{\hat\theta} f_\hat{\theta} (x_i))\\
&amp;= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\hat{\theta} (x_i))(\nabla_{\hat\theta} f_\hat{\theta} (x_i))\\
\end{aligned}
$$</div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We know:</p>
$$
f_\hat{\theta} (x) = \hat{\theta_1} x + \hat{\theta_0}
$$<p>We now need to compute $ \nabla_{\hat\theta} f_\hat{\theta} (x_i) $ which is a length 2 vector.</p>
$$
\begin{aligned}
\nabla_{\hat\theta} f_\hat{\theta} (x_i)
&amp;= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} f_\hat{\theta} (x_i)\\
     \frac{\partial}{\partial \theta_1} f_\hat{\theta} (x_i)
   \end{bmatrix} \\
&amp;= \begin{bmatrix}
     \frac{\partial}{\partial \theta_0} [\hat{\theta_1} x_i + \hat{\theta_0}]\\
     \frac{\partial}{\partial \theta_1} [\hat{\theta_1} x_i + \hat{\theta_0}]
   \end{bmatrix} \\
&amp;= \begin{bmatrix}
     1 \\
     x_i
   \end{bmatrix} \\
\end{aligned}
$$</div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we plug back into our formula above to get</p>
$$
\begin{aligned}
\nabla_{\hat\theta} L(\hat{\theta}, x, y)
&amp;= -\frac{2}{n} \sum_{i = 1}^{n}(y_i - f_\hat{\theta} (x_i))(\nabla_{\hat\theta} f_\hat{\theta} (x_i))\\
&amp;= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - f_\hat{\theta} (x_i)) \begin{bmatrix} 1 \\ x_i \end{bmatrix} \\
&amp;= -\frac{2}{n} \sum_{i = 1}^{n} \begin{bmatrix}
    (y_i - f_\hat{\theta} (x_i)) \\
    (y_i - f_\hat{\theta} (x_i)) x_i
    \end{bmatrix} \\
\end{aligned}
$$<p>This is a length 2 vector since $ (y_i - f_\hat{\theta} (x_i)) $ is a scalar.</p></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Running-Gradient-Descent">Running Gradient Descent<a class="anchor-link" href="#Running-Gradient-Descent">&#182;</a></h2><p>Now, let's fit a linear model on the tips dataset to predict the tip amount from the total table bill.</p>
<p>First, we define a Python function to compute the cost:</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">simple_linear_model</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Returns predictions by a linear model on x_vals.&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_vals</span>

<span class="k">def</span> <span class="nf">mse_cost</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">simple_linear_model</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, we define a function to compute the gradient of the cost:</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">grad_mse_cost</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span>
    <span class="n">grad_0</span> <span class="o">=</span> <span class="n">y_vals</span> <span class="o">-</span> <span class="n">simple_linear_model</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">)</span>
    <span class="n">grad_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">simple_linear_model</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">))</span> <span class="o">*</span> <span class="n">x_vals</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">/</span> <span class="n">n</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad_1</span><span class="p">)])</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_mse_cost</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll use the previously defined <code>minimize</code> function that runs gradient descent, accounting for our new explanatory variable. It has the function signature (body omitted):</p>
<div class="highlight"><pre><span></span><span class="n">minimize</span><span class="p">(</span><span class="n">cost_fn</span><span class="p">,</span> <span class="n">grad_cost_fn</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
</pre></div>
<p>Finally, we run gradient descent!</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">time</span>

thetas = minimize(mse_cost, grad_mse_cost, tips[&#39;total_bill&#39;], tips[&#39;tip&#39;])
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    

<div class="output_subarea output_stream output_stdout output_text">
<pre>theta: [0. 0.] | cost: 10.896283606557377
theta: [0.   0.07] | cost: 3.8937622006094705
theta: [0.  0.1] | cost: 1.9359443267168215
theta: [0.01 0.12] | cost: 1.388538448286097
theta: [0.01 0.13] | cost: 1.235459416905535
theta: [0.01 0.14] | cost: 1.1926273731479433
theta: [0.01 0.14] | cost: 1.1806184944517062
theta: [0.01 0.14] | cost: 1.177227251696266
theta: [0.01 0.14] | cost: 1.1762453624313751
theta: [0.01 0.14] | cost: 1.1759370980989148
theta: [0.01 0.14] | cost: 1.175817178966766
CPU times: user 272 ms, sys: 67.3 ms, total: 339 ms
Wall time: 792 ms
</pre></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that gradient descent converges to the theta values of $\theta_0 = 0.01$ and $\theta_1 = 0.14$. Our linear model is:</p>
<p>$y = 0.14x + 0.01$</p>
<p>We can use our estimated thetas to make and plot our predictions alongside the original data points.</p></div></div></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell"
  style="display:none;"
>
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">55</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;total_bill&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;tip&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">tips</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">simple_linear_model</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;goldenrod&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Tip amount vs. Total Bill&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Total Bill&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Tip Amount&#39;</span><span class="p">);</span>
</pre></div></div></div></div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    



<div class="output_png output_subarea ">
<img src="/notebooks-images/linear_grad_18_0.png"
></div></div></div></div></div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that if a table's bill is \$10, our model will predict that the waiter gets around \$1.50 in tip. Similarly, if a table's bill is \$40, our model will predict a tip of around \$6.00.</p></div></div></div></div>
