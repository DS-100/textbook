{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference: https://jupyterbook.org/interactive/hiding.html\n",
    "# Use {hide, remove}-{input, output, cell} tags to hiding content\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "import myst_nb\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('precision', 2)\n",
    "# This option stops scientific notation for pandas\n",
    "# pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "def display_df(df, rows=pd.options.display.max_rows,\n",
    "               cols=pd.options.display.max_columns):\n",
    "    with pd.option_context('display.max_rows', rows,\n",
    "                           'display.max_columns', cols):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch:files_size)=\n",
    "# File Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers have finite limits on computing power. You have likely\n",
    "encountered these limits firsthand if your computer has slowed down from having\n",
    "too many applications open at once. We want to make sure that we do not\n",
    "exceed the computer's limits while working with data, and we might examine a file differently, depending on its size. If we know that our data set is relatively small, then a text editor or a spreadsheet can be convenient to look at the data. On the other hand, for large datasets, a more programmatic exploration or even distributed computing tools may be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations, we analyze datasets downloaded from the Internet. These\n",
    "files reside on the computer's **disk storage**. In order to use Python to\n",
    "explore and manipulate the data, we need to read the data into the computer's\n",
    "**memory**, also known as random access memory (RAM). All Python code requires\n",
    "the use of RAM, no matter how short the code is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computer's RAM is typically much smaller than a computer's disk storage. For\n",
    "example, one computer model released in 2018 had 32 times more disk storage\n",
    "than RAM.  Unfortunately, this means that data files can often be much bigger\n",
    "than what is feasible to read into memory. \n",
    "Both disk storage and RAM capacity are measured in terms of **bytes**. Roughly\n",
    "speaking, each character in a text file adds one byte to the file's size. For\n",
    "example, the legends.csv file has 120 characters and takes up 120 bytes of\n",
    "disk space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, many of the datasets we work with today contain many characters. To\n",
    "succinctly describe the sizes of larger files, we use the prefixes as described\n",
    "in the following {numref}`byte-prefixes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{table} Prefixes for common filesizes.\n",
    ":name: byte-prefixes\n",
    "\n",
    "| Multiple | Notation | Number of Bytes |\n",
    "| -------- | -------- | --------------- |\n",
    "| Kibibyte | KiB      | 1024    |\n",
    "| Mebibyte | MiB      | 1024²   |\n",
    "| Gibibyte | GiB      | 1024³   |\n",
    "| Tebibyte | TiB      | 1024⁴   |\n",
    "| Pebibyte | PiB      | 1024⁵   |\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a file containing 52428800 characters takes up 52428800 bytes = 50\n",
    "mebibytes = 50 MiB on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use multiples of 1024 instead of simple multiples of 1000 for these\n",
    "prefixes?** This is a historical result of the fact that most computers\n",
    "use a binary number scheme where powers of 2 are simpler to represent. You will\n",
    "also see the typical SI prefixes used to describe size---kilobytes, megabytes,\n",
    "and gigabytes, for example. Unfortunately, these prefixes are used\n",
    "inconsistently. Sometimes a kilobyte refers to 1000 bytes; other times, a\n",
    "kilobyte refers to 1024 bytes. To avoid confusion, we will stick to kibi-,\n",
    "mebi-, and gibibytes which clearly represent multiples of 1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When is it safe to read in a file?** Many computers have much more disk\n",
    "storage than available memory. It is not uncommon to have a data file happily\n",
    "stored on a computer that will overflow the computer's memory if we attempt to\n",
    "manipulate it with a program, including Python programs. We often begin our\n",
    "data work by making sure the files we are of manageable size. To accomplish\n",
    "this, we can use the built-in `os` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File                      Size (KiB)\n",
      "data/inspections.csv      455.0\n",
      "data/co2_mm_mlo.txt       50.0\n",
      "data/violations.csv       3639.0\n",
      "data/DAWN-Data.txt        273531.0\n",
      "data/legend.csv           0.0\n",
      "data/businesses.csv       645.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "kib = 1024\n",
    "line = '{:<25} {}'.format\n",
    "\n",
    "print(line('File', 'Size (KiB)'))\n",
    "for filepath in Path('data').glob('*'):\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(line(str(filepath), np.round(size / kib)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `businesses.csv` file takes up 645 KiB on disk, making it well\n",
    "within the memory capacities of most systems. Although the `violations.csv`\n",
    "file takes up 3.6 MiB of disk storage, most machines can easily read it into a\n",
    "Pandas dataframe too. But `DAWN-Data.txt`, which contains the DAWN survey data,\n",
    "is much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DAWN file takes up nearly 270 MiB of disk storage, and while some computers\n",
    "can work with this file in memory, it can slow down other systems. To make\n",
    "this data more manageable in Python, we can load in a subset of the columns\n",
    "rather than all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folder Sizes.** Sometimes we are interested in the total size of a folder\n",
    "instead of the size of individual files. For example, if we have one file of\n",
    "inspections for each month in a year, we might like to see whether we can\n",
    "combine all the data into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data/ folder contains 271.80 MiB\n"
     ]
    }
   ],
   "source": [
    "mib = 1024**2\n",
    "\n",
    "total = 0\n",
    "for filepath in Path('data').glob('*'):\n",
    "    total += os.path.getsize(filepath) / mib\n",
    "\n",
    "print(f'The data/ folder contains {total:.2f} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory Overhead.** As a rule of thumb, reading in a file using `pandas`\n",
    "usually requires at least double the available memory as the file size. That\n",
    "is, reading in a 1 GiB file will typically require at least 2 GiB of available\n",
    "memory.\n",
    "\n",
    "Note that memory is shared by all programs running on a computer, including the\n",
    "operating system, web browsers, and yes, Jupyter notebook itself. A computer\n",
    "with 4 GiB total RAM might have only 1 GiB available RAM with many applications\n",
    "running. With 1 GiB available RAM, it is unlikely that `pandas` will be able to\n",
    "read in a 1 GiB file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Work With Large Datasets?\n",
    "\n",
    "What can we do if we know our data are far larger than what is feasible to\n",
    "load into memory? This is a common scenario in some scientific domains like \n",
    "astronomy, where telescopes capture many large images of space. It's also\n",
    "common for companies that have lots of users. The popular term \"big data\" \n",
    "generally refers to this scenario, when the data are large enough that even\n",
    "the best computers can't read the data directly into memory.\n",
    "\n",
    "How data scientists draw insights from large datasets is an important research\n",
    "question that motivates the fields of database engineering and distributed\n",
    "computing. While we won't have time to cover these fields in depth in this\n",
    "book, we can provide a brief overview of basic approaches. Interested readers\n",
    "can investigate these approaches further in a more specialized text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset The Data.** One simple approach is to subset the data. Rather than\n",
    "loading in the entire data file, we can either select a specific part of it\n",
    "(e.g. one day's worth of data), or we can randomly sample the dataset. Because\n",
    "of its simplicity, we use this approach quite often in this book. The natural\n",
    "downside is that this approach loses many of the benefits of using a large\n",
    "dataset, like being able to study rare events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use a Database System.** As we discussed in the {ref}`ch:sql` chapter,\n",
    "database systems like PostgreSQL are specifically design to store large\n",
    "datasets. Relational database management systems (RDBMS) let users query the\n",
    "data using SQL, which can even manipulate data that are too large to fit into\n",
    "memory. Because of their advantages, RDBMS's are common in research and\n",
    "industry settings for data storage. One downside is that they often require a\n",
    "separate server for the data that needs its own configuration. Another downside\n",
    "is that SQL is less flexible in what it can compute than Python, which is\n",
    "especially relevant for modeling and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use a Distributed Computing System.** One approach to handle complex\n",
    "computations on large datasets is to use a distributed computing system like\n",
    "Apache Spark or Ray. These systems let users write programs in a programming\n",
    "language like Python, and can automatically run these programs across large\n",
    "datasets. Because of this, these systems have great flexibility and can be used\n",
    "in a variety of scenarios like modeling and prediction. Their main downside is\n",
    "that they typically require a lot of work to install and configure properly,\n",
    "since these systems are typically installed across many computers that need to\n",
    "coordinate with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduced common file size notation, and showed how to check file\n",
    "sizes in Python. We also discussed when a file is feasible to read into memory\n",
    "as a `pandas.DataFrame`, and approaches for big data that can't be read into\n",
    "memory. In the next section, we'll describe command line tools available in the\n",
    "shell for carrying out these same tasks. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
