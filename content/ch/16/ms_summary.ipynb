{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance trade-off allows us to more precisely describe the modeling phenomena that we have seen in this chapter: under-fitting relates to model bias; over-fitting results in model variance. In {numref}`Figure %s <model-bias-variance-diagram>`, the $x$-axis measures model complexity and the $y$-axis measures the components of risk - model bias squared and model variance. Notice how as model complexity increases, model bias decreases and model variance increases. Thinking in terms of test error, we have seen this error first decrease and then increase as the model variance outweighs the decrease in model bias. To select a useful model, we must strike a balance between model bias and variance.\n",
    "\n",
    "\n",
    "```{figure} figures/bias_modeling_bias_var_plot.png\n",
    "---\n",
    "name: model-bias-variance-diagram\n",
    "---\n",
    "\n",
    "Bias-variance Trade-off Diagram. As model complexity increases, model variance increases and model bias decreases. In the other direction, model variance decreases and model bias increases as model complexity decreases.  \n",
    "```\n",
    "\n",
    "Collecting more observations reduces bias if the model can fit the population process exactly. If the model is inherently incapable of modeling the population (as in the example above), even infinite data cannot get rid of model bias. In terms of variance, collecting more data reduces variance. One recent trend is to select a model with low bias and high intrinsic variance (such as a neural network) but collect many data points so that the model variance is low enough to make accurate predictions. While effective in practice, collecting enough data for these models tends to require large amounts of time and money.\n",
    "\n",
    "Creating more features, whether useful or not, typically increases model variance.  Models with many parameters have many possible combinations of parameters and therefore have higher variance than models with few parameters. On the other hand, adding a useful feature to the data, such as a quadratic feature when the underlying process is quadratic, reduces bias, but even adding a useless feature rarely increases bias.\n",
    "\n",
    "Being aware of the bias-variance tradeoff  can help... And techniques of train-test, cv, and reg are designed to ameliorate this issue."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
