{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Summary\n",
    "\n",
    "Bootstrapping is powerful BUT\n",
    "\n",
    "+ Make sure that the original sample is large and random so that the sample resembles the population\n",
    "+ Repeat the bootstrap process many times. Typically 10000 replications is a reasonable number\n",
    "+ The bootstrap tends to have difficulties when\n",
    "\n",
    "    + Parameter estimate is influenced by outliers\n",
    "    + Parameter is based on extreme values of the distribution\n",
    "    + Sampling distribution of the statistic is far from bell-shaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until the 1980s, randomization tests tended to be limited to comparatively simple kinds of data sets, because there was no general and well-understood method for generating random data sets in more complicated situations.  Computers and related theory have been transcending those limitations during the last two decades, so that randomization tests have grown in importance, and are now used much more often than in the past!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an extremely important difference that makes this example different from Martin, however.  For the calcium study, the two groups in fact were chosen purely at random.  For the Martin example, there wasn't any actual randomization; instead, random selection was the null model being tested.   For a randomized controlled experiment like the calcium study, the randomization was a deliberate part of the experimental design.  Although the source of the randomization makes no difference in how you calculate the p-value, it makes a tremendous difference in what it tells you.  For Martin, a tiny p-value tells you to reject the null model of random selection.  For the calcium study, that's not a logical option, because the randomization actually occurred.  That randomization guarantees that there are only two possible explanations for the observed difference between the treatment and control groups: chance, or the treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals can be easily misinterpreted as the chance that the parameter $\\theta^*$ is in the interval. However, the confidence interval is created from one realization of the sampling distribution. The sampling distribution gives us a different probability statement, 95% of the time, an interval constructed in this way will contain $\\theta^*$. Unfortunately, we don't know whether this particular time is one of those that happens 95 times in 100, or not. That is why, the term \"confidence\" is used rather than probability.\n",
    "\n",
    "Recall the sampling distribution ({numref`Figure %s <fig:triptych>`) is a probability distribution that reflects the chance of observing different values of $\\hat{\\theta}$. The 95% confidence interval is an interval where $\\hat{\\theta} -\\theta^*$ falls in that interval 95% of the time. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
