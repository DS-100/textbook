{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference: https://jupyterbook.org/interactive/hiding.html\n",
    "# Use {hide, remove}-{input, output, cell} tags to hiding content\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch:optimization)=\n",
    "# Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "This chapter is under development. When it's finished, this note will be\n",
    "removed.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit models, we minimize the  average loss, and most commonly, we minimize the average squared loss. Properties of squared loss give a simple expression when we fit a linear model (see {numref}`Chapter %s <ch:linear>`). But, empirical loss minimization isn't always so straight forward. LASSO regression, with the addition of the $ L_1 $ penalty to the average squared loss no longer has a closed form solution, and logistic regression uses cross-entropy loss to fit a nonlinear model. In these cases, we use numerical optimization to fit the model, where we systematically choose parameter values to evaluate the average loss, in search of the minimizing value.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first introduced loss functions and minimization of average loss in {numref}`Chapter %s <ch:modeling>`, we performed a simple numerical optimization. We created a grid of $ \\theta $ values and evaluated the average loss at all of the points in the grid (see {numref}`Figure %s <grid-diagram>` for a diagram of this concept). The grid point with the smallest average loss we took as the best fit. Unfortunately, this sort of grid search quickly becomes impractical.\n",
    "\n",
    "+ For more complex models with many features, the grid becomes unwieldy. With only four features and a grid of 100 values for each feature, we must evaluate the average loss at $ 100^4 = 100,000,000 $ grid points.\n",
    "\n",
    "+ The range of parameter values to search over must be specified in advance to create the grid, and when we don't have a good sense of range we need to start with a wide grid and possibly repeat the grid search over narrower ranges.\n",
    "\n",
    "+ With a large number of observations, the evaluation of the average loss over the grid points can become slow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/grid-diagram.png\n",
    "---\n",
    "name: grid-diagram\n",
    "---\n",
    "\n",
    "Searching over a grid of points can be slow computationally or inexact.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we introduce numerical optimization techniques that take advantage of the shape and smoothness of the loss function in the search for the minimizing parameter values. We first introduce the basic idea behind the technique of gradient descent, then describe the properties of the loss function that make gradient descent work, and finally, we provide a few extensions of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
