{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference: https://jupyterbook.org/interactive/hiding.html\n",
    "# Use {hide, remove}-{input, output, cell} tags to hiding content\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives to Gradient Descent\n",
    "\n",
    "In this section, we discuss a few modifications to gradient descent. These are stochastic gradient descent, mini-batch gradient descent, and Newton's method. The first two take the approach of using subsets of the data when computing the next step to take, and so are useful for large datasets. On the other hand, Newton's method assumes the loss function is twice differentiable and uses a quadratic approximation to the loss function, rather than a linear approximation. We discuss each of these in turn, but first we bring back the definition of the average loss so it's clear how the data enter into the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to minimize the average loss,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L({\\boldsymbol{\\theta}}, \\textbf{X}, \\mathbf{y}) & = \\frac{1}{n} \\sum_{i = 1}^{n}{\\cal l}({\\boldsymbol{\\theta}}, \\mathbf{x}_i, {y_i}). \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the gradient descent algorithm takes steps based on the gradient,\n",
    "\n",
    "$$\n",
    "{\\boldsymbol \\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\alpha \\cdot \\nabla_{\\theta} L(\\boldsymbol{\\theta}^{(t)}, \\textbf{X}, \\textbf{y})\n",
    "$$\n",
    "\n",
    "And, since $\\nabla_{\\theta} L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y})$, can be expressed as the average gradient of the loss function $\\cal l$, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} L(\\boldsymbol{\\theta}, \\textbf{X}, \\textbf{y}) &= \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\theta} {\\cal l}(\\boldsymbol{\\theta}, \\textbf{x}_i, y_i).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This representation of the gradient of the average loss in terms of the average of the gradient of loss at each point in the data shows why the algorithm is also called *batch gradient descent*. Two alternatives to batch gradient descent use smaller amounts of the data, rather than the complete \"batch\". The first alternative uses only one observation in each step of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Although batch gradient descent can often find an optimal $ \\boldsymbol{\\theta} $ in relatively few iterations, each iteration can take a long time to compute if the data set contains many observations. To get around this difficulty, stochastic gradient descent approximates the overall gradient with a single, randomly chosen, data point. Since this observation is chosen randomly, we expect that using the gradient at randomly chosen observations will on average go in the correct direction and so eventually converge to the minimizing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, to conduct stochastic gradient descent, we replace the average gradient with the gradient at a single data point. So, the gradient update formula for stochastic gradient descent is just:\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\theta}}^{(t+1)} = {\\boldsymbol{\\theta}}^{(t)} - \\alpha \\cdot \\nabla_{\\theta} {\\cal l}({\\boldsymbol{\\theta}}^{(t)}, \\textbf{x}_i, y_i)\n",
    "$$\n",
    "\n",
    "In this formula, $ \\textbf{x}_i $ and $ y_i $ are chosen randomly from the data. Choosing the points randomly is critical to the success of stochastic gradient descent. If the points are not chosen randomly, stochastic gradient descent may produce significantly worse results than batch gradient descent.\n",
    "\n",
    "We most commonly run stochastic gradient descent by shuffling the data points and using each one in its shuffled order until we complete one entire pass through the data. If the algorithm hasn't converged yet, then we reshuffle the points and run another pass through the data. Each *iteration* of stochastic gradient descent looks at one data point; each complete pass through the data is called an *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since stochastic descent only examines a single data point a time, it at times takes steps away from the minimizer, $ \\hat{\\boldsymbol{\\theta}} $, but on average these steps are in the right direction. And, since stochastic gradient descent computes an update much faster than batch gradient descent, stochastic gradient descent can make significant progress towards the optimal $ \\boldsymbol{\\hat{\\theta}} $ by the time batch gradient descent finishes a single update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "As its name suggests, *Mini-batch gradient descent* strikes a balance between batch gradient descent and stochastic gradient descent by increasing the number of observations selected at random at each iteration. In mini-batch gradient descent, we average the gradient of the loss function at a few data points, instead of a single point or all the points.  We let $\\mathcal{B}$ represent the mini-batch of data points that are randomly sampled from the data set, and define the algorithm's next step as,\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{\\theta}}^{(t+1)} = {\\boldsymbol{\\theta}}^{(t)} - \\alpha \\cdot \\frac{1}{{|\\mathcal{B}|}} \\sum_{{i\\in\\mathcal{B}}} \\nabla_{\\theta} {\\cal l}(\\boldsymbol{\\theta}, \\textbf{x}_i, y_i)\n",
    "$$\n",
    "\n",
    "As with stochastic gradient descent, we perform mini-batch gradient descent by shuffling the data, splitting  the data into consecutive mini-batches, and iterating through the batches in sequence. After each epoch, we re-shuffle our data and select new mini-batches.\n",
    "\n",
    "While we have made the distinction between stochastic and mini-batch gradient descent, stochastic gradient descent is sometimes used as an umbrella term that encompasses the selection of a mini-batch of any size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Newton's method uses the second derivative of the loss $ \\cal{l} $ to find the minimizing value. The basic idea is to approximate the average loss, $ L( \\boldsymbol{\\theta}) $ in small neighborhoods of $ \\boldsymbol{\\theta} $ with a quadratic curve, rather than a linear approximation.  The approximations looks as follows, for a small step, $ \\mathbf{s} $\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\boldsymbol{\\theta} + \\mathbf{s}) \\approx L(\\boldsymbol{\\theta}) + g( \\boldsymbol{\\theta})^T \\mathbf{s}\n",
    " + \\frac{1}{2} \\mathbf{s}^T H(\\boldsymbol{\\theta})\\mathbf{s}, \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $ g(\\boldsymbol{\\theta}) = \\nabla_{\\theta} L(\\boldsymbol{\\theta} $ is the gradient and $  H(\\boldsymbol{\\theta}) = \\nabla_{\\theta}^2 L(\\boldsymbol{\\theta}$ is the  Hessian of $ L(\\boldsymbol{\\theta}) $. More specifically, $ H $ is a $ p \\times p $ matrix of second order partial derivatives in $ \\boldsymbol{\\theta} $ with $ i $, $ j $ element: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{i, j} =  \\frac {\\partial^2 \\cal{l}}  {\\partial \\theta_i \\partial \\theta_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This quadratic approximation to $ L(\\boldsymbol{\\theta} + \\mathbf{s})  $  has a minimum at:\n",
    "$ \\mathbf{s} = - [H^{-1} (\\boldsymbol{\\theta})]g(\\boldsymbol{\\theta}) $. (Convexity implies that $ H $ is a symmetric square matrix that can be inverted). Then, the algorithm steps are:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} + \\frac {1} {n} \n",
    "\\sum_{i=1}^{n} - [H^{-1} (\\boldsymbol{\\theta}^{(t)}]g(\\boldsymbol{\\theta}^{(t)})\n",
    "$$\n",
    "\n",
    "\n",
    "{numref}`Figure %s <newton-diagram>` gives the idea behind the Newton method of optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/newton-diagram.png\n",
    "---\n",
    "name: newton-diagram\n",
    "---\n",
    "\n",
    "Newton method uses a local quadratic approximation to the curve to take steps toward the minimizing value of a  convex function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique converges fast if the approximation is accurate and the steps are small. Otherwise, Newton's method can diverge, which often happens if the function is nearly flat in a dimension. Unlike with gradient descent, there is no step-size that guarantees that steps are all small. For example, if the function is relatively flat then the derivative is near zero and its inverse can be quite large. Since the approximation works locally, large steps can move to $ \\boldsymbol{\\theta} $ that are far from where the approximation is accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
