{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch:risk)=\n",
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "This chapter is under development. When it's finished, this note will be\n",
    "removed.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting models, so far in this book, we have decided which features to include in the model by:\n",
    "\n",
    "+ assessing model fit with residual plots\n",
    "+ connecting the statistical model to a physical model\n",
    "+ keeping the model simple\n",
    "+ comparing improvements in average loss between increasingly complex models \n",
    "\n",
    "For example, when we examined the one-variable model of upward mobility in {numref}`Chapter %s <ch:linear>`, we found curvature in the residual plot. Adding a second variable greatly improved the fit in terms of average loss (MSE and, relatedly, multiple-R-squared), but some curvature remained in the residuals. A seven-variable model made little improvement over the two-variable model, in terms of a decrease in MSE, so although the two-variable model still showed some patterns in the residuals, we opted for this simpler model.\n",
    "\n",
    "As another example, when we modeled the weight of a donkey (see {numref}`Chapter %s <ch:donkey>`), we took guidance from a physical model. We ignored the donkey's appendages and drew on the similarity between a barrel and a donkey's body to begin fitting a model that explained weight by its length and girth (comparable to a barrel's height and circumference). We continued to adjust that model\n",
    "by adding categorical features related to the donkey's physical condition and age, collapsing categories and excluding other possible features to keep the model simple. \n",
    "\n",
    "The decisions we made in building these models were based on judgment calls, and in \n",
    "this chapter we augment these with more formal criteria. \n",
    "To begin, we provide an example that shows why it's typically not a good idea to include too many features in a model. This phenomenon, called overfitting, often leads to models that follow the data too closely and capture some of the noise in the data. Then, when new observations come along, the predictions are worse than those from a simpler model. The remainder of the chapter provides techniques for limiting the impact of overfitting. These techniques are especially helpful when there are a large number of potential features to include in a model. We also briefly delve into the theory that explains the phenomenon of overfitting by a closer examination of the notion of loss, which was first introduced in {numref}`Chapter %s <ch:modeling>`. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
