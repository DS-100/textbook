{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "outlier_czs = [34105, 34113, 34112, 34106]\n",
    "df = (\n",
    "    pd.read_csv('data/mobility.csv')\n",
    "    # filter out rows with NaN AUM values\n",
    "    .query('not aum.isnull()', engine='python')\n",
    "    # take out outlier CZs\n",
    "    .query('cz not in @outlier_czs')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:linear_multi)=\n",
    "# Multiple Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've used a single predictor variable $ x $ to predict the outcome\n",
    "$ y $.\n",
    "Now, we'll introduce the *multiple linear model*, a linear model that uses\n",
    "multiple predictors to predict $ y $.\n",
    "This is useful because having multiple predictors can\n",
    "improve our model's fit to the data and improve accuracy.\n",
    "After defining the multiple linear model, we'll use it to predict AUM using\n",
    "a combination of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple predictors, we say that $ x $ is a $ p $-dimensional\n",
    "column vector\n",
    "$ x = [ x_1, x_2, \\ldots, x_p ] $. Then, for a given $ x $ the\n",
    "outcome $ y $ depends on a linear combination of $ x_i $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_p x_p + \\epsilon\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the simple linear model, our\n",
    "multiple linear model $ f_{\\theta}(x) $ predicts $ y $ for a given $ x $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_p x_p\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify this notation if we add an intercept term to $ x $ so that\n",
    "$ x = [ 1, x_1, x_2, \\ldots, x_p ] $.\n",
    "Since we also write our model parameters as a column vector\n",
    "$ \\theta = [ \\theta_0, \\theta_1, \\ldots, \\theta_p ] $, we can\n",
    "use the definition of the dot product to\n",
    "write our model as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\theta}(x) &= \\theta_0 + \\theta_1 x_1 + \\ldots + \\theta_p x_p \\\\\n",
    "&= \\theta \\cdot x \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final simplification, we'll use matrix notation to show how our\n",
    "models works on our entire dataset.\n",
    "Before, we said that a single observation is $ (x, y) $, where $ x $ is a\n",
    "vector of predictor variables and $ y $ is the scalar outcome. \n",
    "Now, we'll say that $ X $ is a matrix. Each row of $ X $ has the predictors\n",
    "for a single observation.\n",
    "We'll also say that $ y $ is a vector (instead of scalar) with the outcomes\n",
    "for each observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "  &        & \\vdots &        &        \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "& &\n",
    "y =  \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call $ X $ the *design matrix*.\n",
    "It's a $ n \\times (p + 1) $ matrix (remember that we added an extra dimension\n",
    "for the intercept term).\n",
    "Now, we can write the predictions for the entire dataset using\n",
    "matrix multiplication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\theta}(x) &= X \\theta\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ X $ is an $ n \\times (p + 1) $ matrix and $ \\theta $ is a $ (p + 1) $-dimensional column vector.\n",
    "This means that $ X \\theta $ is\n",
    "an $ n $-dimensional column vector. Each item in the vector\n",
    "is the model's predictions for one observation.\n",
    "It's easier to understand the design matrix through an example, so let's\n",
    "return to the Opportunity data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data prepared for modeling, in the next section we'll \n",
    "fit our model by finding the $ \\hat{\\theta} $ that minimizes our loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:linear_multi_fit)=\n",
    "## Fitting the Multiple Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a $ n \\times (p + 1) $ design matrix $ X $, a $ n $-dimensional\n",
    "column vector of outcomes $ y $, and a $ (p + 1) $-dimensional column \n",
    "vector of model parameters $ \\theta $, we assume that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y = X \\theta + \\epsilon\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $ \\epsilon $ is a $ n $-dimensional column vector that represents the\n",
    "sampling error.\n",
    "We define the multiple linear model as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\theta}(X) = X \\theta\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the simple linear model, we'll fit $ f_{\\theta}(X) $ using\n",
    "the squared loss function.\n",
    "We want to find the model parameters $ \\hat{\\theta} $ that minimize the\n",
    "mean squared loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\theta, X, y)\n",
    " &= \\frac{1}{n} \\left | y - f_{\\theta}(X) \\right|^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using the notation $ |v|^2 $ for a vector $ v $ as a\n",
    "shorthand for the sum of each vector element squared [^l2]:\n",
    "$ |v|^2 = \\sum_i v_i^2 $ .\n",
    "\n",
    "[^l2]: $ |v| $ is also called the $ \\ell_2 $ norm of a\n",
    "vector $ v $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll fit our model by figuring out what the\n",
    "minimizing $ \\hat{\\theta} $ is.\n",
    "One idea is to use calculus as we did for the simple linear model.\n",
    "However, this approach needs knowledge of vector calculus that we won't\n",
    "cover in this book.\n",
    "Instead, we'll use a geometric argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Geometric Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is the find the $ \\hat{\\theta} $ that minimizes our loss\n",
    "function---we want to make $ L(\\theta, X, y) $ as small as possible\n",
    "for a given $ X $ and $ y $.\n",
    "The key insight is that we can restate this goal in a geometric way.\n",
    "Remember: the model predictions $ f_{\\theta}(X) $ and the true outcomes\n",
    "$ y $ are both vectors.\n",
    "We can treat vectors as points---for example, we can plot\n",
    "the vector $ [ 2, 3 ] $ at $ x = 2, y = 3 $ in 2D space.\n",
    "Then, minimizing $ L(\\theta, X, y) $ is equivalent to finding\n",
    "$ \\hat{\\theta} $ that makes $ f_{\\theta}(X) $ as close as possible to\n",
    "$ y $ when we plot them as points.\n",
    "As depicted in {numref}`Figure %s <fig:geom-2d>`, different values of\n",
    "$ \\theta $ give different predictions $ f_{\\theta}(X) $ (hollow points).\n",
    "Then, $ \\hat{\\theta} $ is the vector of parameters that put\n",
    "$ f_{\\theta}(X) $ as close to $ y $ (filled point) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/geom-2d.svg\n",
    "---\n",
    "name: fig:geom-2d\n",
    "width: 250px\n",
    "---\n",
    "\n",
    "A plot showing different values of $ f_{\\theta}(X) $ (hollow points) and\n",
    "the outcome vector $ y $ (filled point).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the possible values of $ f_{\\theta}(X) $.\n",
    "In {numref}`Figure %s <fig:geom-2d>`, we showed a few possible \n",
    "$ f_{\\theta}(X) $.\n",
    "Instead of just plotting a few possible points, we can\n",
    "plot *all* possible values of $ f_{\\theta}(X) $ by varying $ \\theta $.\n",
    "This results in a subspace of possible $ f_{\\theta}(X) $ values, as shown in\n",
    "{numref}`Figure %s <fig:geom-span>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/geom-span.svg\n",
    "---\n",
    "name: fig:geom-span\n",
    "width: 250px\n",
    "---\n",
    "\n",
    "A plot showing all possible values of $ f_{\\theta}(X) $ as a line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above {numref}`Figure %s <fig:geom-span>`, we drew the set of\n",
    "possible $ f_{\\theta}(X) $ values as a line.\n",
    "Since our model is $ f_{\\theta}(X) = X \\theta $, from a property of\n",
    "matrix-vector multiplication we know that $ f_{\\theta}(X) $ is a linear\n",
    "combination of the columns of $ X $, which we also call $ \\text{span}(X) $.\n",
    "Now, we need to figure out which point within $ \\text{span}(X) $ lies the\n",
    "closest to $ y $.\n",
    "\n",
    "As {numref}`Figure %s <fig:geom-span>` suggests, the closest point to $ y $ \n",
    "is the point where the error $ \\epsilon = y - f_{\\theta}(X) $ is perpendicular\n",
    "to $ \\text{span}(X) $. We'll leave the complete proof as an exercise.\n",
    "With this final fact, we can solve for $ \\hat{\\theta} $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_\\hat{\\theta}(X) + \\epsilon &= y \\\\\n",
    "X \\hat{\\theta} + \\epsilon &= y \\\\\n",
    "X^\\top X \\hat{\\theta} + X^\\top \\epsilon &= X^\\top y\n",
    "    & (\\text{left-multiply both sides by } X^\\top) \\\\\n",
    "X^\\top X \\hat{\\theta} &= X^\\top y\n",
    "    & (X^\\top \\epsilon = 0 \\text{ since } \\epsilon \\perp \\text{span}(X)) \\\\\n",
    "\\hat{\\theta} &= (X^\\top X)^{-1} X^\\top y\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this derivation done, we can now write a short function to\n",
    "fit the multiple linear model using $ X $ and $ y $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that deriving $ \\hat{\\theta} $ for the multiple linear model also gives\n",
    "us $ \\hat{\\theta} $ for the simple linear model too. If we set\n",
    "$ X $ to contain the intercept column and one column of features, using the\n",
    "formula for $ \\hat{\\theta} $ gives us the intercept and slope of the best-fit\n",
    "line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
