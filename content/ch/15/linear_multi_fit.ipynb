{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:linear_multi_fit)=\n",
    "# Fitting the Multiple Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we considered the case of two explanatory variables by using one variable called $x$ and the other $v$. Now we want to generalize the approach to $p$ explanatory variables. Our approach of choosing new letters to represent additional variables fails us. Instead, we use a more formal approach that represents multiple predictors as a matrix: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\\n",
    "  &        & \\vdots &        &        \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We call $\\textbf{X}$ the *design matrix*. (Notice that $\\textbf{X}$ is an $ n \\times (p + 1)$ matrix.) Each column of $\\textbf{X}$ represents a variable, and each row represents an observation. That is, $x_{i,j}$ is the measurement taken on observation $i$ of variable $j$.\n",
    "For example, for a given observation, say, the second row in $\\textbf{X}$, we represent the outcome $y_2$ by the linear approximation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_2 \\approx  \\theta_0 + \\theta_1 x_{2,1} + \\ldots + \\theta_p x_{2,p} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similar to the simple linear model, our\n",
    "multiple linear model predicts $y_2$ for the given values $[x_{2,1}, \\ldots,x_{2,p}]$.\n",
    "But now, we have a collection of predictor (explanatory) variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the model parameters as a $p+1$ column vector\n",
    "${\\boldsymbol{\\theta}} = [ \\theta_0, \\theta_1, \\ldots, \\theta_p ]^t$.\n",
    "We can also represent $\\mathbf{y}$ as a column vector of the outcomes:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} =  \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Putting these notational definitions together, we can write the vector of predictions for the entire dataset using matrix multiplication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{y}} &= {\\textbf{X}} {\\boldsymbol{\\theta}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the dimensions of $\\textbf{X}$ and $\\boldsymbol{\\theta}$ confirms that $\\hat{\\mathbf{y}}$ is an $ n $-dimensional column vector. \n",
    "Each item in this vector is the model's prediction for one observation.\n",
    "Additionally, the errors in using $\\hat{\\mathbf{y}}$ to predict $\\mathbf{y}$ can be expressed as the $n$-dimensional column vector:\n",
    "\n",
    "$$ \\mathbf{e} = \\mathbf{y}  - \\hat{\\mathbf{y}}.$$\n",
    "\n",
    "Matrix notation will help us fit the multiple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the simple linear model, we fit ${\\textbf{X}} {\\boldsymbol{\\theta}}$ using squared loss.\n",
    "That is, we want to find the model parameters $\\hat{\\boldsymbol{\\theta}}$ that minimize the average squared loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L({\\boldsymbol{\\theta}}, \\textbf{X}, {\\mathbf{y}})\n",
    " &= \\frac{1}{n} \\sum_i [y_i - (\\theta_0 + \\theta_1 x_{i,1} + \\cdots + \\theta_p x_{i,p})]^2 \\\\\n",
    " &= \\frac{1}{n}  \\lVert \\mathbf{y} - {\\textbf{X}} {\\boldsymbol{\\theta}} \\rVert^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the notation $ \\lVert \\mathbf{v} \\rVert^2 $ for a vector $\\mathbf{v}$ as a\n",
    "shorthand for the sum of each vector element squared [^ell2]:\n",
    "$\\lVert \\mathbf{v} \\rVert^2 = \\sum_i v_i^2$.\n",
    "\n",
    "[^ell2]: $\\lVert \\mathbf{v} \\rVert^2$ is also called the $\\ell_2$ norm of a vector $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit our model using calculus as we did for the simple linear model.\n",
    "However, this approach can get cumbersome, and instead we use a geometric argument that is more intuitive and easily leads to useful properties of the design matrix, errors, and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Geometric Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our goal is the find the $ \\hat{\\theta} $ that minimizes our loss\n",
    "function---we want to make $ L(\\theta, X, y) $ as small as possible\n",
    "for a given $ X $ and $ y $.\n",
    "The key insight is that we can restate this goal in a geometric way.\n",
    "Remember: the model predictions $ f_{\\theta}(X) $ and the true outcomes\n",
    "$ y $ are both vectors.\n",
    "We can treat vectors as points---for example, we can plot\n",
    "the vector $ [ 2, 3 ] $ at $ x = 2, y = 3 $ in 2D space.\n",
    "Then, minimizing $ L(\\theta, X, y) $ is equivalent to finding\n",
    "$ \\hat{\\theta} $ that makes $ f_{\\theta}(X) $ as close as possible to\n",
    "$ y $ when we plot them as points.\n",
    "As depicted in {numref}`Figure %s <fig:geom-2d>`, different values of\n",
    "$ \\theta $ give different predictions $ f_{\\theta}(X) $ (hollow points).\n",
    "Then, $ \\hat{\\theta} $ is the vector of parameters that put\n",
    "$ f_{\\theta}(X) $ as close to $ y $ (filled point) as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/geom-2d.svg\n",
    "---\n",
    "name: fig:geom-2d\n",
    "width: 250px\n",
    "---\n",
    "\n",
    "A plot showing different values of $ f_{\\theta}(X) $ (hollow points) and\n",
    "the outcome vector $ y $ (filled point).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the possible values of $ f_{\\theta}(X) $.\n",
    "In {numref}`Figure %s <fig:geom-2d>`, we showed a few possible \n",
    "$ f_{\\theta}(X) $.\n",
    "Instead of just plotting a few possible points, we can\n",
    "plot *all* possible values of $ f_{\\theta}(X) $ by varying $ \\theta $.\n",
    "This results in a subspace of possible $ f_{\\theta}(X) $ values, as shown in\n",
    "{numref}`Figure %s <fig:geom-span>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} figures/geom-span.svg\n",
    "---\n",
    "name: fig:geom-span\n",
    "width: 250px\n",
    "---\n",
    "\n",
    "A plot showing all possible values of $ f_{\\theta}(X) $ as a line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above {numref}`Figure %s <fig:geom-span>`, we drew the set of\n",
    "possible $ f_{\\theta}(X) $ values as a line.\n",
    "Since our model is $ f_{\\theta}(X) = X \\theta $, from a property of\n",
    "matrix-vector multiplication we know that $ f_{\\theta}(X) $ is a linear\n",
    "combination of the columns of $ X $, which we also call $ \\text{span}(X) $.\n",
    "Now, we need to figure out which point within $ \\text{span}(X) $ lies the\n",
    "closest to $ y $.\n",
    "\n",
    "As {numref}`Figure %s <fig:geom-span>` suggests, the closest point to $ y $ \n",
    "is the point where the error $ \\epsilon = y - f_{\\theta}(X) $ is perpendicular\n",
    "to $ \\text{span}(X) $. We'll leave the complete proof as an exercise.\n",
    "With this final fact, we can solve for $ \\hat{\\theta} $:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f_\\hat{\\theta}(X) + \\epsilon &= y \\\\\n",
    "X \\hat{\\theta} + \\epsilon &= y \\\\\n",
    "X^\\top X \\hat{\\theta} + X^\\top \\epsilon &= X^\\top y\n",
    "    & (\\text{left-multiply both sides by } X^\\top) \\\\\n",
    "X^\\top X \\hat{\\theta} &= X^\\top y\n",
    "    & (X^\\top \\epsilon = 0 \\text{ since } \\epsilon \\perp \\text{span}(X)) \\\\\n",
    "\\hat{\\theta} &= (X^\\top X)^{-1} X^\\top y\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this derivation done, we can now write a short function to\n",
    "fit the multiple linear model using $ X $ and $ y $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that deriving $ \\hat{\\theta} $ for the multiple linear model also gives\n",
    "us $ \\hat{\\theta} $ for the simple linear model too. If we set\n",
    "$ X $ to contain the intercept column and one column of features, using the\n",
    "formula for $ \\hat{\\theta} $ gives us the intercept and slope of the best-fit\n",
    "line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
