{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if not any(path.endswith('textbook') for path in sys.path):\n",
    "    sys.path.append(os.path.abspath('../../..'))\n",
    "from textbook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec:linear_summary)=\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models help us describe relationships between features.\n",
    "We discussed the simple linear model and extended it to linear models in multiple\n",
    "variables.\n",
    "Along the way, we applied mathematical techniques that are widely useful\n",
    "in modeling---calculus to minimize loss for\n",
    "the simple linear model and matrix geometry for the multiple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models may seem basic, but they are used for all sorts of tasks today.\n",
    "And, they are flexible enough to allow us to include categorical features as well as\n",
    "nonlinear transformations of variables, such as log-transformations, polynomials, and ratios.\n",
    "Linear models have the advantage of being broadly interpretable for non-technical people,\n",
    "yet sophisticated enough to capture many common patterns in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be tempting to throw all of the variables available to us into a model to get the \"best fit possible\". But, we should keep in mind the geometry of least squares when fitting models. Recall, that $p$ explanatory variables can be thought of as $p$ vectors in $n$-dimensional space, and if these vectors are highly correlated, then the projections onto this space will be similar to projections onto smaller spaces made up of fewer vectors. This implies that:\n",
    "\n",
    "+ Adding more variables may not provide a large improvement in the model\n",
    "+ Interpretation of the coefficients can be difficult \n",
    "+ Several models can be equally effective in predicting/explaining the response variable\n",
    "\n",
    "If we are concerned with making inferences, where we want to interpret/understand the model, then we should err on the side of simpler models. \n",
    "On the other hand, if our primary concern is the predictive ability of a model, then we tend not to concern ourselves with the number of coefficients and their interpretation. But, this \"black box\" approach can lead to models that, say, overly depend on anomalous values in the data or models that are inadequate in other ways. So be careful with the black box approach, especially when the predictions may be harmful to people.  \n",
    "\n",
    "In this chapter, we have used linear models in a descriptive way. We introduced a few notions for deciding when to include a feature in a model by examining residuals for patterns, comparing the size of standard errors and the change in the multiple $R^2$. Often times we settled for a \n",
    "simpler model that was easier to interpret. In the next chapter, we look at other more formal tools for choosing the features to include in a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
