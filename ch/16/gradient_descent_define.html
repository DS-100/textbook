
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>16.2. Gradient Descent &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="16.3. Convexity" href="gradient_convexity.html" />
    <link rel="prev" title="16.1. Loss Minimization Using a Program" href="gradient_basics.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  The Data Science Lifecycle
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_1.html">
     1.1. The Students of Data 100
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_2.html">
     1.2. Exploratory Data Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_3.html">
     1.3. What’s in a Name?
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../02/design_intro.html">
   2. Generalizing from Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_dewey_truman.html">
     2.1. Dewey Defeats Truman
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_data.html">
     2.2. [In Progress] Data Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_sampling.html">
     2.3. Probability Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_srs_vs_big_data.html">
     2.4. SRS vs. “Big Data”
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../03/modeling_intro.html">
   3. Modeling and Estimation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_simple.html">
     3.1. Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_loss_functions.html">
     3.2. Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_abs_huber.html">
     3.3. Absolute and Huber Loss
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/cycle_case_study_intro.html">
   4. [In progress] Case Study
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Rectangular Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../05/sql_intro.html">
   5. Relational Databases and SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_rdbms.html">
     5.1. The Relational Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_basics.html">
     5.2. SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_joins.html">
     5.3. SQL Joins
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../06/pandas_intro.html">
   6. Data Tables in Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_indexes.html">
     6.1. Indexes, Slicing, and Sorting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_grouping_pivoting.html">
     6.2. Grouping and Pivoting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_apply_strings_plotting.html">
     6.3. Apply, Strings, and Plotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_joins.html">
     6.4. [In progress] Joins
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Preparing and Exploring Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../07/repr_intro.html">
   7. [In Progress] Data Representation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_structure.html">
     7.1. Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_semantics.html">
     7.2. [In Progress] Semantics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_data_types.html">
     7.3. Data Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_granularity.html">
     7.4. Granularity
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/quality_intro.html">
   8. [In Progress] Data Quality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/eda_intro.html">
   9. [In Progress] Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../10/viz_intro.html">
   10. Data Visualization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_quantitative.html">
     10.1. Visualizing Quantitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_qualitative.html">
     10.2. Visualizing Qualitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_matplotlib.html">
     10.3. Customizing Plots using
     <code class="docutils literal notranslate">
      <span class="pre">
       matplotlib
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_principles.html">
     10.4. Visualization Principles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_principles_2.html">
     10.5. Visualization Principles Continued
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_philosophy.html">
     10.6. Philosophy for Data Visualization
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11/police_intro.html">
   11. [In progress] Case Study: Berkeley Policing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11/police_calls.html">
     11.1. Cleaning the Calls Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/police_stops.html">
     11.2. Cleaning The Stops Dataset
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Other Data Sources
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12/text_intro.html">
   12. Working with Text
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_strings.html">
     12.1. Python String Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_regex.html">
     12.2. Regular Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_re.html">
     12.3. Regex and Python
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13/web_intro.html">
   13. Web Technologies
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_http.html">
     13.1. HTTP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_rest.html">
     13.2. [In Progress] REST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_html.html">
     13.3. [In Progress] XPath and HTML
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear Modeling
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14/linear_models.html">
   14. Linear Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_tips.html">
     14.1. Predicting Tip Amounts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_fitting.html">
     14.2. [In progress] Fitting a linear model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_inference.html">
     14.3. [In progress] Inference for Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_projection.html">
     14.4. Least Squares — A Geometric Perspective
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../15/prob_and_gen.html">
   15. Probability and Generalization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../15/prob_random_vars.html">
     15.1. Random Variables
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/prob_exp_var.html">
     15.2. Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/prob_risk.html">
     15.3. Risk
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="gradient_descent.html">
   16. Gradient Descent and Numerical Optimization
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_basics.html">
     16.1. Loss Minimization Using a Program
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     16.2. Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_convexity.html">
     16.3. Convexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_stochastic.html">
     16.4. Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient_lin_reg.html">
     16.5. Fitting a Linear Model Using Gradient Descent
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17/donkey_intro.html">
   17. [In progress] Case Study: Donkey Dimensions
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17/donkey_analysis.html">
     17.1. Linear Regression Case Study
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Multiple Linear Modeling
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../18/mult_intro.html">
   18. [In progress] Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../18/mult_model.html">
     18.1. Multiple Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/mult_inference.html">
     18.2. Inference for Multiple Linear Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19/feature_engineering.html">
   19. Feature Engineering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19/feature_one_hot.html">
     19.1. The Walmart dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/feature_polynomial.html">
     19.2. Predicting Ice Cream Ratings
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../20/bias_intro.html">
   20. The Bias-Variance Tradeoff
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_risk.html">
     20.1. Risk and Loss Minimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_modeling.html">
     20.2. Model Bias and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_cv.html">
     20.3. Cross-Validation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21/reg_intro.html">
   21. Regularization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_intuition.html">
     21.1. Regularization Intuition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_ridge.html">
     21.2. L2 Regularization: Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_lasso.html">
     21.3. L1 Regularization: Lasso Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../22/mult_case_intro.html">
   22. [In progress] Case Study: Multiple Linear Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23/classification_intro.html">
   23. Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_prob.html">
     23.1. Regression on Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_log_model.html">
     23.2. The Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_cost.html">
     23.3. A Loss Function for the Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_log_reg.html">
     23.4. Using Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_cost_justification.html">
     23.5. Approximating the Empirical Probability Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_sgd.html">
     23.6. Fitting a Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_sensitivity_specificity.html">
     23.7. Evaluating Logistic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_multiclass.html">
     23.8. Multiclass Classification
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Replicability
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../24/repl_intro.html">
   24. [In progress] Replicable Research
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../24/repl_phacking.html">
     24.1. P-hacking
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Extra Topics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25/pca_intro.html">
   25. Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_dims.html">
     25.1. Dimensions of a Data Table
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_svd.html">
     25.2. PCA using the Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_in_practice.html">
     25.3. PCA in Practice
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../26/dtrees_intro.html">
   26. [In progress] Decision Trees and Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../27/clustering_intro.html">
   27. [In progress] Clustering
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../a01/prob_review.html">
   Probability Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a02/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../a03/ref_intro.html">
   Reference Tables
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/ref_pandas.html">
     pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/ref_seaborn.html">
     Seaborn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/ref_matplotlib.html">
     matplotlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/ref_sklearn.html">
     scikit-learn
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a04/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/16/gradient_descent_define.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/16/gradient_descent_define.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#intuition">
   16.2.1. Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-analysis">
   16.2.2. Gradient Descent Analysis
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-minimize-function">
   16.2.3. Defining the
   <code class="docutils literal notranslate">
    <span class="pre">
     minimize
    </span>
   </code>
   Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#minimizing-the-huber-loss">
   16.2.4. Minimizing the Huber loss
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   16.2.5. Summary
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tips</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;tips&#39;</span><span class="p">)</span>
<span class="n">tips</span><span class="p">[</span><span class="s1">&#39;pcttip&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tips</span><span class="p">[</span><span class="s1">&#39;tip&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">tips</span><span class="p">[</span><span class="s1">&#39;total_bill&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad_mse</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_loss</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">xlim</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">]</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">*</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">loss_fn</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$ \theta $&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">plot_theta_on_loss</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
    <span class="n">default_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$ \theta $&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">xkcd_rgb</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">theta</span><span class="p">],</span> <span class="p">[</span><span class="n">loss</span><span class="p">],</span> <span class="o">**</span><span class="p">{</span><span class="o">**</span><span class="n">default_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">})</span>

<span class="k">def</span> <span class="nf">plot_tangent_on_loss</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="p">((</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span> <span class="o">-</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">))</span>
             <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">))</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">theta</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">xs</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">xkcd_rgb</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="gradient-descent">
<h1><span class="section-number">16.2. </span>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>We are interested in creating a function that can minimize a loss function without forcing the user to predetermine which values of <span class="math notranslate nohighlight">\(\theta\)</span> to try. In other words, while the <code class="docutils literal notranslate"><span class="pre">simple_minimize</span></code> function has the following signature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">simple_minimize</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">thetas</span><span class="p">)</span>
</pre></div>
</div>
<p>We would like a function that has the following signature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>This function needs to automatically find the minimizing <span class="math notranslate nohighlight">\(\theta\)</span> value no matter how small or large it is. We will use a technique called gradient descent to implement this new <code class="docutils literal notranslate"><span class="pre">minimize</span></code> function.</p>
<div class="section" id="intuition">
<h2><span class="section-number">16.2.1. </span>Intuition<a class="headerlink" href="#intuition" title="Permalink to this headline">¶</a></h2>
<p>As with loss functions, we will discuss the intuition for gradient descent first, then formalize our understanding with mathematics.</p>
<p>Since the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> function is not given values of <span class="math notranslate nohighlight">\(\theta\)</span> to try, we start by picking a <span class="math notranslate nohighlight">\(\theta\)</span> anywhere we’d like. Then, we can iteratively improve the estimate of <span class="math notranslate nohighlight">\(\theta\)</span>. To improve an estimate of <span class="math notranslate nohighlight">\(\theta\)</span>, we look at the slope of the loss function at that choice of <span class="math notranslate nohighlight">\( \theta \)</span>.</p>
<p>For example, suppose we are using MSE for the simple dataset <span class="math notranslate nohighlight">\( \textbf{y} = [ 12.1, 12.8, 14.9, 16.3, 17.2 ] \)</span> and our current choice of <span class="math notranslate nohighlight">\( \theta \)</span> is 12.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">])</span>
<span class="n">plot_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plot_theta_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gradient_descent_define_5_0.png" src="../../_images/gradient_descent_define_5_0.png" />
</div>
</div>
<p>We’d like to choose a new value for <span class="math notranslate nohighlight">\(\theta\)</span> that decreases the loss. To do this, we look at the slope of the loss function at <span class="math notranslate nohighlight">\(\theta= 12 \)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">])</span>
<span class="n">plot_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plot_tangent_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gradient_descent_define_7_0.png" src="../../_images/gradient_descent_define_7_0.png" />
</div>
</div>
<p>The slope is negative, which means that increasing <span class="math notranslate nohighlight">\(\theta\)</span> will decrease the loss.</p>
<p>If <span class="math notranslate nohighlight">\(\theta = 16.5 \)</span> on the other hand, the slope of the loss function is positive:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">])</span>
<span class="n">plot_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plot_tangent_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mf">16.5</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gradient_descent_define_9_0.png" src="../../_images/gradient_descent_define_9_0.png" />
</div>
</div>
<p>When the slope is positive, decreasing <span class="math notranslate nohighlight">\( \theta \)</span> will decrease the loss.</p>
<p>The slope of the tangent line tells us which direction to move <span class="math notranslate nohighlight">\( \theta \)</span> in order to decrease the loss. If the slope is negative, we want <span class="math notranslate nohighlight">\( \theta \)</span> to move in the positive direction. If the slope is positive, <span class="math notranslate nohighlight">\(\theta \)</span> should move in the negative direction. Mathematically, we write:</p>
<div class="math notranslate nohighlight">
\[
\theta^{(t+1)} = \theta^{(t)} - \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y})
\]</div>
<p>Where <span class="math notranslate nohighlight">\( \theta^{(t)} \)</span> is the current estimate and <span class="math notranslate nohighlight">\( \theta^{(t+1)} \)</span> is the next estimate.</p>
<p>For the MSE, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L(\theta, \textbf{y})
&amp;= \frac{1}{n} \sum_{i = 1}^{n}(y_i - \theta)^2\\
\frac{\partial}{\partial \hat{\theta}} L(\theta, \textbf{y})
&amp;= \frac{1}{n} \sum_{i = 1}^{n} -2(y_i - \theta) \\
&amp;= -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) \\
\end{aligned}
\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\( \theta^{(t)} = 12 \)</span>, we can compute <span class="math notranslate nohighlight">\( -\frac{2}{n} \sum_{i = 1}^{n} (y_i - \theta) = -5.32 \)</span>. Thus, <span class="math notranslate nohighlight">\( \theta^{(t+1)} = 12 - (-5.32) = 17.32 \)</span>.</p>
<p>We’ve plotted the old value of <span class="math notranslate nohighlight">\( \theta \)</span> as a green outlined circle and the new value as a filled in circle on the loss curve below.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">])</span>
<span class="n">plot_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plot_theta_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span>
                   <span class="n">edgecolor</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">xkcd_rgb</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plot_theta_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mf">17.32</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/gradient_descent_define_11_0.png" src="../../_images/gradient_descent_define_11_0.png" />
</div>
</div>
<p>Although <span class="math notranslate nohighlight">\( \theta \)</span> went in the right direction, it ended up as far away from the minimum as it started. We can remedy this by multiplying the slope by a small constant before subtracting it from <span class="math notranslate nohighlight">\( \theta\)</span>. Our final update formula is:</p>
<div class="math notranslate nohighlight">
\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta^{(t)}, \textbf{y})
\]</div>
<p>where <span class="math notranslate nohighlight">\( \alpha \)</span> is a small constant. For example, if we set <span class="math notranslate nohighlight">\( \alpha = 0.3 \)</span>, this is the new <span class="math notranslate nohighlight">\( \theta^{(t+1)} \)</span>:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_one_gd_iter</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_loss</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
    <span class="n">plot_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">18</span><span class="p">),</span> <span class="n">loss_fn</span><span class="p">)</span>
    <span class="n">plot_theta_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span>
                       <span class="n">edgecolor</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">xkcd_rgb</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plot_theta_on_loss</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="n">new_theta</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;old theta: </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;new theta: </span><span class="si">{</span><span class="n">new_theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_one_gd_iter</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">grad_mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>old theta: 12
new theta: 13.596
</pre></div>
</div>
<img alt="../../_images/gradient_descent_define_14_1.png" src="../../_images/gradient_descent_define_14_1.png" />
</div>
</div>
<p>Here are the <span class="math notranslate nohighlight">\( \theta \)</span> values for successive iterations of this process. Notice that <span class="math notranslate nohighlight">\( \theta \)</span> changes more slowly as it gets closer to the minimum loss because the slope is also smaller.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_one_gd_iter</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mf">13.60</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">grad_mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>old theta: 13.6
new theta: 14.236
</pre></div>
</div>
<img alt="../../_images/gradient_descent_define_16_1.png" src="../../_images/gradient_descent_define_16_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_one_gd_iter</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mf">14.24</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">grad_mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>old theta: 14.24
new theta: 14.492
</pre></div>
</div>
<img alt="../../_images/gradient_descent_define_17_1.png" src="../../_images/gradient_descent_define_17_1.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_one_gd_iter</span><span class="p">(</span><span class="n">pts</span><span class="p">,</span> <span class="mf">14.49</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">grad_mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>old theta: 14.49
new theta: 14.592
</pre></div>
</div>
<img alt="../../_images/gradient_descent_define_18_1.png" src="../../_images/gradient_descent_define_18_1.png" />
</div>
</div>
</div>
<div class="section" id="gradient-descent-analysis">
<h2><span class="section-number">16.2.2. </span>Gradient Descent Analysis<a class="headerlink" href="#gradient-descent-analysis" title="Permalink to this headline">¶</a></h2>
<p>We now have the full algorithm for gradient descent:</p>
<ol class="simple">
<li><p>Choose a starting value of <span class="math notranslate nohighlight">\( \theta \)</span> (0 is a common choice).</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\( \theta - \alpha \cdot \frac{\partial}{\partial \theta} L(\theta, \textbf{y}) \)</span> and store this as the new value of <span class="math notranslate nohighlight">\( \theta \)</span>.</p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\( \theta \)</span> doesn’t change between iterations.</p></li>
</ol>
<p>You will more commonly see the gradient <span class="math notranslate nohighlight">\( \nabla_\theta \)</span> in place of the partial derivative <span class="math notranslate nohighlight">\( \frac{\partial}{\partial \theta} \)</span>. The two notations are essentially equivalent, but since the gradient notation is more common we will use it in the gradient update formula from now on:</p>
<div class="math notranslate nohighlight">
\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \nabla_\theta L(\theta^{(t)}, \textbf{y})
\]</div>
<p>To review notation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \theta^{(t)} \)</span> is the current estimate of <span class="math notranslate nohighlight">\( \theta^* \)</span> at the <span class="math notranslate nohighlight">\(t\)</span>th iteration.</p></li>
<li><p><span class="math notranslate nohighlight">\( \theta^{(t+1)} \)</span> is the next choice of <span class="math notranslate nohighlight">\( \theta \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \alpha \)</span> is called the learning rate, usually set to a small constant. Sometimes it is useful to start with a larger <span class="math notranslate nohighlight">\( \alpha \)</span> and decrease it over time. If <span class="math notranslate nohighlight">\( \alpha \)</span> changes between iterations, we use the variable <span class="math notranslate nohighlight">\( \alpha^t \)</span> to mark that <span class="math notranslate nohighlight">\( \alpha \)</span> varies over time <span class="math notranslate nohighlight">\( t \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( \nabla_\theta L(\theta^{(t)}, \textbf{y}) \)</span> is the partial derivative / gradient of the loss function with respect to <span class="math notranslate nohighlight">\( \theta \)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
</ul>
<p>You can now see the importance of choosing a differentiable loss function: <span class="math notranslate nohighlight">\( \nabla_\theta L(\theta, \textbf{y}) \)</span> is a crucial part of the gradient descent algorithm. (While it is possible to estimate the gradient by computing the difference in loss for two slightly different values of <span class="math notranslate nohighlight">\( \theta \)</span> and dividing by the distance between <span class="math notranslate nohighlight">\( \theta \)</span> values, this typically increases the runtime of gradient descent so significantly that it becomes impractical to use.)</p>
<p>The gradient algorithm is simple yet powerful since we can use it for many types of models and many types of loss functions. It is the computational tool of choice for fitting many important models, including linear regression on large datasets and neural networks.</p>
</div>
<div class="section" id="defining-the-minimize-function">
<h2><span class="section-number">16.2.3. </span>Defining the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> Function<a class="headerlink" href="#defining-the-minimize-function" title="Permalink to this headline">¶</a></h2>
<p>Now we return to our original task: defining the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> function. We will have to change our function signature slightly since we now need to compute the gradient of the loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">minimize</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_loss_fn</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Uses gradient descent to minimize loss_fn. Returns the minimizing value of</span>
<span class="sd">    theta_hat once theta_hat changes less than 0.001 between iterations.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">progress</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;theta: </span><span class="si">{</span><span class="n">theta</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_loss_fn</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">new_theta</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">gradient</span>
        
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_theta</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">new_theta</span>
        
        <span class="n">theta</span> <span class="o">=</span> <span class="n">new_theta</span>
</pre></div>
</div>
</div>
</div>
<p>Then we can define functions to compute our MSE and its gradient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">grad_mse</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_vals</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can use the <code class="docutils literal notranslate"><span class="pre">minimize</span></code> function to compute the minimizing value of <span class="math notranslate nohighlight">\( \theta \)</span> for <span class="math notranslate nohighlight">\( \textbf{y} = [12.1, 12.8, 14.9, 16.3, 17.2] \)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span> <span class="n">grad_mse</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Minimizing theta: </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>theta: 0.00 | loss: 218.76
theta: 5.86 | loss: 81.21
theta: 9.38 | loss: 31.70
theta: 11.49 | loss: 13.87
theta: 12.76 | loss: 7.45
theta: 13.52 | loss: 5.14
theta: 13.98 | loss: 4.31
theta: 14.25 | loss: 4.01
theta: 14.41 | loss: 3.90
theta: 14.51 | loss: 3.86
theta: 14.57 | loss: 3.85
theta: 14.61 | loss: 3.85
theta: 14.63 | loss: 3.84
theta: 14.64 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.65 | loss: 3.84
theta: 14.66 | loss: 3.84
theta: 14.66 | loss: 3.84
Minimizing theta: 14.658511131035242

CPU times: user 7.88 ms, sys: 3.58 ms, total: 11.5 ms
Wall time: 8.54 ms
</pre></div>
</div>
</div>
</div>
<p>We can see that gradient descent quickly finds the same solution as the analytic method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="mf">12.1</span><span class="p">,</span> <span class="mf">12.8</span><span class="p">,</span> <span class="mf">14.9</span><span class="p">,</span> <span class="mf">16.3</span><span class="p">,</span> <span class="mf">17.2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>14.66
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="minimizing-the-huber-loss">
<h2><span class="section-number">16.2.4. </span>Minimizing the Huber loss<a class="headerlink" href="#minimizing-the-huber-loss" title="Permalink to this headline">¶</a></h2>
<p>Now, we can apply gradient descent to minimize the Huber loss on our dataset of tip percentages.</p>
<p>The Huber loss is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    \frac{1}{2}(y_i - \theta)^2 &amp;  | y_i - \theta | \le \delta \\
     \delta (|y_i - \theta| - \frac{1}{2} \delta ) &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>The gradient of the Huber loss is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\theta} L_\delta(\theta, \textbf{y}) = \frac{1}{n} \sum_{i=1}^n \begin{cases}
    -(y_i - \theta) &amp;  | y_i - \theta | \le \delta \\
    - \delta \cdot \text{sign} (y_i - \theta) &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>(Note that in previous definitions of Huber loss we used the variable <span class="math notranslate nohighlight">\( \alpha \)</span> to denote the transition point. To avoid confusion with the <span class="math notranslate nohighlight">\( \alpha \)</span> used in gradient descent, we replace the transition point parameter of the Huber loss with <span class="math notranslate nohighlight">\( \delta \)</span>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">huber_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">dataset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">,</span>
                 <span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">dataset</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
                 <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="n">d</span> <span class="o">-</span> <span class="n">delta</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">))</span>
    <span class="p">)</span>

<span class="k">def</span> <span class="nf">grad_huber_loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">theta</span> <span class="o">-</span> <span class="n">dataset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;=</span> <span class="n">delta</span><span class="p">,</span>
                 <span class="o">-</span><span class="p">(</span><span class="n">dataset</span> <span class="o">-</span> <span class="n">theta</span><span class="p">),</span>
                 <span class="o">-</span><span class="n">delta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">dataset</span> <span class="o">-</span> <span class="n">theta</span><span class="p">))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s minimize the Huber loss on the tips dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">huber_loss</span><span class="p">,</span> <span class="n">grad_huber_loss</span><span class="p">,</span> <span class="n">tips</span><span class="p">[</span><span class="s1">&#39;pcttip&#39;</span><span class="p">],</span> <span class="n">progress</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Minimizing theta: </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimizing theta: 15.506849531471964

CPU times: user 194 ms, sys: 4.13 ms, total: 198 ms
Wall time: 208 ms
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2><span class="section-number">16.2.5. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Gradient descent gives us a generic way to minimize a loss function when we cannot solve for the minimizing value of <span class="math notranslate nohighlight">\( \theta \)</span> analytically. As our models and loss functions increase in complexity, we will turn to gradient descent as our tool of choice to fit models.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/16"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="gradient_basics.html" title="previous page"><span class="section-number">16.1. </span>Loss Minimization Using a Program</a>
    <a class='right-next' id="next-link" href="gradient_convexity.html" title="next page"><span class="section-number">16.3. </span>Convexity</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>