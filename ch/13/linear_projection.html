
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>13.4. Least Squares — A Geometric Perspective &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.2d2078699c18a0efb88233928e1cf6ed.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.be0a4a0c39cd630af62a2fcf693f3f06.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13.5. Linear Regression Case Study" href="linear_case_study.html" />
    <link rel="prev" title="13.3. Multiple Linear Regression" href="linear_multiple.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div>

<form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>

<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <p class="caption">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/design_intro.html">
   2. Data Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/pandas_intro.html">
   3. Working with Tabular Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/eda_intro.html">
   4. Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/cleaning_intro.html">
   5. Data Cleaning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/viz_intro.html">
   6. Data Visualization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/web_intro.html">
   7. Web Technologies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/text_intro.html">
   8. Working with Text
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/sql_intro.html">
   9. Relational Databases and SQL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10/modeling_intro.html">
   10. Modeling and Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../11/gradient_descent.html">
   11. Gradient Descent and Numerical Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../12/prob_and_gen.html">
   12. Probability and Generalization
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="reference internal" href="linear_models.html">
   13. Linear Models
  </a>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="linear_tips.html">
     13.1. Predicting Tip Amounts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_grad.html">
     13.2. Fitting a Linear Model Using Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_multiple.html">
     13.3. Multiple Linear Regression
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     13.4. Least Squares — A Geometric Perspective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linear_case_study.html">
     13.5. Linear Regression Case Study
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../14/feature_engineering.html">
   14. Feature Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../15/bias_intro.html">
   15. The Bias-Variance Tradeoff
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../16/reg_intro.html">
   16. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../17/classification_intro.html">
   17. Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../18/hyp_intro.html">
   18. Statistical Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../19/pca_intro.html">
   19. Dimensionality Reduction and PCA
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../20/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../21/ref_intro.html">
   Reference Tables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../22/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav>

 <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        <div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    
    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/13/linear_projection.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
    
</div>
        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/13/linear_projection.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#least-squares-constant-model">
   13.4.1. Least Squares: Constant Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#least-squares-simple-linear-model">
   13.4.2. Least Squares: Simple Linear Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#geometric-intuition">
   13.4.3. Geometric Intuition
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-algebra">
   13.4.4. Linear Algebra
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finishing-up-the-case-study">
   13.4.5. Finishing up the Case Study
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#when-variables-are-linearly-dependent">
   13.4.6. When Variables are Linearly Dependent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#two-schools-of-thought">
   13.4.7. Two Schools of Thought
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="least-squares-a-geometric-perspective">
<h1><span class="section-number">13.4. </span>Least Squares — A Geometric Perspective<a class="headerlink" href="#least-squares-a-geometric-perspective" title="Permalink to this headline">¶</a></h1>
<p>Recall that we found the optimal coefficients for linear models by optimizing their loss functions with gradient descent. We also mentioned that least squares linear regression can be solved analytically. While gradient descent is practical, this geometric perspective will provide a deeper understanding of linear regression.</p>
<p>A Vector Space Review is included in the Appendix. We will assume familiarity with vector arithmetic, the 1-vector, span of a collection of vectors, and projections.</p>
<p>Suppose we seek a linear model for this data:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>-1</p></td>
<td><p>-2</p></td>
</tr>
</tbody>
</table>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="p">],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/linear_projection_5_0.png" src="../../_images/linear_projection_5_0.png" />
</div>
</div>
<p>Assume that the best model is one with the least error, and that the least squares error is an acceptable measure.</p>
<div class="section" id="least-squares-constant-model">
<h2><span class="section-number">13.4.1. </span>Least Squares: Constant Model<a class="headerlink" href="#least-squares-constant-model" title="Permalink to this headline">¶</a></h2>
<p>Like we did with the tips dataset, let’s start with the constant model: the model that only ever predicts a single number.</p>
<div class="math notranslate nohighlight">
\[ \theta = C\]</div>
<p>Thus, we are working with just the <span class="math notranslate nohighlight">\(y\)</span>-values.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>-2</p></td>
</tr>
</tbody>
</table>
<p>Our goal is to find the <span class="math notranslate nohighlight">\( \theta \)</span> that results in the line that minimizes the squared loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split} L(\theta, \textbf{y}) = \sum_{i = 1}^{n}(y_i - \theta)^2\\ \end{split}\]</div>
<p>Recall that for the constant model, the minimizing <span class="math notranslate nohighlight">\(\theta\)</span> for MSE is <span class="math notranslate nohighlight">\(\bar{\textbf{y}}\)</span>, the average of the <span class="math notranslate nohighlight">\(\textbf{y}\)</span> values. The calculus derivation can be found in the Loss Functions lesson in the Modeling and Estimations chapter. For the linear algebra derivation, please refer to the Vector Space Review in the Appendix.</p>
<p>Notice that our loss function is a sum of squares. The <em>L2</em>-norm for a vector is also a sum of squares, but with a square root:</p>
<div class="math notranslate nohighlight">
\[\Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}\]</div>
<p>If we let <span class="math notranslate nohighlight">\(y_i - \theta = v_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L(\theta, \textbf{y}) 
&amp;= v_1^2 + v_2^2 + \dots + v_n^2 \\
&amp;= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split}\]</div>
<p>This means our loss can be expressed as the <em>L2</em>-norm of some vector <span class="math notranslate nohighlight">\(\textbf{v}\)</span>, squared. We can express <span class="math notranslate nohighlight">\(v_i\)</span> as <span class="math notranslate nohighlight">\(y_i - \theta \quad \forall i \in [1,n]\)</span> so that in Cartesian notation,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\textbf{v} \quad &amp;= \quad \begin{bmatrix} y_1 - \theta \\ y_2 - \theta \\ \vdots \\ y_n - \theta \end{bmatrix} \\
&amp;= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\begin{bmatrix} \theta \\ \theta \\ \vdots \\ \theta \end{bmatrix} \\
&amp;= \quad \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>So our loss function can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{aligned}
L(\theta, \textbf{y})
\quad &amp;= \quad \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{bmatrix} \quad - \quad 
\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}
\qquad \right \Vert ^2 \\
\quad &amp;= \quad \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
\end{split}\]</div>
<p>The expression <span class="math notranslate nohighlight">\(\theta \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix}\)</span>  is a scalar multiple of the columns of the <span class="math notranslate nohighlight">\(\textbf{1}\)</span> vector, and is the result of our predictions, denoted <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span>.</p>
<p>This gives us a new perspective on what it means to minimize the least squares error.</p>
<p><span class="math notranslate nohighlight">\(\textbf{y}\)</span> and <span class="math notranslate nohighlight">\(\textbf{1}\)</span> are fixed, but <span class="math notranslate nohighlight">\(\theta\)</span> can take on any value, so <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> can be any scalar multiple of <span class="math notranslate nohighlight">\(\textbf{1}\)</span>. We want to find <span class="math notranslate nohighlight">\(\theta\)</span> so that <span class="math notranslate nohighlight">\( \theta \textbf{1} \)</span> is as close to <span class="math notranslate nohighlight">\(\textbf{y}\)</span> as possible. We use <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to denote this best-fit <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/1dprojection.png"><img alt="1dprojection.png" class="align-center" src="../../_images/1dprojection.png" style="width: 300px;" /></a>
<p>The projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto <span class="math notranslate nohighlight">\(\textbf{1}\)</span> is guaranteed to be the closest vector (see “Vector Space Review” in the Appendix).</p>
</div>
<div class="section" id="least-squares-simple-linear-model">
<h2><span class="section-number">13.4.2. </span>Least Squares: Simple Linear Model<a class="headerlink" href="#least-squares-simple-linear-model" title="Permalink to this headline">¶</a></h2>
<p>Now, let’s look at the simple linear regression model. This is strongly parallel to the constant model derivation, but be mindful of the differences and think about how you might generalize to multiple linear regression.</p>
<p>The simple linear model is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_\boldsymbol\theta (x_i) 
&amp;= \theta_0 + \theta_1 x_i \\
\end{aligned}
\end{split}\]</div>
<p>Our goal is to find the <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> that results in the line with the least squared error:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L(\boldsymbol\theta, \textbf{x}, \textbf{y})
&amp;= \sum_{i = 1}^{n}(y_i - f_\boldsymbol\theta (x_i))^2\\
&amp;= \sum_{i = 1}^{n}(y_i - \theta_0 - \theta_1 x_i)^2\\
&amp;= \sum_{i = 1}^{n}(y_i - \begin{bmatrix} 1 &amp; x_i \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix} ) ^2
\end{aligned}
\end{split}\]</div>
<p>To help us visualize the translation of our loss summation into matrix form, let’s expand out the loss with <span class="math notranslate nohighlight">\(n = 3\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&amp;=
(y_1 - \begin{bmatrix} 1 &amp; x_1 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2  \\
&amp;+
(y_2 - \begin{bmatrix} 1 &amp; x_2 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
&amp;+
(y_3 - \begin{bmatrix} 1 &amp; x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix})^2 \\
\end{aligned}
\end{split}\]</div>
<p>Again, our loss function is a sum of squares and the <em>L2</em>-norm for a vector is the square root of a sum of squares:</p>
<div class="math notranslate nohighlight">
\[\Vert \textbf{v} \Vert = \sqrt{v_1^2 + v_2^2 + \dots + v_n^2}\]</div>
<p>If we let $y_i - \begin{bmatrix} 1 &amp; x_i \end{bmatrix}</p>
<div class="amsmath math notranslate nohighlight" id="equation-b0f191d1-07b4-4be2-9028-2f25d56fd39b">
<span class="eqno">(13.1)<a class="headerlink" href="#equation-b0f191d1-07b4-4be2-9028-2f25d56fd39b" title="Permalink to this equation">¶</a></span>\[\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) 
&amp;= v_1^2 + v_2^2 + \dots + v_n^2 \\
&amp;= \Vert \textbf{v} \Vert^2
\end{aligned}
\end{split}\]</div>
<p>As before, our loss can be expressed as the <em>L2</em>-norm of some vector <span class="math notranslate nohighlight">\(\textbf{v}\)</span>, squared. With each component $v_i = y_i - \begin{bmatrix} 1 &amp; x_i \end{bmatrix}</p>
<div class="amsmath math notranslate nohighlight" id="equation-49e55d21-2a3f-4226-9b9a-e9de2e03b2be">
<span class="eqno">(13.2)<a class="headerlink" href="#equation-49e55d21-2a3f-4226-9b9a-e9de2e03b2be" title="Permalink to this equation">¶</a></span>\[\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{aligned}
L(\boldsymbol{\theta}, \textbf{x}, \textbf{y})
&amp;= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ 1 &amp; x_3 \end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&amp;= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\textbf{X}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}
\qquad \right \Vert ^2 \\
&amp;= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
f_\boldsymbol\theta(\textbf{x})
\qquad \right \Vert ^2 \\
&amp;= \left \Vert  \qquad  
\textbf{y} 
\quad - \quad 
\hat{\textbf{y}}
\qquad \right \Vert ^2 \\
\end{aligned}
\end{split}\]</div>
<p>The matrix multiplication $\begin{bmatrix} 1 &amp; x_1 \ 1 &amp; x_2 \ 1 &amp; x_3 \end{bmatrix}</p>
<div class="amsmath math notranslate nohighlight" id="equation-9e7db235-4586-44c2-a2eb-d49661ca9c61">
<span class="eqno">(13.3)<a class="headerlink" href="#equation-9e7db235-4586-44c2-a2eb-d49661ca9c61" title="Permalink to this equation">¶</a></span>\[\begin{bmatrix} 
     \theta_0 \\
     \theta_1
\end{bmatrix}\]</div>
<p><span class="math notranslate nohighlight">\(\textbf{X}\)</span> and <span class="math notranslate nohighlight">\(\textbf{y}\)</span> are fixed, but <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> can take on any value, so <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> can take on any of the infinite linear combinations of the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span>. To have the smallest loss, we want to choose <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> such that <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> is as close to <span class="math notranslate nohighlight">\(\textbf{y}\)</span> as possibled, denoted as <span class="math notranslate nohighlight">\(\hat{\boldsymbol\theta}\)</span>.</p>
</div>
<div class="section" id="geometric-intuition">
<h2><span class="section-number">13.4.3. </span>Geometric Intuition<a class="headerlink" href="#geometric-intuition" title="Permalink to this headline">¶</a></h2>
<p>Now, let’s develop an intuition for why it matters that <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> is restricted to the linear combinations of the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span>. Although the span of any set of vectors includes an infinite number of linear combinations, infinite does not mean any—the linear combinations are restricted by the basis vectors.</p>
<p>As a reminder, here is our loss function and scatter plot:</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{\theta}, \textbf{x}, \textbf{y}) \quad = \quad \left \Vert  \quad  
\textbf{y} 
\quad - \quad 
\textbf{X} \boldsymbol\theta
\quad \right \Vert ^2\]</div>
<div class="cell tag_hide_input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDDEN</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/linear_projection_19_0.png" src="../../_images/linear_projection_19_0.png" />
</div>
</div>
<p>By inspecting our scatter plot, we see that no line can perfectly fit our points, so we cannot achieve 0 loss. Thus, we know that <span class="math notranslate nohighlight">\(\textbf{y}\)</span> is not in the plane spanned by <span class="math notranslate nohighlight">\(\textbf{x}\)</span> and <span class="math notranslate nohighlight">\(\textbf{1}\)</span>, represented as a parallelogram below.</p>
<a class="reference internal image-reference" href="../../_images/proj1.png"><img alt="proj1.png" class="align-center" src="../../_images/proj1.png" style="width: 500px;" /></a>
<p>Since our loss is distance-based, we can see that to minimize <span class="math notranslate nohighlight">\( L(\boldsymbol\theta, \textbf{x}, \textbf{y}) = \left \Vert  \textbf{y} - \textbf{X} \boldsymbol\theta \right \Vert ^2\)</span>, we want <span class="math notranslate nohighlight">\(\textbf{X} \boldsymbol\theta\)</span> to be as close to <span class="math notranslate nohighlight">\(\textbf{y}\)</span> as possible.</p>
<p>Mathematically, we are looking for the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto the vector space spanned by the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span>, as the projection of any vector is the closest point in <span class="math notranslate nohighlight">\(Span(\textbf{X})\)</span> to that vector. Thus, choosing <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> such that <span class="math notranslate nohighlight">\(\hat{\textbf{y}} = \textbf{X} \boldsymbol\theta= \)</span> proj<span class="math notranslate nohighlight">\(_{Span(\textbf{X})} \)</span> <span class="math notranslate nohighlight">\(\textbf{y}\)</span> is the best solution.</p>
<a class="reference internal image-reference" href="../../_images/proj2.png"><img alt="proj2.png" class="align-center" src="../../_images/proj2.png" style="width: 500px;" /></a>
<p>To see why, consider other points on the vector space, in purple.</p>
<a class="reference internal image-reference" href="../../_images/proj3.png"><img alt="proj3.png" class="align-center" src="../../_images/proj3.png" style="width: 500px;" /></a>
<p>By the Pythagorean Theorem, any other point on the plane is farther from <span class="math notranslate nohighlight">\(\textbf{y}\)</span> than <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> is. The length of the perpendicular corresponding to <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> represents the least squared error.</p>
</div>
<div class="section" id="linear-algebra">
<h2><span class="section-number">13.4.4. </span>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Permalink to this headline">¶</a></h2>
<p>Since we’ve snuck in a lot of linear algebra concepts already, all that’s left is solving for the <span class="math notranslate nohighlight">\(\hat{\boldsymbol\theta}\)</span> that yields our desired <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span>.</p>
<p>A couple things to note:</p>
<a class="reference internal image-reference" href="../../_images/proj4.png"><img alt="proj4.png" class="align-center" src="../../_images/proj4.png" style="width: 500px;" /></a>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{\textbf{y}} + \textbf{e} = \textbf{y}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{e}\)</span> is perpendicular to <span class="math notranslate nohighlight">\(\textbf{x}\)</span> and <span class="math notranslate nohighlight">\(\textbf{1}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\textbf{y}} = \textbf{X} \hat{\boldsymbol\theta}\)</span> is the vector closest to <span class="math notranslate nohighlight">\(\textbf{y}\)</span> in the vector space spanned by <span class="math notranslate nohighlight">\(\textbf{x}\)</span> and <span class="math notranslate nohighlight">\(\textbf{1}\)</span></p></li>
</ul>
<p>Thus, we arrive at the equation:</p>
<div class="math notranslate nohighlight">
\[\textbf{X}  \hat{\boldsymbol\theta} + \textbf{e} = \textbf{y}\]</div>
<p>Left-multiplying both sides by <span class="math notranslate nohighlight">\(\textbf{X}^T\)</span>:</p>
<div class="math notranslate nohighlight">
\[\textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} + \textbf{X}^T \textbf{e} = \textbf{X}^T \textbf{y}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\textbf{e}\)</span> is perpendicular to the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span>, <span class="math notranslate nohighlight">\(\textbf{X}^T \textbf{e}\)</span> is a column vector of <span class="math notranslate nohighlight">\(0\)</span>’s. Thus, we arrive at the Normal Equation:</p>
<div class="math notranslate nohighlight">
\[\textbf{X}^T \textbf{X}  \hat{\boldsymbol\theta} = \textbf{X}^T \textbf{y}\]</div>
<p>From here, we can easily solve for <span class="math notranslate nohighlight">\(\hat{\boldsymbol\theta}\)</span> by left-multiplying both sides by <span class="math notranslate nohighlight">\((\textbf{X}^T \textbf{X})^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\boldsymbol\theta} = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}\]</div>
<p>Note: we can get this same solution by minimizing with vector calculus, but in the case of least squares loss, vector calculus isn’t necessary. For other loss functions, we will need to use vector calculus to get the analytic solution.</p>
</div>
<div class="section" id="finishing-up-the-case-study">
<h2><span class="section-number">13.4.5. </span>Finishing up the Case Study<a class="headerlink" href="#finishing-up-the-case-study" title="Permalink to this headline">¶</a></h2>
<p>Let’s return to our case study, apply what we’ve learned, and explain why our solution is sound.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\textbf{y} = \begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \qquad \textbf{X} = \begin{bmatrix} 1 &amp; 3 \\ 1 &amp; 0 \\ 1 &amp; -1 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\boldsymbol\theta} 
&amp;= 
\left(
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 3 &amp; 0 &amp; -1 \end{bmatrix}
\begin{bmatrix} 1 &amp; 3 \\ 1 &amp; 0 \\ 1 &amp; -1 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 3 &amp; 0 &amp; -1 \end{bmatrix}
\begin{bmatrix} 2 \\ 1 \\ -2  \end{bmatrix} \\
&amp;= 
\left(
\begin{bmatrix} 3 &amp; 2\\ 2 &amp; 10 \end{bmatrix}
\right)^{-1}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&amp;=
\frac{1}{30-4}
\begin{bmatrix} 10 &amp; -2\\ -2 &amp; 3 \end{bmatrix}
\begin{bmatrix} 1 \\ 8 \end{bmatrix} \\
&amp;=
\frac{1}{26}
\begin{bmatrix} -6 \\ 22 \end{bmatrix}\\
&amp;=
\begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix}
\end{align}
\end{split}\]</div>
<p>We have analytically found that best model for least squares regression is <span class="math notranslate nohighlight">\(f_\boldsymbol{\boldsymbol\theta}(x_i) = - \frac{3}{13} + \frac{11}{13} x_i\)</span>. We know that our choice of <span class="math notranslate nohighlight">\(\boldsymbol\theta\)</span> is sound by the mathematical property that the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto the span of the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span> yields the closest point in the vector space to <span class="math notranslate nohighlight">\(\textbf{y}\)</span>. Under linear constraints using the least squares loss, solving for <span class="math notranslate nohighlight">\(\hat{\boldsymbol\theta}\)</span> by taking the projection guarantees us the optimal solution.</p>
</div>
<div class="section" id="when-variables-are-linearly-dependent">
<h2><span class="section-number">13.4.6. </span>When Variables are Linearly Dependent<a class="headerlink" href="#when-variables-are-linearly-dependent" title="Permalink to this headline">¶</a></h2>
<p>For every additional variable, we add one column to <span class="math notranslate nohighlight">\(\textbf{X}\)</span>. The span of the columns of <span class="math notranslate nohighlight">\(\textbf{X}\)</span> is the linear combinations of the column vectors, so adding columns only changes the span if it is linearly independent from all existing columns.</p>
<p>When the added column is linearly dependent, it can be expressed as a linear combination of some other columns, and thus will not introduce new any vectors to the subspace.</p>
<p>Recall that the span of <span class="math notranslate nohighlight">\(\textbf{X}\)</span> is important because it is the subspace we want to project <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto. If the subspace does not change, then the projection will not change.</p>
<p>For example, when we introduced <span class="math notranslate nohighlight">\(\textbf{x}\)</span> to the constant model to get the simple linear model, we introduced a independent variable. <span class="math notranslate nohighlight">\(\textbf{x} = \begin{bmatrix} 3 \\ 0 \\ -1 \end{bmatrix}\)</span> cannot be expressed as a scalar of <span class="math notranslate nohighlight">\(\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}\)</span>. Thus, we moved from finding the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto a line:</p>
<a class="reference internal image-reference" href="../../_images/1dprojection.png"><img alt="1dprojection.png" class="align-center" src="../../_images/1dprojection.png" style="width: 250px;" /></a>
<p>to finding the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto a plane:</p>
<a class="reference internal image-reference" href="../../_images/proj1.png"><img alt="proj1.png" class="align-center" src="../../_images/proj1.png" style="width: 400px;" /></a>
<p>Now, lets introduce another variable, <span class="math notranslate nohighlight">\(\textbf{z}\)</span>, and explicitly write out the bias column:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>z</strong></p></th>
<th class="head"><p><strong>1</strong></p></th>
<th class="head"><p><strong>x</strong></p></th>
<th class="head"><p><strong>y</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>4</p></td>
<td><p>1</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>1</p></td>
<td><p>-1</p></td>
<td><p>-2</p></td>
</tr>
</tbody>
</table>
<p>Notice that <span class="math notranslate nohighlight">\(\textbf{z} = \textbf{1} + \textbf{x}\)</span>. Since <span class="math notranslate nohighlight">\(\textbf{z}\)</span> is a linear combination of <span class="math notranslate nohighlight">\(\textbf{1}\)</span> and <span class="math notranslate nohighlight">\(\textbf{x}\)</span>, it lies in the original <span class="math notranslate nohighlight">\(Span(\textbf{X})\)</span>. Formally, <span class="math notranslate nohighlight">\(\textbf{z}\)</span> is linearly dependent to <span class="math notranslate nohighlight">\(\{\textbf{1}\)</span>, <span class="math notranslate nohighlight">\(\textbf{x}\}\)</span> and does not change <span class="math notranslate nohighlight">\(Span(\textbf{X})\)</span>. Thus, the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto the subspace spanned by <span class="math notranslate nohighlight">\(\textbf{1}\)</span>, <span class="math notranslate nohighlight">\(\textbf{x}\)</span>, and <span class="math notranslate nohighlight">\(\textbf{z}\)</span> would be the same as the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto the subspace spanned by <span class="math notranslate nohighlight">\(\textbf{1}\)</span> and <span class="math notranslate nohighlight">\(\textbf{x}\)</span>.</p>
<a class="reference internal image-reference" href="../../_images/dependent_variablesz.png"><img alt="dependent_variablesz.png" class="align-center" src="../../_images/dependent_variablesz.png" style="width: 500px;" /></a>
<p>We can also observe this from minimizing the loss function:</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
\begin{aligned}
L(\boldsymbol\theta, \textbf{d}, \textbf{y})
&amp;= \left \Vert  \qquad   
\begin{bmatrix} y_1 \\ y_2 \\ y_3  \end{bmatrix} \quad - \quad 
\begin{bmatrix} 1 &amp; x_1 &amp; z_1 \\ 1 &amp; x_2 &amp; z_2\\ 1 &amp; x_3 &amp; z_3\end{bmatrix}
\begin{bmatrix} 
     \theta_0 \\
     \theta_1 \\
     \theta_2
\end{bmatrix}
\qquad \right \Vert ^2
\end{aligned}
\end{split}\]</div>
<p>Our possible solutions follow the form <span class="math notranslate nohighlight">\(\theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 \textbf{z}\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\textbf{z} = \textbf{1} + \textbf{x}\)</span>, regardless of <span class="math notranslate nohighlight">\(\theta_0\)</span>, <span class="math notranslate nohighlight">\(\theta_1\)</span>, and <span class="math notranslate nohighlight">\(\theta_2\)</span>, the possible values can be rewritten as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\theta_0 \textbf{1} + \theta_1 \textbf{x} + \theta_2 (\textbf{1} + \textbf{x})
&amp;= 
(\theta_0 + \theta_2) \textbf{1} + (\theta_1 + \theta_2) \textbf{x} \\
\end{aligned}
\end{split}\]</div>
<p>So adding <span class="math notranslate nohighlight">\(\textbf{z}\)</span> does not change the problem at all. The only difference is, we can express this projection in multiple ways. Recall that we found the projection of <span class="math notranslate nohighlight">\(\textbf{y}\)</span> onto the plane spanned by <span class="math notranslate nohighlight">\(\textbf{1}\)</span> and <span class="math notranslate nohighlight">\(\textbf{x}\)</span> to be:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} \textbf{1} &amp; \textbf{x} \end{bmatrix}  \begin{bmatrix} - \frac{3}{13} \\ \frac{11}{13} \end{bmatrix} = - \frac{3}{13} \textbf{1} + \frac{11}{13} \textbf{x}\end{split}\]</div>
<p>However, with the introduction of <span class="math notranslate nohighlight">\(\textbf{z}\)</span>, we have more ways to express this same projection vector.</p>
<p>Since <span class="math notranslate nohighlight">\(\textbf{1} = \textbf{z} - \textbf{x}\)</span>, <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> can also be expressed as:</p>
<div class="math notranslate nohighlight">
\[ - \frac{3}{13} (\textbf{z} - \textbf{x}) + \frac{11}{13} \textbf{x} = - \frac{3}{13} \textbf{z} + \frac{14}{13} \textbf{x} \]</div>
<p>Since <span class="math notranslate nohighlight">\(\textbf{x} = \textbf{z} + \textbf{1}\)</span>, <span class="math notranslate nohighlight">\(\hat{\textbf{y}}\)</span> can also be expressed as:</p>
<div class="math notranslate nohighlight">
\[ - \frac{3}{13} \textbf{1} + \frac{11}{13} (\textbf{z} + \textbf{1}) = \frac{8}{13} \textbf{1} + \frac{11}{13} \textbf{z} \]</div>
<p>But all three expressions represent the same projection.</p>
<p>In conclusion, adding a linearly dependent column to <span class="math notranslate nohighlight">\(\textbf{X}\)</span> does not change <span class="math notranslate nohighlight">\(Span(\textbf{X})\)</span>, and thus will not change the projection and solution to the least squares problem.</p>
</div>
<div class="section" id="two-schools-of-thought">
<h2><span class="section-number">13.4.7. </span>Two Schools of Thought<a class="headerlink" href="#two-schools-of-thought" title="Permalink to this headline">¶</a></h2>
<p>We included the scatter plots twice in this lesson. The first reminded us that like before, we are finding the best-fit line for the data. The second showed that there was no line that could fit all points. Apart from these two occurences, we tried not to disrupt our vector space drawings with scatter plots. This is because scatter plots correspond with the row-space perspective of the least squares problem: looking at each data point and trying to minimize the distance between our predictions and each datum. In this lesson, we looked at the column-space perspective: each feature was a vector, constructing a space of possible solutions (projections).</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/13"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="linear_multiple.html" title="previous page"><span class="section-number">13.3. </span>Multiple Linear Regression</a>
    <a class='right-next' id="next-link" href="linear_case_study.html" title="next page"><span class="section-number">13.5. </span>Linear Regression Case Study</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2020.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>