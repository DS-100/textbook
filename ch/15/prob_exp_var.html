
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>15.2. Expectation and Variance &#8212; Principles and Techniques of Data Science</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="15.3. Risk" href="prob_risk.html" />
    <link rel="prev" title="15.1. Random Variables" href="prob_random_vars.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  
  <h1 class="site-logo" id="site-title">Principles and Techniques of Data Science</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../preface.html">
   To the Reader
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../prereqs.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../notation.html">
   Notation
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  The Data Science Lifecycle
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_1.html">
     1.1. The Students of Data 100
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_2.html">
     1.2. Exploratory Data Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_students_3.html">
     1.3. What’s in a Name?
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../02/design_intro.html">
   2. Generalizing from Data
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_dewey_truman.html">
     2.1. Dewey Defeats Truman
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_data.html">
     2.2. Data Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_sampling.html">
     2.3. Probability Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/design_srs_vs_big_data.html">
     2.4. SRS vs. “Big Data”
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../03/modeling_intro.html">
   3. Modeling and Estimation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_simple.html">
     3.1. Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_loss_functions.html">
     3.2. Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/modeling_abs_huber.html">
     3.3. Absolute and Huber Loss
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/cycle_case_study_intro.html">
   4. [In progress] Case Study
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Rectangular Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../05/sql_intro.html">
   5. Relational Databases and SQL
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_rdbms.html">
     5.1. The Relational Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_basics.html">
     5.2. SQL
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/sql_joins.html">
     5.3. SQL Joins
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../06/pandas_intro.html">
   6. Data Tables in Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_indexes.html">
     6.1. Indexes, Slicing, and Sorting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_grouping_pivoting.html">
     6.2. Grouping and Pivoting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_apply_strings_plotting.html">
     6.3. Apply, Strings, and Plotting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_joins.html">
     6.4. [In progress] Joins
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Understanding The Data
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../07/repr_intro.html">
   7. Data Representation
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_structure.html">
     7.1. Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_data_types.html">
     7.2. Data Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/repr_granularity.html">
     7.3. Granularity
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/quality_intro.html">
   8. [In Progress] Data Quality
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/eda_intro.html">
   9. [In Progress] Exploratory Data Analysis
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../10/viz_intro.html">
   10. Data Visualization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_quantitative.html">
     10.1. Visualizing Quantitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_qualitative.html">
     10.2. Visualizing Qualitative Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_matplotlib.html">
     10.3. Customizing Plots using
     <code class="docutils literal notranslate">
      <span class="pre">
       matplotlib
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_principles.html">
     10.4. Visualization Principles
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_principles_2.html">
     10.5. Visualization Principles Continued
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/viz_philosophy.html">
     10.6. Philosophy for Data Visualization
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../11/police_intro.html">
   11. [In progress] Case Study: Berkeley Policing
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../11/police_calls.html">
     11.1. Cleaning the Calls Dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/police_stops.html">
     11.2. Cleaning The Stops Dataset
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Other Data Sources
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../12/text_intro.html">
   12. Working with Text
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_strings.html">
     12.1. Python String Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_regex.html">
     12.2. Regular Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/text_re.html">
     12.3. Regex and Python
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../13/web_intro.html">
   13. Web Technologies
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_http.html">
     13.1. HTTP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_rest.html">
     13.2. [In Progress] REST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/web_html.html">
     13.3. [In Progress] XPath and HTML
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Linear Modeling
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../14/linear_models.html">
   14. Linear Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_tips.html">
     14.1. Predicting Tip Amounts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_fitting.html">
     14.2. [In progress] Fitting a linear model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_inference.html">
     14.3. [In progress] Inference for Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/linear_projection.html">
     14.4. Least Squares — A Geometric Perspective
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="prob_and_gen.html">
   15. Probability and Generalization
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="prob_random_vars.html">
     15.1. Random Variables
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     15.2. Expectation and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="prob_risk.html">
     15.3. Risk
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../16/gradient_descent.html">
   16. Gradient Descent and Numerical Optimization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../16/gradient_basics.html">
     16.1. Loss Minimization Using a Program
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/gradient_descent_define.html">
     16.2. Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/gradient_convexity.html">
     16.3. Convexity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/gradient_stochastic.html">
     16.4. Stochastic Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/gradient_lin_reg.html">
     16.5. Fitting a Linear Model Using Gradient Descent
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../17/donkey_intro.html">
   17. [In progress] Case Study: Donkey Dimensions
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../17/donkey_analysis.html">
     17.1. Linear Regression Case Study
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Multiple Linear Modeling
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../18/mult_intro.html">
   18. [In progress] Multiple Linear Regression
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../18/mult_model.html">
     18.1. Multiple Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/mult_inference.html">
     18.2. Inference for Multiple Linear Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../19/feature_engineering.html">
   19. Feature Engineering
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../19/feature_one_hot.html">
     19.1. The Walmart dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/feature_polynomial.html">
     19.2. Predicting Ice Cream Ratings
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../20/bias_intro.html">
   20. The Bias-Variance Tradeoff
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_risk.html">
     20.1. Risk and Loss Minimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_modeling.html">
     20.2. Model Bias and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/bias_cv.html">
     20.3. Cross-Validation
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../21/reg_intro.html">
   21. Regularization
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_intuition.html">
     21.1. Regularization Intuition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_ridge.html">
     21.2. L2 Regularization: Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/reg_lasso.html">
     21.3. L1 Regularization: Lasso Regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../22/mult_case_intro.html">
   22. [In progress] Case Study: Multiple Linear Models
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../23/classification_intro.html">
   23. Classification
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_prob.html">
     23.1. Regression on Probabilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_log_model.html">
     23.2. The Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_cost.html">
     23.3. A Loss Function for the Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_log_reg.html">
     23.4. Using Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_cost_justification.html">
     23.5. Approximating the Empirical Probability Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_sgd.html">
     23.6. Fitting a Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_sensitivity_specificity.html">
     23.7. Evaluating Logistic Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../23/classification_multiclass.html">
     23.8. Multiclass Classification
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Replicability
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../24/repl_intro.html">
   24. [In progress] Replicable Research
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../24/repl_phacking.html">
     24.1. P-hacking
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Extra Topics
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../25/pca_intro.html">
   25. Dimensionality Reduction and PCA
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_dims.html">
     25.1. Dimensions of a Data Table
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_svd.html">
     25.2. PCA using the Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../25/pca_in_practice.html">
     25.3. PCA in Practice
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../26/dtrees_intro.html">
   26. [In progress] Decision Trees and Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../27/clustering_intro.html">
   27. [In progress] Clustering
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../a01/prob_review.html">
   Probability Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a02/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../a03/hyp_intro.html">
   Statistical Inference Review
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/hyp_introduction.html">
     Hypothesis Testing and Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a03/hyp_introduction_part2.html">
     Permutation Test
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../a04/ref_intro.html">
   Reference Tables
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_pandas.html">
     pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_seaborn.html">
     Seaborn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_matplotlib.html">
     matplotlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_sklearn.html">
     scikit-learn
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a05/contributors.html">
   Contributors
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/ch/15/prob_exp_var.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/ch/15/prob_exp_var.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#expectation">
   15.2.1. Expectation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linearity-of-expectation">
     15.2.1.1. Linearity of Expectation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#variance">
   15.2.2. Variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance">
     15.2.2.1. Covariance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bernoulli-random-variables">
   15.2.3. Bernoulli Random Variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sample-means">
   15.2.4. Sample Means
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   15.2.5. Summary
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="c1"># Ignore numpy dtype warnings. These warnings are caused by an interaction</span>
<span class="c1"># between numpy and Cython and can be safely ignored.</span>
<span class="c1"># Reference: https://stackoverflow.com/a/40846742</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.dtype size changed&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;numpy.ufunc size changed&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">nbinteract</span> <span class="k">as</span> <span class="nn">nbi</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s1">&#39;talk&#39;</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_rows</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">max_columns</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># This option stops scientific notation for pandas</span>
<span class="c1"># pd.set_option(&#39;display.float_format&#39;, &#39;{:.2f}&#39;.format)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="expectation-and-variance">
<h1><span class="section-number">15.2. </span>Expectation and Variance<a class="headerlink" href="#expectation-and-variance" title="Permalink to this headline">¶</a></h1>
<p>Although a random variable is completely described by its probability mass function (PMF), we often use <strong>expectation</strong> and <strong>variance</strong> to describe the variable’s long-run average and spread. These two values have unique mathematical properties that hold particular importance for data science—for example, we can show that an estimation is accurate in the long term by showing that its expected value is equal to the population parameter. We proceed by defining expectation and variance, introducing their most useful mathematical properties, and conclude with a brief application to estimation.</p>
<div class="section" id="expectation">
<h2><span class="section-number">15.2.1. </span>Expectation<a class="headerlink" href="#expectation" title="Permalink to this headline">¶</a></h2>
<p>We are often interested in the long-run average of a random variable because it gives us a sense of the center of the variable’s distribution. We call this long-run average the <strong>expected value</strong>, or the <strong>expectation</strong> of a random variable. The expected value of a random variable <span class="math notranslate nohighlight">\( X \)</span> is:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \sum_{x\in \mathbb{X}} x \cdot P(X = x)\]</div>
<p>For example, if <span class="math notranslate nohighlight">\( X \)</span> represents the roll of a single fair six-sided die,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[X]
&amp;= 1 \cdot P(X = 1) + 2 \cdot P(X = 2) + \ldots + 6 \cdot P(X = 6) \\
&amp;= 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + \ldots + 6 \cdot \frac{1}{6} \\
&amp;= 3.5
\end{aligned}
\end{split}\]</div>
<p>Notice that the expected value of <span class="math notranslate nohighlight">\( X \)</span> does not have to be a possible value of <span class="math notranslate nohighlight">\( X \)</span>. Although <span class="math notranslate nohighlight">\( \mathbb{E}[X] = 3.5 \)</span>, <span class="math notranslate nohighlight">\( X \)</span> cannot actually take on the value <span class="math notranslate nohighlight">\( 3.5 \)</span>.</p>
<p><strong>Example:</strong> Recall our dataset from the previous section:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;Name&quot;</span><span class="p">:[</span><span class="s2">&quot;Carol&quot;</span><span class="p">,</span><span class="s2">&quot;Bob&quot;</span><span class="p">,</span><span class="s2">&quot;John&quot;</span><span class="p">,</span><span class="s2">&quot;Dave&quot;</span><span class="p">],</span> <span class="s1">&#39;Age&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span><span class="mi">52</span><span class="p">,</span><span class="mi">51</span><span class="p">,</span><span class="mi">50</span><span class="p">]}</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">people</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Carol</td>
      <td>50</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bob</td>
      <td>52</td>
    </tr>
    <tr>
      <th>2</th>
      <td>John</td>
      <td>51</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Dave</td>
      <td>50</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We pick one person from this dataset uniformly at random. Let <span class="math notranslate nohighlight">\( Y \)</span> be a random variable representing the age of this person. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[Y]
&amp;= 50 \cdot P(Y = 50) + 51 \cdot P(Y = 51) + 52 \cdot P(Y = 52) \\
&amp;= 50 \cdot \frac{2}{4} + 51 \cdot \frac{1}{4} + 52 \cdot \frac{1}{4} \\
&amp;= 50.75
\end{aligned}
\end{split}\]</div>
<p><strong>Example:</strong> Suppose we sample two people from the dataset with replacement. If the random variable <span class="math notranslate nohighlight">\( Z \)</span> represents the difference between the ages of the first and second persons in the sample, what is  <span class="math notranslate nohighlight">\( \mathbb{E}[Z] \)</span>?</p>
<p>As in the previous section, we define <span class="math notranslate nohighlight">\(X\)</span> as the age of the first person and <span class="math notranslate nohighlight">\(Y\)</span> as the age of the second such that <span class="math notranslate nohighlight">\(Z = X - Y\)</span>. From the joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given in the previous section, we can find the PMF for <span class="math notranslate nohighlight">\( Z \)</span>. For example, <span class="math notranslate nohighlight">\( P(Z = 1) = P(X = 51, Y = 50) + P(X = 52, Y = 51) = \frac{3}{16} \)</span>. Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[Z]
&amp;= (-2) \cdot P(Z = -2) + (-1) \cdot P(Z = -1) + \ldots + (2) \cdot P(Z = 2) \\
&amp;= (-2) \cdot \frac{2}{16} + (-1) \cdot \frac{3}{16}+ \ldots + (2) \cdot \frac{2}{16} \\
&amp;= 0
\end{aligned}
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\( \mathbb{E}[Z] = 0 \)</span>, we expect that in the long run the difference between the ages of the people in a sample of size 2 will be 0.</p>
<div class="section" id="linearity-of-expectation">
<h3><span class="section-number">15.2.1.1. </span>Linearity of Expectation<a class="headerlink" href="#linearity-of-expectation" title="Permalink to this headline">¶</a></h3>
<p>When working with linear combinations of random variables as we did above, we can often make good use of the <strong>linearity of expectation</strong> instead of tediously calculating each joint probability individually.</p>
<p>The linearity of expectation states that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[X + Y] &amp;= \mathbb{E}[X] + \mathbb{E}[Y] \\
\end{aligned}
\end{split}\]</div>
<p>From this statement we may also derive:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[cX] &amp;= c\mathbb{E}[X] \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are random variables, and <span class="math notranslate nohighlight">\(c\)</span> is a constant.</p>
<p>In words, the expectation of a sum of any two random variables is equal to the sum of the expectations of the variables.</p>
<p>In the previous example, we saw that <span class="math notranslate nohighlight">\( Z = X - Y \)</span>. Thus,  <span class="math notranslate nohighlight">\( \mathbb{E}[Z] = \mathbb{E}[X - Y] = \mathbb{E}[X] - \mathbb{E}[Y] \)</span>.</p>
<p>Now we can calculate <span class="math notranslate nohighlight">\( \mathbb{E}[X] \)</span> and  <span class="math notranslate nohighlight">\( \mathbb{E}[Y] \)</span> separately from each other. Since <span class="math notranslate nohighlight">\( \mathbb{E}[X] = \mathbb{E}[Y] = 50.75 \)</span>, <span class="math notranslate nohighlight">\( \mathbb{E}[Z] = 50.75 - 50.75 = 0 \)</span>.</p>
<p>The linearity of expectation holds even if <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> are dependent on each other! As an example, let us again consider the case in which we sample two people from our small dataset in the previous section without replacement. As before, we define <span class="math notranslate nohighlight">\(X\)</span> as the age of the first person and <span class="math notranslate nohighlight">\(Y\)</span> as the age of the second, and <span class="math notranslate nohighlight">\(Z = X - Y\)</span>. Clearly, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are not independent—knowing <span class="math notranslate nohighlight">\( X = 52 \)</span>, for example, means that <span class="math notranslate nohighlight">\( Y \neq 52 \)</span>.</p>
<p>From the joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> given in the previous section, we can find <span class="math notranslate nohighlight">\(\mathbb{E}[Z]\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[Z]
&amp;= (-2) \cdot P(Z = -2) + (-1) \cdot P(Z = -1) + \ldots + (2) \cdot P(Z = 2) \\
&amp;= (-2) \cdot \frac{2}{12} + (-1) \cdot \frac{3}{12}+ \ldots + (2) \cdot \frac{2}{12} \\
&amp;= 0
\end{aligned}
\end{split}\]</div>
<p>A simpler way to compute this expectation is to use the linearity of expectation. Even though <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> dependent, <span class="math notranslate nohighlight">\(\mathbb{E}[Z] = \mathbb{E}[X - Y] = \mathbb{E}[X] - \mathbb{E}[Y]\)</span>. Recall from the previous section that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have the same PMF even though we are sampling without replacement, which means that <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mathbb{E}[Y] = 50.75\)</span>. Hence as in the first scenario, <span class="math notranslate nohighlight">\(\mathbb{E}[Z] = 0\)</span>.</p>
<p>Note that the linearity of expectation only holds for linear combinations of random variables. For example, <span class="math notranslate nohighlight">\( \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y] \)</span> is not a linear combination of <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span>. In this case, <span class="math notranslate nohighlight">\( \mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y] \)</span> is true in general only for independent random variables.</p>
</div>
</div>
<div class="section" id="variance">
<h2><span class="section-number">15.2.2. </span>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">¶</a></h2>
<p>The variance of a random variable is a numerical description of the variable’s spread. For a random variable <span class="math notranslate nohighlight">\( X \)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Var(X) &amp;= \mathbb{E}[(X - \mathbb{E}[X])^2] \\
\end{aligned}
\end{split}\]</div>
<p>The above formula states that the variance of <span class="math notranslate nohighlight">\( X \)</span> is the average squared distance from <span class="math notranslate nohighlight">\( X \)</span>‘s expected value.</p>
<p>With some algebraic manipulation that we omit for brevity, we may also equivalently write:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Var(X) &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\
\end{aligned}
\end{split}\]</div>
<p>Consider the following two random variables <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> with the following probability distributions:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_pmf</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">rv_name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">val_name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">prob_denom</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">ms</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">mec</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="si">{</span><span class="n">val_name</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;$P(</span><span class="si">{</span><span class="n">rv_name</span><span class="si">}</span><span class="s1"> = </span><span class="si">{</span><span class="n">val_name</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prob_denom</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
               <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">]</span>
               <span class="o">+</span> <span class="p">[</span><span class="sa">rf</span><span class="s1">&#39;$\frac</span><span class="se">{{</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="se">}}{{</span><span class="si">{</span><span class="n">prob_denom</span><span class="si">}</span><span class="se">}}</span><span class="s1">$&#39;</span>
                  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prob_denom</span><span class="p">)]</span>
               <span class="o">+</span> <span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PMF of $</span><span class="si">{</span><span class="n">rv_name</span><span class="si">}</span><span class="s1">$&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_pmf</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_pmf</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="n">rv_name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">val_name</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/prob_exp_var_13_0.png" src="../../_images/prob_exp_var_13_0.png" />
</div>
</div>
<p><span class="math notranslate nohighlight">\( X \)</span> takes on values -1 and 1 with probability <span class="math notranslate nohighlight">\( \frac{1}{2} \)</span> each. <span class="math notranslate nohighlight">\( Y \)</span> takes on values -2, -1, 1, and 2 with probability <span class="math notranslate nohighlight">\( \frac{1}{4} \)</span> each. We find that <span class="math notranslate nohighlight">\( \mathbb{E}[X] = \mathbb{E}[Y] = 0 \)</span>. Since <span class="math notranslate nohighlight">\( Y \)</span>‘s distribution has a higher spread than <span class="math notranslate nohighlight">\( X \)</span>‘s, we expect that <span class="math notranslate nohighlight">\( Var(Y) \)</span> is larger than <span class="math notranslate nohighlight">\( Var(X) \)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Var(X)
&amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\
&amp;= \mathbb{E}[X^2] - 0^2 \\
&amp;= \mathbb{E}[X^2] \\
&amp;= (-1)^2 P(X = -1) + (1)^2 P(X = 1) \\
&amp;= 1 \cdot 0.5 + 1 \cdot 0.5 \\
&amp;= 1 \\\\
Var(Y)
&amp;= \mathbb{E}[Y^2] - \mathbb{E}[Y]^2 \\
&amp;= \mathbb{E}[Y^2] - 0^2 \\
&amp;= \mathbb{E}[Y^2] \\
&amp;= (-2)^2 P(Y = -2) + (-1)^2 P(Y = -1) + (1)^2 P(Y = 1) + (2)^2 P(Y = 2) \\
&amp;= 4 \cdot 0.25 + 1 \cdot 0.25 + 1 \cdot 0.25 + 4 \cdot 0.25\\
&amp;= 2.5
\end{aligned}
\end{split}\]</div>
<p>As expected, the variance of <span class="math notranslate nohighlight">\( Y \)</span> is greater than the variance of <span class="math notranslate nohighlight">\( X \)</span>.</p>
<p>The variance has a useful property to simplify some calculations. If <span class="math notranslate nohighlight">\( X \)</span> is a random variable:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
Var(aX + b) &amp;= a^2 Var(X)
\end{aligned}
\]</div>
<p>If two random variables <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> are independent:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
Var(X + Y) = Var(X) + Var(Y)
\end{aligned}
\]</div>
<p>Note that the linearity of expectation holds for any <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> even if they are dependent. However, <span class="math notranslate nohighlight">\( Var(X + Y) = Var(X) + Var(Y) \)</span> holds only when <span class="math notranslate nohighlight">\( X \)</span> and <span class="math notranslate nohighlight">\( Y \)</span> are <strong>independent</strong>.</p>
<div class="section" id="covariance">
<h3><span class="section-number">15.2.2.1. </span>Covariance<a class="headerlink" href="#covariance" title="Permalink to this headline">¶</a></h3>
<p>The covariance of two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
Cov(X, Y) &amp;= \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
\end{aligned}
\]</div>
<p>Again, we can perform some algebraic manipulation to obtain:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
Cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\end{aligned}
\]</div>
<p>Note that although the variance of a single random variable must be non-negative, the covariance of two random variables can be negative. In fact, the covariance helps measure the correlation between two random variables; the sign of the covariance helps us determine whether two random variables are positively or negatively correlated. If two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, then <span class="math notranslate nohighlight">\(Cov(X, Y) = 0\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]\)</span>.</p>
</div>
</div>
<div class="section" id="bernoulli-random-variables">
<h2><span class="section-number">15.2.3. </span>Bernoulli Random Variables<a class="headerlink" href="#bernoulli-random-variables" title="Permalink to this headline">¶</a></h2>
<p>Suppose we want to use a random variable <span class="math notranslate nohighlight">\(X\)</span> to a simulate a biased coin with <span class="math notranslate nohighlight">\(P(Heads) = p\)</span>. We can say that <span class="math notranslate nohighlight">\(X = 1\)</span> if the coin flip is heads, and <span class="math notranslate nohighlight">\(X = 0\)</span> if the coin flip is tails. Therefore, <span class="math notranslate nohighlight">\(P(X = 1) = p\)</span>, and <span class="math notranslate nohighlight">\(P(X = 0) = 1 - p\)</span>. This type of binary random variable is called a Bernoulli random variable; we can calculate its expected value and variance as follows:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = 1 \times p + 0 \times (1 - p) = p\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Var(X) &amp;= \mathbb{E}[X^2] - \mathbb{E}[X]^2 \\
&amp;= 1^2 \times p + 0^2 \times (1 - p) - p^2 \\
&amp;= p - p^2 \\
&amp;= p(1 - p)
\end{aligned}
\end{split}\]</div>
</div>
<div class="section" id="sample-means">
<h2><span class="section-number">15.2.4. </span>Sample Means<a class="headerlink" href="#sample-means" title="Permalink to this headline">¶</a></h2>
<p>Suppose we possess a biased coin with <span class="math notranslate nohighlight">\(P(Heads) = p\)</span> and we would like to estimate <span class="math notranslate nohighlight">\( p \)</span>. We can flip the coin <span class="math notranslate nohighlight">\( n \)</span> times to collect a sample of flips and calculate the proportion of heads in our sample, <span class="math notranslate nohighlight">\( \hat p \)</span>. If we know that <span class="math notranslate nohighlight">\( \hat p \)</span> is often close to <span class="math notranslate nohighlight">\( p \)</span>, we can use <span class="math notranslate nohighlight">\( \hat p \)</span> as an <strong>estimator</strong> for <span class="math notranslate nohighlight">\( p \)</span>.</p>
<p>Notice that <span class="math notranslate nohighlight">\( p \)</span> is <em>not</em> a random quantity; it is a fixed value based on the bias of the coin. <span class="math notranslate nohighlight">\( \hat p \)</span>, however, is a random quantity since it is generated from the random outcomes of flipping the coin. Thus, we can compute the expectation and variance of <span class="math notranslate nohighlight">\( \hat p \)</span> to precisely understand how well it estimates <span class="math notranslate nohighlight">\( p \)</span>.</p>
<p>To compute <span class="math notranslate nohighlight">\( \mathbb{E}[\hat p] \)</span>, we will first define random variables for each flip in the sample. Let <span class="math notranslate nohighlight">\(X_i\)</span> be a Bernoulli random variable for the <span class="math notranslate nohighlight">\(i^{th}\)</span> coin flip. Then, we know that:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\hat p = \frac{X_1 + X_2 + \ldots + X_n}{n}
\end{aligned}
\]</div>
<p>To calculate the expectation of <span class="math notranslate nohighlight">\( \hat p \)</span>, we can plug in the formula above and use the fact that <span class="math notranslate nohighlight">\( \mathbb{E}[X_i] = p \)</span> since <span class="math notranslate nohighlight">\( X_i \)</span> is a Bernoulli random variable.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[\hat p] 
&amp;= \mathbb{E} \left[ \frac{X_1 + X_2 + \ldots + X_n}{n} \right] \\
&amp;= \frac{1}{n} \mathbb{E}[X_1 + \ldots + X_n] \\
&amp;= \frac{1}{n} \left( \mathbb{E}[X_1] +  \ldots + \mathbb{E}[X_n] \right) \\
&amp;= \frac{1}{n} (p + \ldots + p) \\
&amp;= \frac{1}{n} (np) \\
\mathbb{E}[\hat p] &amp;= p
\end{aligned}
\end{split}\]</div>
<p>We find that <span class="math notranslate nohighlight">\( \mathbb{E}[\hat p] = p \)</span>. In other words, with enough flips we expect our estimator <span class="math notranslate nohighlight">\( \hat p \)</span> to converge to the true coin bias <span class="math notranslate nohighlight">\( p \)</span>. We say that <span class="math notranslate nohighlight">\( \hat p \)</span> is an <strong>unbiased estimator</strong> of <span class="math notranslate nohighlight">\( p \)</span>.</p>
<p>Next, we calculate the variance of <span class="math notranslate nohighlight">\( \hat p \)</span>. Since each flip is independent from the others, we know that <span class="math notranslate nohighlight">\( X_i \)</span> are independent. This allows us to use the linearity of variance.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Var(\hat p) &amp;= Var \left(\frac{1}{n} \sum_{i=1}^{n} X_i \right) \\
&amp;= \frac{1}{n^2} \sum_{i=1}^{n}Var(X_i) \\
&amp;= \frac{1}{n^2} \times np(1-p) \\
Var(\hat p) &amp;= \frac{p(1-p)}{n}
\end{aligned}
\end{split}\]</div>
<p>From the equivalence above, we see that the variance of our estimator decreases as we increase <span class="math notranslate nohighlight">\( n \)</span>, the number of flips in our sample. In other words, if we collect lots of data we can be more certain about our estimator’s value. This behavior is known as the law of large numbers.</p>
</div>
<div class="section" id="summary">
<h2><span class="section-number">15.2.5. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>We use expectation and variance to provide simple descriptions of a random variable’s center and spread. These mathematical tools allow us to determine how well an quantity calculated from a sample estimates a quantity in the population.</p>
<p>Minimizing a loss function creates a model that is accurate on its training data. Expectation and variance allow us to make general statements about the model’s accuracy on unseen data from the population.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/15"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="prob_random_vars.html" title="previous page"><span class="section-number">15.1. </span>Random Variables</a>
    <a class='right-next' id="next-link" href="prob_risk.html" title="next page"><span class="section-number">15.3. </span>Risk</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
        
            &copy; Copyright 2021.<br/>
          <div class="extra_footer">
            <p>
License: CC BY-NC-ND 4.0
</p>

          </div>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-113006011-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>