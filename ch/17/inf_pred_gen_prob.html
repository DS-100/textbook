
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>17.6. Probability for Inference and Prediction &#8212; Learning Data Science</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/custom.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="17.7. Summary" href="inf_pred_gen_summary.html" />
    <link rel="prev" title="17.5. Basics of Prediction Intervals" href="inf_pred_gen_PI.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-113006011-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Learning Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Frontmatter
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../preface.html">
   Preface
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Data Science Lifecycle
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01/lifecycle_intro.html">
   1. The Data Science Lifecycle
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_cycle.html">
     1.1. The Stages of the Lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_map.html">
     1.2. Examples of the Lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/lifecycle_summary.html">
     1.3. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02/data_scope_intro.html">
   2. Questions and Data Scope
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_big_data_hubris.html">
     2.1. Big Data and New Opportunities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_construct.html">
     2.2. Target Population, Access Frame, Sample
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_protocols.html">
     2.3. Instruments and Protocols
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_natural.html">
     2.4. Measuring Natural Phenomenon
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_accuracy.html">
     2.5. Accuracy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/data_scope_summary.html">
     2.6. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03/theory_intro.html">
   3. Simulation and Data Design
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/theory_urn.html">
     3.1. The Urn Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/theory_election.html">
     3.2. Example: Simulating Election Poll Bias and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/theory_vaccine_efficacy.html">
     3.3. Example: Simulating a Randomized Trial for a Vaccine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/theory_measurement_error.html">
     3.4. Example: Measuring Air Quality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/theory_summary.html">
     3.5. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04/modeling_intro.html">
   4. Modeling with Summary Statistics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/modeling_simple.html">
     4.1. The Constant Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/modeling_loss_functions.html">
     4.2. Minimizing Loss
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/modeling_summary.html">
     4.3. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05/bus_intro.html">
   5. Case Study: Why is my Bus Always Late?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/bus_scope.html">
     5.1. Question and Scope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/bus_clean.html">
     5.2. Data Wrangling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/bus_eda.html">
     5.3. Exploring Bus Times
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/bus_modeling.html">
     5.4. Modeling Wait Times
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05/bus_summary.html">
     5.5. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Rectangular Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06/pandas_intro.html">
   6. Working With Dataframes Using pandas
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_subsetting.html">
     6.1. Subsetting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_aggregating.html">
     6.2. Aggregating
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_joining.html">
     6.3. Joining
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_transforming.html">
     6.4. Transforming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_other_reps.html">
     6.5. How are Dataframes Different from Other Data Representations?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/pandas_summary.html">
     6.6. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07/sql_intro.html">
   7. Working With Relations Using SQL
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/sql_subsetting.html">
     7.1. Subsetting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/sql_aggregating.html">
     7.2. Aggregating
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/sql_joining.html">
     7.3. Joining
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/sql_transforming.html">
     7.4. Transforming and Common Table Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/sql_summary.html">
     7.5. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Understanding The Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08/files_intro.html">
   8. Wrangling Files
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_datasets.html">
     8.1. Data Source Examples
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_formats.html">
     8.2. File Formats
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_encoding.html">
     8.3. File Encoding
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_size.html">
     8.4. File Size
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_command_line.html">
     8.5. The Shell and Command Line Tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_granularity.html">
     8.6. Table Shape and Granularity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/files_summary.html">
     8.7. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09/wrangling_intro.html">
   9. Wrangling Dataframes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_co2.html">
     9.1. Example: Wrangling CO2 Measurements from Mauna Loa Observatory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_checks.html">
     9.2. Quality Checks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_missing.html">
     9.3. Missing Values and Records
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_transformations.html">
     9.4. Transformations and Timestamps
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_structure.html">
     9.5. Modifying Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_restaurants.html">
     9.6. Example: Wrangling Restaurant Safety Violations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/wrangling_summary.html">
     9.7. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10/eda_intro.html">
   10. Exploratory Data Analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_feature_types.html">
     10.1. Feature Types
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_distributions.html">
     10.2. What to Look For in a Distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_relationships.html">
     10.3. What to Look For in a Relationship
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_multi.html">
     10.4. Comparisons in Multivariate Settings
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_guidelines.html">
     10.5. Guidelines for Exploration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_example.html">
     10.6. Example: Sale Prices for Houses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/eda_summary.html">
     10.7. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11/viz_intro.html">
   11. Data Visualization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_scale.html">
     11.1. Choosing Scale to Reveal Structure
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_smoothing.html">
     11.2. Smoothing and Aggregating Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_comparisons.html">
     11.3. Facilitating Meaningful Comparisons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_data_design.html">
     11.4. Incorporating the Data Design
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_context.html">
     11.5. Adding Context
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_plotly.html">
     11.6. Creating Plots Using
     <code class="docutils literal notranslate">
      <span class="pre">
       plotly
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_other_tools.html">
     11.7. Other Tools for Visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11/viz_summary.html">
     11.8. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12/pa_intro.html">
   12. Case Study: How Accurate are Air Quality Measurements?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_scope.html">
     12.1. Question, Design, and Scope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_collocated.html">
     12.2. Finding Collocated Sensors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_cleaning_aqs.html">
     12.3. Wrangling and Cleaning AQS Sensor Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_cleaning_purpleair.html">
     12.4. Wrangling PurpleAir Sensor Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_eda.html">
     12.5. Exploring PurpleAir and AQS Measurements
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_modeling.html">
     12.6. Creating a Model to Correct PurpleAir Measurements
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12/pa_conclusion.html">
     12.7. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other Data Sources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13/text_intro.html">
   13. Working with Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/text_examples.html">
     13.1. Examples of Text and Tasks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/text_strings.html">
     13.2. String Manipulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/text_regex.html">
     13.3. Regular Expressions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/text_sotu.html">
     13.4. Text Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13/text_summary.html">
     13.5. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14/web_intro.html">
   14. Data Exchange
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_netCDF.html">
     14.1. NetCDF Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_json.html">
     14.2. JSON Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_http.html">
     14.3. HTTP
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_rest.html">
     14.4. REST
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_html.html">
     14.5. XML, HTML, and XPath
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14/web_summary.html">
     14.6. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear Modeling
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15/linear_intro.html">
   15. Linear Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_simple.html">
     15.1. Simple Linear Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_pa.html">
     15.2. Example: A Simple Linear Model for Air Quality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_simple_fit.html">
     15.3. Fitting the Simple Linear Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_multi.html">
     15.4. Multiple Linear Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_multi_fit.html">
     15.5. Fitting the Multiple Linear Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_case.html">
     15.6. Example: Where is the Land of Opportunity?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_feature_eng.html">
     15.7. Feature Engineering for Numeric Measurements
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_categorical.html">
     15.8. Feature Engineering for Categorical Measurements
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15/linear_summary.html">
     15.9. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16/ms_intro.html">
   16. Model Selection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_overfitting.html">
     16.1. Overfitting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_train_test.html">
     16.2. Train-Test Split
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_cv.html">
     16.3. Cross-Validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_regularization.html">
     16.4. Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_risk.html">
     16.5. Model Bias and Variance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16/ms_summary.html">
     16.6. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="inf_pred_gen_intro.html">
   17. Theory for Inference and Prediction
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_dist.html">
     17.1. Distributions: Population, Empirical, Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_HT.html">
     17.2. Basics of Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_boot.html">
     17.3. Bootstrapping for Inference
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_CI.html">
     17.4. Basics of Confidence Intervals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_PI.html">
     17.5. Basics of Prediction Intervals
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     17.6. Probability for Inference and Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="inf_pred_gen_summary.html">
     17.7. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18/donkey_intro.html">
   18. Case Study: How to Weigh a Donkey
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/donkey_scope.html">
     18.1. Donkey Study Question and Scope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/donkey_clean.html">
     18.2. Wrangling and Transforming
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/donkey_eda.html">
     18.3. Exploring
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/donkey_model.html">
     18.4. Modeling a Donkey’s Weight
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18/donkey_summary.html">
     18.5. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../19/class_intro.html">
   19. Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_example.html">
     19.1. Example: Wind Damaged Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_pred.html">
     19.2. Modeling and Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_log_model.html">
     19.3. Modeling Proportions (and Probabilities)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_loss.html">
     19.4. A Loss Function for the Logistic Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_dr.html">
     19.5. From Probabilities to Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../19/class_summary.html">
     19.6. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../20/gd_intro.html">
   20. Numerical Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/gd_basics.html">
     20.1. Gradient Descent Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/gd_example.html">
     20.2. Minimizing Huber Loss
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/gd_convex.html">
     20.3. Convex and Differentiable Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/gd_alternative.html">
     20.4. Variants of Gradient Descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../20/gd_summary.html">
     20.5. Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../21/fake_news_intro.html">
   21. Case Study: Detecting Fake News
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/fake_news_question.html">
     21.1. Question and Scope
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/fake_news_data.html">
     21.2. Obtaining and Wrangling the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/fake_news_exploring.html">
     21.3. Exploring the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/fake_news_modeling.html">
     21.4. Modeling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../21/fake_news_summary.html">
     21.5. Summary
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../additional_resources.html">
   Additional Material
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_sources.html">
   Data Sources
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Appendices
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../a01/prob_review.html">
   Probability Review
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a02/vector_space_review.html">
   Vector Space Review
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../a04/ref_intro.html">
   Reference Tables
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_pandas.html">
     pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_seaborn.html">
     Seaborn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_matplotlib.html">
     matplotlib
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../a04/ref_sklearn.html">
     scikit-learn
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../a11/contributors.html">
   Contributors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../references.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A//github.com/ds-100/textbook&urlpath=tree/textbook/content/ch/17/inf_pred_gen_prob.ipynb&branch=master"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on JupyterHub"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="headerbtn__text-container">JupyterHub</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/ch/17/inf_pred_gen_prob.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formalizing-the-theory-for-average-rank-statistics">
   17.6.1. Formalizing the Theory for Average rank statistics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-properties-of-random-variables">
   17.6.2. General Properties of Random Variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-behind-testing-and-intervals">
   17.6.3. Probability Behind Testing and Intervals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-behind-model-selection">
   17.6.4. Probability Behind Model Selection
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Probability for Inference and Prediction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#formalizing-the-theory-for-average-rank-statistics">
   17.6.1. Formalizing the Theory for Average rank statistics
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-properties-of-random-variables">
   17.6.2. General Properties of Random Variables
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-behind-testing-and-intervals">
   17.6.3. Probability Behind Testing and Intervals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-behind-model-selection">
   17.6.4. Probability Behind Model Selection
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="probability-for-inference-and-prediction">
<span id="sec-theory-probintro"></span><h1><span class="section-number">17.6. </span>Probability for Inference and Prediction<a class="headerlink" href="#probability-for-inference-and-prediction" title="Permalink to this headline">#</a></h1>
<p>Hypothesis testing, confidence intervals, and prediction intervals rely on probability calculations computed from the sampling distribution and the data generation process. These probability frameworks also enable us to run simulation and bootstrap studies for a hypothetical survey, an experiment, or some other chance process in order to study its random behavior. For example, we found the sampling distribution for an average of ranks under the assumption that the treatment in a Wikipedia experiment was not effective. Using simulation, we quantified the typical deviations from the expected outcome and the distribution of the possible values for the summary statistic. The triptych in <a class="reference internal" href="inf_pred_gen_dist.html#triptych"><span class="std std-numref">Figure 1 17.1</span></a> provided a diagram to guide us in the process; it helped keep straight the differences between the population, probability, and sample and also showed their connections. In this section, we bring more mathematical rigor to these concepts.</p>
<p>We formally introduce the notions of expected value, standard deviation, and random variable, and we connect them to the concepts we have been using in this chapter for testing hypotheses and making confidence and prediction intervals. We begin with the specific example from the Wikipedia experiment, and then generalize. Along the way, we connect this formalism to the triptych that we have used as our guide throughout the chapter.</p>
<section id="formalizing-the-theory-for-average-rank-statistics">
<h2><span class="section-number">17.6.1. </span>Formalizing the Theory for Average rank statistics<a class="headerlink" href="#formalizing-the-theory-for-average-rank-statistics" title="Permalink to this headline">#</a></h2>
<p>Recall in the Wikipedia experiment, we pooled the post-award productivity values from the treatment and control groups and converted them into ranks, <span class="math notranslate nohighlight">\(1, 2, 3, \ldots, 200\)</span> so the population is simply made up of the integers from 1 to 200. <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a> is a diagram that represents this specific situation. Notice that the population distribution is flat and ranges from 1 to 200 (leftside of <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a>). Also, the population summary (called <em>population parameter</em>) we use is the average rank:</p>
<div class="math notranslate nohighlight">
\[
\theta^* ~=~ \text{Avg}(\text{pop}) ~=~  \frac{1}{200} \Sigma_{k=1}^{200} k ~=~ 100.5.
\]</div>
<p>Another relevant summary is the spread about <span class="math notranslate nohighlight">\(\theta^*\)</span>, defined as the population standard deviation:</p>
<div class="math notranslate nohighlight">
\[ 
\text{SD}(\text{pop}) ~=~ \sqrt{\frac {1}{200} \Sigma_{k=1}^{200} (k - \theta^*)^2} ~=~  
\sqrt{\frac {1}{200} \Sigma_{k=1}^{200} (k - 100.5)^2}
~\approx~ 57.7 
\]</div>
<p>The SD(pop) represents the typical deviation of a rank from the population average.   To calculate SD(pop) for this example takes some mathematical handiwork.</p>
<figure class="align-default" id="triptychrank">
<img alt="../../_images/TriptychRank.png" src="../../_images/TriptychRank.png" />
<figcaption>
<p><span class="caption-number">Fig. 17.3 </span><span class="caption-text">This diagram shows the population, sampling, and sample distributions and their summaries from the Wikipedia example. In this example, the population is known to consist of the integers from 1 to 200, and the sample are the ranks of the observed post-productivity measurements for the treatment group. In the middle, the sampling distribution of the average rank is created from a simulation study. Notice it is normal in shape with a center that matches the population average.</span><a class="headerlink" href="#triptychrank" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The observed sample consists of the integer ranks of the treatment group; we refer to these values as <span class="math notranslate nohighlight">\(k_1, k_2, \ldots, k_{100}.\)</span> The sample distribution appears on the right in <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a> (each of the 100 integers appears once).</p>
<p>The parallel to the population average is the sample average, which is our statistic of interest:</p>
<div class="math notranslate nohighlight">
\[
\text{Avg}(\text{sample}) ~=~  \frac{1}{100} \Sigma_{i=1}^{100} k_i ~=~ \bar{k} ~=~113.7.
\]</div>
<p>The <span class="math notranslate nohighlight">\(\text{Avg}(\text{sample})\)</span> is the observed value for <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. Similarly, the spread about <span class="math notranslate nohighlight">\(\text{Avg}(\text{sample})\)</span>, called the standard deviation of the sample, represents the typical deviation of a rank in the sample from the sample average:</p>
<div class="math notranslate nohighlight">
\[ 
\text{SD}(\text{sample}) ~=~ \sqrt{\frac {1}{100} \Sigma_{i=1}^{100} (k_i - \bar{k})^2} ~=~ 55.3.
\]</div>
<p>Notice the parallel between the definitions of the sample statistic and the population parameter, in the case where they are averages. The parallel between the two SDs is also note worthy.</p>
<p>Next we turn to the data generation process: draw 100 marbles from the urn (with values <span class="math notranslate nohighlight">\(1, 2,\ldots,200\)</span>), without replacement, to create the treatment ranks.  We represent the action of drawing the first marble from the urn and the integer that we get, by the capital letter <span class="math notranslate nohighlight">\(Z_1\)</span>. This <span class="math notranslate nohighlight">\(Z_1\)</span> is called a <em>random variable</em>. It has a probability distribution determined by the urn model. That is, we can list all of the values that <span class="math notranslate nohighlight">\(Z_1\)</span> might take and the probability associated with each:</p>
<div class="math notranslate nohighlight">
\[
{\mathbb{P}}(Z_1 = k) ~=~ \frac{1}{200} ~~~~\text{ for } k=1, \ldots, 200.
\]</div>
<p>In this example, the probability distribution of <span class="math notranslate nohighlight">\(Z_1\)</span> is determined by a simple formula because all of the integers are equally likely to be drawn from the urn.</p>
<p>We often summarize the distribution of a random variable by its <em>expected value</em> and <em>standard deviation</em>. Like with the population and sample, these two quantities give us a sense of what to expect as an outcome and how far the actual value might be from what is expected.</p>
<p>For our example, the expected value of <span class="math notranslate nohighlight">\(Z_1\)</span> is simply,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[Z_1]
&amp;= 1 \mathbb{P}(Z_1 = 1) + 2 \mathbb{P}(Z_1 = 2) +  \cdots + 200 \mathbb{P}(Z_1 = 200) \\
&amp;= 1 \times \frac{1}{200} + 2 \times \frac{1}{200} + \cdots + 200 \times \frac{1}{200} \\
&amp;= 100.5
\end{aligned}
\end{split}\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(\mathbb{E}[Z_1] = \theta^*\)</span>, the population average from the urn. The average value in a population and the expected value of a random variable that represents one draw at random  from an urn that contains the population are always the same. This is more easily seen by expressing the population average as a weighted average of the unique values in the population weighted by the fraction of units that have that value. The expected value of a random variable of a draw at random from the population urn uses the exact same weights because they match the chance of selecting the particular value.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The term <em>expected value</em> can be a bit confusing because it need not be a possible value of the random variable. For example, <span class="math notranslate nohighlight">\(\mathbb{E}[Z_1] = 100.5\)</span>, but only integers are possible values for <span class="math notranslate nohighlight">\(Z_1\)</span>.</p>
</div>
<p>Next, the variance of <span class="math notranslate nohighlight">\(Z_1\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{V}(Z_1) &amp;= \mathbb{E}[Z_1 - \mathbb{E}(Z_1)]^2 \\
&amp;= [1 - \mathbb{E}(Z_1)]^2 \mathbb{P}(Z_1 = 1) + \cdots + [200 - \mathbb{E}(Z_1)]^2  \mathbb{P}(Z_1 = 200) \\
&amp;= (1 - 100.5)^2 \times \frac{1}{200} + \cdots + (200 - 100.5)^2  \times \frac{1}{200} \\
&amp;= 3333.25
\end{aligned}
\end{split}\]</div>
<p>Additionally,</p>
<div class="math notranslate nohighlight">
\[
\text{SD}(Z_1) = \sqrt{\mathbb{V}(Z_1)} = 57.7
\]</div>
<p>We again point out that the standard deviation of <span class="math notranslate nohighlight">\(Z_1 \)</span> matches the <span class="math notranslate nohighlight">\(\text{SD}(pop)\)</span>.</p>
<p>To describe the entire data generation process in <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a>, we also define, <span class="math notranslate nohighlight">\(Z_2 , Z_3, \ldots, Z_{100}\)</span> as the result of the remaining 99 draws from the urn. By symmetry these random variables should all have the same probability distribution. That is, for any <span class="math notranslate nohighlight">\(k = 1, \ldots, 200\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Z_1 = k) ~=~ \mathbb{P}(Z_2 = k) ~=~ \cdots ~=~ \mathbb{P}(Z_{100} = k) ~=~ \frac{1}{200}.
\]</div>
<p>This implies that each <span class="math notranslate nohighlight">\(Z_i\)</span> has the same expected value, 100.5, and standard deviation, 57.7.  However, these random variables are not independent. For example, if you know that <span class="math notranslate nohighlight">\(Z_1 = 17\)</span>, then it is not possible for <span class="math notranslate nohighlight">\(Z_2 = 17\)</span>.</p>
<p>To complete the middle portion of <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a>, which involves the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, we express the average rank statistic as follows:</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta} = \frac{1}{100} \Sigma_{i=1}^{100} Z_i
\]</div>
<p>We can use the expected value and SD of <span class="math notranslate nohighlight">\(Z_1\)</span> and our knowledge of the data generation process to find the expected value and SD of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. However, we need some more information about how combinations of random variables behave so we first present the results and then circle back to explain why. We first find the expected value of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}(\hat{\theta}) ~&amp;=~ \mathbb{E}\left[\frac{1}{100} \Sigma_{i=1}^{100} Z_i\right]\\
~&amp;=~ \frac{1}{100} \Sigma_{i=1}^{100} \mathbb{E}[Z_i] \\
~&amp;=~  100.5 \\
~&amp;=~ \theta^*
\end{aligned}
\end{split}\]</div>
<p>In other words, the expected value of the average of random draws from the population equals the population average. Below we provide formulas for the variance of the average in terms of the population variance, as well as the SD.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{V}(\hat{\theta}) ~&amp;=~ \mathbb{V}\left[\frac{1}{100} \Sigma_{i=1}^{100} Z_i\right]\\
 ~&amp;=~ \frac{200-100}{100-1} \times \frac{\mathbb{V}(Z_i)}{100} \\
 ~&amp;=~ 16.75 \\
 ~&amp;~\\
 \text{SD}(\hat{\theta}) ~&amp;=~ \sqrt{\frac{100}{199}} \frac{\text{SD}(Z_1)}{10} \\
 ~&amp;=~ 4.1 
\end{aligned}
\end{split}\]</div>
<p>These computations relied on several properties of expected value and variance of a random variable and sums of random variables. Next, we provide properties of sums and averages of random variables that be used to derive the formulas we just presented.</p>
</section>
<section id="general-properties-of-random-variables">
<h2><span class="section-number">17.6.2. </span>General Properties of Random Variables<a class="headerlink" href="#general-properties-of-random-variables" title="Permalink to this headline">#</a></h2>
<p>In general, a <em>random variable</em> represents a numeric outcome of a chance event. In this book, we use capital letters like <span class="math notranslate nohighlight">\(X\)</span> or <span class="math notranslate nohighlight">\(Y\)</span> or <span class="math notranslate nohighlight">\(Z\)</span> to denote a random variable. The probability distribution for <span class="math notranslate nohighlight">\(X\)</span> is the specification, <span class="math notranslate nohighlight">\(\mathbb{P}(X = x) = p_x\)</span> for all values <span class="math notranslate nohighlight">\(x\)</span> that the random variable takes on.</p>
<p>Then, the expected value of <span class="math notranslate nohighlight">\(X\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \sum_{x} x p_x,\]</div>
<p>the variance <span class="math notranslate nohighlight">\(X\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{V}(X) ~&amp;=~ \mathbb{E}[(X - \mathbb{E}[X])^2] \\
 ~&amp;=~ \sum_{x} [x - \mathbb{E}(X)]^2  p_x,
\end{aligned}
\end{split}\]</div>
<p>and, the <span class="math notranslate nohighlight">\(\text{SD}(X)\)</span> is the square-root of <span class="math notranslate nohighlight">\(\mathbb{V}(X)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although random variables can represent either discrete (such as the number of children in a family drawn at random from a population) or continuous (such as the air quality measured by an air monitor) quantities, we address only random variables with discrete outcomes in this book. Since most measurements are made to a certain degree of precision, this simplification doesn’t limit us too much.</p>
</div>
<p>Simple formulas provide the expected value, variance, and standard deviation when we make scale and shift changes to random variables, such as <span class="math notranslate nohighlight">\(a + bX\)</span> for constants <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}(a + bX) ~&amp;=~ a + b\mathbb{E}(X)  \\
\mathbb{V}(a + bX) ~&amp;=~ b^2\mathbb{V}(X) \\
SD(a + bX) ~&amp;=~ |b|SD(X) \\
\end{aligned}
\end{split}\]</div>
<p>To convince yourself that these formulas make sense, think about how a distribution changes if you add a constant <span class="math notranslate nohighlight">\(a\)</span> to each value or scale each value by <span class="math notranslate nohighlight">\(b\)</span>. Adding <span class="math notranslate nohighlight">\(a\)</span> to each value would simply shift the distribution, which in turn would shift the expected value but not change the size of the deviations about the expected value. On the other hand, scaling the values by, say, 2, would spread the distribution out and essentially double both the expected value and the deviations from the expected value.</p>
<p>We are also interested in the properties of the sum of two or more random variables. Let’s consider two random variables, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Then,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(a + bX + cY) ~=~ a + b\mathbb{E}(X) + c\mathbb{E}(Y).
\]</div>
<p>But, to find the variance of <span class="math notranslate nohighlight">\(a + bX + cY\)</span>, we need to know how <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> vary together, which is called the <em>joint distribution</em> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. The joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> assigns probabilities to combinations of their outcomes,</p>
<div class="math notranslate nohighlight">
\[ 
\mathbb{P}(X =x, Y=y) ~=~ p_{x,y} 
\]</div>
<p>A summary of how <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> vary together, called the covariance, is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
Cov(X, Y) ~&amp;=~ \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] \\
 ~&amp;=~ \mathbb{E}[(XY) - \mathbb{E}(X)\mathbb{E}(Y)] \\
 ~&amp;=~ \Sigma{x,y}[(xy) - \mathbb{E}(X)\mathbb{E}(Y)]p_{x,y} 
 \end{aligned}
\end{split}\]</div>
<p>The covariance enters into the calculation of <span class="math notranslate nohighlight">\(\mathbf(a + bX + cY)\)</span>, as shown below:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}(a + bX + cY) ~=~ b^2\mathbb{V}(X) + 2bcCov(X,Y) + c^2\mathbb{V}(Y)
\]</div>
<p>In the special case where <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent, their joint distribution is simplified to  <span class="math notranslate nohighlight">\(p_{x,y} = p_x p_y\)</span>. And, in this case, <span class="math notranslate nohighlight">\(Cov(X,Y) = 0\)</span> so</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}(a + bX + cY) ~=~ b^2\mathbb{V}(X) + c^2\mathbb{V}(Y)
\]</div>
<p>These properties can be used to show that for random variables, <span class="math notranslate nohighlight">\(X_1, X_2, \ldots X_n\)</span>, that are independent with expected value <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, the average, <span class="math notranslate nohighlight">\(\bar{X}\)</span>, has the following expected value, variance, and standard deviation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}(\bar{X}) ~&amp;=~ \mu\\
\mathbb{V}(\bar{X}) ~&amp;=~ \sigma^2 /n\\
SD(\bar{X}) ~&amp;=~ \sigma/\sqrt{n}
\end{aligned}
\end{split}\]</div>
<p>This situation arises with the urn model where <span class="math notranslate nohighlight">\(X_1, \ldots,X_n\)</span> are the result of random draws with replacement. In this case, <span class="math notranslate nohighlight">\(\mu\)</span> represents the average of the urn and <span class="math notranslate nohighlight">\(\sigma\)</span> the standard deviation.</p>
<p>However, when we make random draws from the urn without replacement, the <span class="math notranslate nohighlight">\(X_i\)</span> are not independent. In this situation, <span class="math notranslate nohighlight">\(\bar{X}\)</span> has the following expected value and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}(\bar{X}) ~&amp;=~ \mu\\
\mathbb{V}(\bar{X}) ~&amp;=~ \frac{N-n}{N-1} \times \frac{\sigma^2}{n}\\
\end{aligned}
\end{split}\]</div>
<p>Notice that while the expected value is the same as when the draws are without replacement, the variance and SD are smaller. These quantities are adjusted by <span class="math notranslate nohighlight">\((N-n)/(N-1)\)</span>, which is called the <em>finite population correction factor</em>.  We used this formula earlier to compute the <span class="math notranslate nohighlight">\(SD(\hat{\theta})\)</span> in our Wikipedia example.</p>
<p>Returning to <a class="reference internal" href="#triptychrank"><span class="std std-numref">Figure 17.3</span></a>, we see that the sampling distribution for <span class="math notranslate nohighlight">\(\bar{X}\)</span> in the center of the diagram has an expectation that matches the population average; the SD decreases like <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> but even faster because we are drawing without replacement; and the distribution is shaped like a normal curve.  We saw these properties earlier in our simulation study.</p>
<p>Now that we have outlined the general properties of random variables and their sums, we connect these ideas to testing, confidence, and prediction intervals.</p>
</section>
<section id="probability-behind-testing-and-intervals">
<h2><span class="section-number">17.6.3. </span>Probability Behind Testing and Intervals<a class="headerlink" href="#probability-behind-testing-and-intervals" title="Permalink to this headline">#</a></h2>
<p>As mentioned at the beginning of this chapter, probability is the underpinning behind conducting a hypothesis test, providing a confidence interval for an estimator and a prediction interval for a future observation.</p>
<p>We now have the technical machinery to explain these concepts, which we have carefully defined in this chapter without the use of formal technicalities. This time we present the results in terms of random variables and their distributions.</p>
<p>Recall that a hypothesis test relies on a null model which provides the probability distribution for the statistic, <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>. The tests we carried out were essentially computing (sometimes approximately) the following probability: given the assumptions of the null distribution,</p>
<div class="math notranslate nohighlight">
\[ \mathbb{P}(\hat{\theta} \geq \text{observed statistic}) \]</div>
<p>Often times, the random variable is normalized to make these computations easier and standard:</p>
<div class="math notranslate nohighlight">
\[ 
\mathbb{P}\left( \frac{\hat{\theta} - {\theta}^*}{SD(\hat{\theta})} \geq \frac{\text{observed stat}- \theta^*}{SD(\hat{\theta})}\right)\]</div>
<p>When, <span class="math notranslate nohighlight">\(SD(\hat{\theta})\)</span> is not known, we have approximated it via simulation or, when we have a formula for <span class="math notranslate nohighlight">\(SD(\hat{\theta})\)</span> in terms of <span class="math notranslate nohighlight">\(SD(pop)\)</span>, we substitute <span class="math notranslate nohighlight">\(SD(samp)\)</span> in for <span class="math notranslate nohighlight">\(SD(pop)\)</span>. This normalization is popular because it simplifies the null distribution. For example, if <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> has an approximate normal distribution than the normalized version will have a standard normal distribution with center 0 and SD of 1. These approximations are useful when a lot of hypothesis tests are being carried out, such as with A/B testing, for there is no need to simulate for every statistic because we can just use the normal curve probabilities.</p>
<p>The probability statement behind a confidence interval is quite similar to the probability calculations used in testing. In particular, to create a 95% confidence interval where the sampling distribution of the estimator is roughly normal, we standardize and use the probability,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}\left( \frac{|\hat{\theta} - \theta^*|}{SD(\hat{\theta})} \leq 1.96 \right) &amp;~=~ \mathbb{P}\left(\hat{\theta} - 1.96SD(\hat{\theta}) \leq \theta^* \leq \hat{\theta} + 1.96SD(\hat{\theta}) \right) \\
&amp;~\approx~ 0.95
\end{aligned}
\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is a random variable in the above probability statement and <span class="math notranslate nohighlight">\(\theta^*\)</span> is considered a fixed unknown parameter value. The confidence interval is created by substituting the observed statistic in for <span class="math notranslate nohighlight">\(\hat\theta\)</span> and calling it a 95% confidence interval:</p>
<div class="math notranslate nohighlight">
\[
\left[\text{observed stat} - 1.96SD(\hat{\theta}),~ \text{observed stat} + 1.96SD(\hat{\theta}) \right]
\]</div>
<p>Once the observed statistic is substituted in for the random variable, then we say that we are 95% confident that the interval we have created contains the true value <span class="math notranslate nohighlight">\(\theta^*\)</span>. In other words, in 100 cases where we compute an interval in this way, we expect 95 of them to cover the population parameter that we are estimating.</p>
<p>Next, we consider prediction intervals. The basic notion is to provide an interval that denotes the expected variation of a future observation about the estimator. In the simple case, where the statistic is <span class="math notranslate nohighlight">\(\bar{X}\)</span> and we have a hypothetical new observation <span class="math notranslate nohighlight">\(X_0\)</span> that has the same expected value, say <span class="math notranslate nohighlight">\(\mu\)</span>, and standard deviation, say <span class="math notranslate nohighlight">\(\sigma\)</span>, of the <span class="math notranslate nohighlight">\(X_i\)</span>, then we find the expected variation of the squared loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[(X_0 - \bar{X})^2] ~&amp;=~ \mathbb{E}\{[(X_0 - \mu) - (\bar{X} - \mu)]^2\} \\
 ~&amp;=~  \mathbb{V}(X_0) + \mathbb{V}(\bar{X}) \\
 ~&amp;=~  \sigma^2 + \sigma^2/n \\
 ~&amp;=~ \sigma\sqrt{1 + 1/n}
\end{aligned}
\end{split}\]</div>
<p>Notice there are two parts to the variation: one due to the variation of <span class="math notranslate nohighlight">\(X_0\)</span> and the other due to the approximation of <span class="math notranslate nohighlight">\(\mathbb{E}(X_0)\)</span> by <span class="math notranslate nohighlight">\(\bar{X}\)</span>.</p>
<p>In the case of more complex models, the variation in prediction also breaks down into two components: the inherent variation in the data about the model plus the variation in the sampling distribution due to the estimation of the model. Assuming the model is roughly correct,
we can express it as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y} ~=~ \textbf{X}\boldsymbol{\theta}^{*} + \boldsymbol{\epsilon},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^*\)</span> is a <span class="math notranslate nohighlight">\((p+1) \times 1\)</span> column vector, <span class="math notranslate nohighlight">\(\textbf{X}\)</span> is a <span class="math notranslate nohighlight">\(n \times (p+1)\)</span> design matrix, and <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> consists of <span class="math notranslate nohighlight">\(n\)</span> independent random variables that each has expected value 0 and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. In this equation, <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is a
vector of random variables, where the expected value of each variable is determined by the design matrix and the variance is <span class="math notranslate nohighlight">\(\sigma^2\)</span>. That is, the variation about the line is constant in that it does not change with <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>When we create prediction intervals in regression, they are given an <span class="math notranslate nohighlight">\(1 \times (p+1)\)</span> row vector of covariates, called <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Then, the prediction is <span class="math notranslate nohighlight">\(\mathbf{x}_0\boldsymbol{\hat{\theta}}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\theta}}\)</span> is the estimated parameter vector based on the original
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and design matrix <span class="math notranslate nohighlight">\(\textbf{X}\)</span>. The expected squared error in this prediction is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[(Y_0 - \mathbf{x_0} \boldsymbol{\hat{\theta}})^2] ~&amp;=~ 
\mathbb{E}\{[(Y_0 - \mathbf{x_0\boldsymbol{\theta}^{*} }) - (\mathbf{x_0}\boldsymbol{\hat{\theta}}  - \mathbf{x_0}\boldsymbol{\theta}^{*})]^2\} \\
 ~&amp;=~  \mathbb{V}(\epsilon_0) + \mathbb{V}(\mathbf{x_0}\boldsymbol{\hat{\theta}}) \\
 ~&amp;=~  \sigma^2 [1 + \mathbf{x}_0 (\textbf{X}^\top \textbf{X})^{-1} \mathbf{x}_0^\top] \\
\end{aligned}
\end{split}\]</div>
<p>We approximate the variance of <span class="math notranslate nohighlight">\(\epsilon\)</span> with the variance of the residuals from the least squares fit.</p>
<p>The prediction intervals we create using the normal curve rely on the additional assumption that the distribution of the errors is approximately normal. This is a stronger assumption than we  make for the confidence intervals. With confidence intervals, the probability distribution of <span class="math notranslate nohighlight">\(X_i\)</span> need not look normal for <span class="math notranslate nohighlight">\(\bar{X}\)</span> to have an approximate normal distribution. Similarly, the probability distribution of <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> in the linear model need not look normal for the estimator <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> to have an approximate normal distribution.</p>
<p>We also assume that the linear model is approximately correct when making these prediction intervals. In <a class="reference internal" href="../16/ms_intro.html#ch-risk"><span class="std std-numref">Chapter 16</span></a> we consider the case where the fitted model doesn’t match the model that has produced the data. We now have the technical machinery to derive the model bias-variance tradeoff introduced in that chapter. It’s very similar to the prediction interval derivation with a couple of small twists.</p>
</section>
<section id="probability-behind-model-selection">
<h2><span class="section-number">17.6.4. </span>Probability Behind Model Selection<a class="headerlink" href="#probability-behind-model-selection" title="Permalink to this headline">#</a></h2>
<p>In <a class="reference internal" href="../16/ms_intro.html#ch-risk"><span class="std std-numref">Chapter 16</span></a> we introduced model under- and over-fitting with mean square error.
We described a general set up where the data might be expressed as follows:</p>
<div class="math notranslate nohighlight">
\[ y = g(\mathbf{x}) + {\epsilon}.\]</div>
<p>The <span class="math notranslate nohighlight">\(\epsilon\)</span> are assumed to behave like random errors that have no trends or patterns, constant variance, and are independent of one another. The <em>signal</em> in the model is the function, <span class="math notranslate nohighlight">\(g()\)</span>. The data are the <span class="math notranslate nohighlight">\( (\mathbf{x}_i, y_i) \)</span> pairs, and we fit models by minimizing the MSE,</p>
<div class="math notranslate nohighlight">
\[
\min_{f \in \cal{F}} \frac{1}{n} \sum_{i = 1}^{n} (y_i - f(\mathbf{x}_i)^2.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\cal{F}\)</span> is the collection of models over which we are minimizing. This collection might be all polynomials of degree <span class="math notranslate nohighlight">\(m\)</span> or less, bent lines with a bend at point <span class="math notranslate nohighlight">\(k\)</span>, etc. Note that <span class="math notranslate nohighlight">\(g\)</span> doesn’t have to be in the collection of functions that we are using to fit a model.</p>
<p>Our goal in model selection is to land on a model that predicts a new observation well. For a new observation, we would like the expected loss to be small:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[ y_0 - f(\mathbf{x}_0)]^2 
\]</div>
<p>This expectation is with respect to the distribution of possible <span class="math notranslate nohighlight">\((\mathbf{x}_0, y_0)\)</span>, and is called <em>risk</em>. Since we don’t know the population distribution of <span class="math notranslate nohighlight">\( (\mathbf{x}_0 , y_0) \)</span>, we can’t calculate the risk, but we can approximate it by the average loss over the data we have collected:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}[ y_0 - f(\mathbf{x}_0)]^2 &amp; \approx 
 \frac{1}{n} \sum_{i = 1}^{n} (y_i - f(\mathbf{x}_i))^2 \\
\end{aligned}
\end{split}\]</div>
<p>This approximation goes by the name of <em>empirical risk</em>. But, hopefully you recognize it as mean square error (MSE).</p>
<p>We fit models by minimizing the empirical risk (or MSE) over all possible models, <span class="math notranslate nohighlight">\( \cal{F} = \{ f \} \)</span>,</p>
<div class="math notranslate nohighlight">
\[
\min_{f \in \cal{F}} \frac{1}{n} \sum_{i = 1}^{n}  (y_i - f(\mathbf{x}_i))^2 
\]</div>
<p>The fitted model is called <span class="math notranslate nohighlight">\( \hat{f} \)</span>, a slightly more general representation of the linear model <span class="math notranslate nohighlight">\( \textbf{X}\hat{\boldsymbol{\theta}}\)</span>. This technique is aptly called <em>empirical risk minimization</em>.</p>
<p>In <a class="reference internal" href="../16/ms_intro.html#ch-risk"><span class="std std-numref">Chapter 16</span></a>, we saw problems arise when we used the empirical risk to both fit a model and evaluate the risk for a new observation. Ideally, we want to estimate the risk (expected loss),</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{E}[ (y_0 - \hat{f}(\mathbf{x}_0))^2 ],
\end{aligned}
\]</div>
<p>where the expected value is over both the new observation <span class="math notranslate nohighlight">\( (\mathbf{x}_0, y_0) \)</span> and over <span class="math notranslate nohighlight">\( {\hat{f}} \)</span> (which involves the original data <span class="math notranslate nohighlight">\( (\textbf{x}_i, {y}_i) \)</span>, <span class="math notranslate nohighlight">\( i = 1, \ldots, n \)</span>.</p>
<p>To understand the problem, we decompose this risk into three parts representing the model bias, model variance, and the <em>irreducible error</em> from <span class="math notranslate nohighlight">\( \epsilon \)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E} &amp; [ y_0 -  \hat{f}(x_0)]^2 \\
 &amp; = \mathbb{E} [ g(x_0) + \epsilon_0 -  \hat{f}(x_0)]^2  &amp;\textrm{definition}~\textrm{of}~y_0\\
 &amp; = \mathbb{E} [ g(x_0) + \epsilon_0 - \mathbb{E}[\hat{f}(x_0)] + \mathbb{E}[\hat{f}(x_0)] -  \hat{f}(x_0)]^2 &amp;\textrm{adding}~ \pm \mathbb{E}[\hat{f}(x_0)] \\
 &amp; = \mathbb{E} [ g(x_0) - \mathbb{E}[\hat{f}(x_0)] -  (\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)]) + \epsilon_0]^2  &amp;\text{rearranging terms}\\
 &amp; = [ g(x_0) - \mathbb{E}[\hat{f}(x_0)]]^2 + \mathbb{E}[\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0)]]^2 + \sigma^2 &amp;\text{expanding the square} \\
 &amp; =  ~~~\text{model bias}^2 ~~~+~~~ \text{model variance} ~~+~~ \text{error}
\end{aligned}
\end{split}\]</div>
<p>To derive the equality labeled “expanding the square” above, we need to formally prove that the cross product terms in the expansion are all 0. This takes a bit of algebra and we don’t present it here. But the main idea is that the terms <span class="math notranslate nohighlight">\(\epsilon_0\)</span> and <span class="math notranslate nohighlight">\((\hat{f}(x_0) - \mathbb{E}[\hat{f}(x_0])\)</span> are independent and both have expected value 0.</p>
<ul class="simple">
<li><p>The first of the three terms in the final equation is model bias (squared). When, the signal, <span class="math notranslate nohighlight">\( g \)</span>, does not belong to the model space, we have model bias. If the model space can approximate <span class="math notranslate nohighlight">\(g\)</span> well, then the bias is small. Note that this term is not present in our prediction intervals because we assumed that there is no (or minimal) model bias.</p></li>
<li><p>The second term represents the variability in the fitted model that comes from the data. We have seen in earlier examples that high degree polynomials can over fit, and so vary a lot from one set of data to the next. The more complex the model space, the greater the variability in the fitted model.</p></li>
<li><p>Finally, the last term is the variability in the error, the <span class="math notranslate nohighlight">\(\epsilon_0\)</span>, which is dubbed the “irreducible error”. This error sticks around whether we have under fit with a simple model (high bias) or over fit with a complex model (high variance).</p></li>
</ul>
<p>This representation of the expected loss shows the bias-variance decomposition of a fitted model. Model selection aims to balance these two competing sources of error. The train-test split, cross-validation, and regularization introduced in <a class="reference internal" href="../16/ms_intro.html#ch-risk"><span class="std std-numref">Chapter 16</span></a> are techniques to either mimic the expected loss for a new observation or penalize a model from over fitting.</p>
<p>While we have covered a lot of theory in this chapter, we have attempted to tie it to the basics of the urn model and the three distributions: population, sample, and sampling. We wrap up the chapter with a few cautions to keep in mind when performing hypothesis tests and when making confidence or prediction intervals.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch/17"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="inf_pred_gen_PI.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">17.5. </span>Basics of Prediction Intervals</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="inf_pred_gen_summary.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">17.7. </span>Summary</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sam Lau, Joey Gonzalez, and Deb Nolan<br/>
  
      &copy; Copyright 2023.<br/>
    <div class="extra_footer">
      <p>
License: CC BY-NC-ND 4.0
</p>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>