{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares - A geometric perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we found the optimal coefficients for linear models by optimizing their loss functions with gradient descent. We also mentioned that least squares linear regression can be solved analytically. While gradient descent is practical, this geometric perspective will provide a deeper understanding of linear regression.\n",
    "\n",
    "A review of vectors and vector spaces is included at the bottom. We will assume familiarity with vector arithmetic, the 1-vector, span of a collection of vectors, and projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been tasked with finding a good linear model for the below data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | y |\n",
    "| - |:-|\n",
    "| 3 | 2 |\n",
    "| 0 | 1 |\n",
    "| -1 | -2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        [3,2],\n",
    "        [0,1],\n",
    "        [-1,-2]\n",
    "    ],\n",
    "    columns=['x', 'y']\n",
    ")\n",
    "\n",
    "sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the best model is one with the least error, and that the least squares error is an acceptable measure.\n",
    "\n",
    "### Least Squares: Constant Model\n",
    "Like we did with the tips dataset, let's start with the constant model: the model that only ever predicts a single number.\n",
    "\n",
    "$$ f = \\hat{\\theta} $$\n",
    "\n",
    "Our goal is to find the $\\hat{\\theta}$ that results in the line that minimizes the least squares loss:\n",
    "\n",
    "$$ L(\\hat{\\theta}, \\vec{y}) = \\sum_{i = 1}^{n}(y_i - \\hat{\\theta})^2\\\\ $$\n",
    "\n",
    "Recall that for the constant model, the minimizing $\\theta$ for the MSE cost is $\\bar{y}$, the average of the $y$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HIDDEN\n",
    "sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);\n",
    "plt.plot([-2, 4], [0.333, 0.333], color='b', linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our loss function is a sum of squares. The *L2*-norm for a vector is also a sum of squares, but with a square root: \n",
    "\n",
    "$$\\Vert \\vec{v} \\Vert = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let $y_i - \\hat{\\theta} = v_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{y}) \n",
    "&= v_1^2 + v_2^2 + \\dots + v_n^2 \\\\\n",
    "&= \\Vert \\vec{v} \\Vert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means our loss can be expressed as the *L2*-norm of some vector $v$, squared. We can express $v_i$ as $y_i - \\hat{\\theta} \\quad \\forall i \\in [1,n]$ so that in Cartesian notation, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\vec{v} \\quad &= \\quad \\begin{bmatrix} y_1 - \\hat{\\theta} \\\\ y_2 - \\hat{\\theta} \\\\ \\vdots \\\\ y_n - \\hat{\\theta} \\end{bmatrix} \\\\\n",
    "&= \\quad \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} \\hat{\\theta} \\\\ \\hat{\\theta} \\\\ \\hat{\\theta} \\end{bmatrix} \\\\\n",
    "&= \\quad \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\n",
    "\\hat{\\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So our loss function can be written as:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{y})\n",
    "\\quad &= \\quad \\left \\Vert  \\qquad   \n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}\n",
    "\\hat{\\theta}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "\\quad &= \\quad \\left \\Vert  \\qquad  \n",
    "\\vec{y} \n",
    "\\quad - \\quad \n",
    "\\hat{y}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression $\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\ \\hat{\\theta}$  is a scalar multiple of the columns of the $\\vec{1}$ vector, and is the result of our predictions, denoted $\\hat{y}$.\n",
    "\n",
    "This gives us a new perspective on what it means to minimize the least squares error.\n",
    "\n",
    "$\\vec{y}$ and $\\vec{1}$ are fixed, but $\\hat{\\theta}$ can take on any value, so $\\hat{y}$ can be any scalar multiple of $\\vec{1}$. We want to find $\\hat{\\theta}$ so that $\\hat{\\theta} \\ \\vec{1}$ is closest to $\\vec{y}$.\n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection__1dprojection.png\" width=\"300\" />\n",
    "\n",
    "The projection of $\\vec{y}$ onto $\\vec{1}$ is guaranteed to be the closest vector (see \"Vector Space Review\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares: Simple Linear Model\n",
    "Now, let's look at the simple linear regression model. This is strongly parallel to the constant model derivation, but be mindful of the differences and think about how you might generalize to multiple linear regression.\n",
    "\n",
    "The simple linear model is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_\\hat{\\theta} (x_i) \n",
    "&= \\hat{\\theta_0} + \\hat{\\theta_1} x_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our goal is to find the $\\hat{\\theta}$ that results in the line with the least squared error:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{x}, \\vec{y})\n",
    "&= \\sum_{i = 1}^{n}(y_i - f_\\hat{\\theta} (x_i))^2\\\\\n",
    "&= \\sum_{i = 1}^{n}(y_i - \\hat{\\theta_0} - \\hat{\\theta_1} x_i)^2\\\\\n",
    "&= \\sum_{i = 1}^{n}(y_i - \\begin{bmatrix} 1 & x_i \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix} ) ^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        [3,2],\n",
    "        [0,1],\n",
    "        [-1,-2]\n",
    "    ],\n",
    "    columns=['x', 'y']\n",
    ")\n",
    "\n",
    "sns.regplot(x='x', y='y', data=data, ci=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us visualize the translation of our loss summation into matrix form, let's expand out the loss with $n = 3$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{x}, \\vec{y})\n",
    "&=\n",
    "(y_1 - \\begin{bmatrix} 1 & x_1 \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix})^2  \\\\\n",
    "&+\n",
    "(y_2 - \\begin{bmatrix} 1 & x_2 \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix})^2 \\\\\n",
    "&+\n",
    "(y_3 - \\begin{bmatrix} 1 & x_3 \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix})^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our loss function is a sum of squares and the *L2*-norm for a vector is the square root of a sum of squares: \n",
    "\n",
    "$$\\Vert \\vec{v} \\Vert = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}$$ \n",
    "\n",
    "If we let $y_i - \\begin{bmatrix} 1 & x_i \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix} = v_i$: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{x}, \\vec{y}) \n",
    "&= v_1^2 + v_2^2 + \\dots + v_n^2 \\\\\n",
    "&= \\Vert \\vec{v} \\Vert^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As before, our loss can be expressed as the *L2*-norm of some vector $v$, squared. With each component $v_i = y_i - \\begin{bmatrix} 1 & x_i \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix} \\quad \\forall i \\in [1,3]$:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{x}, \\vec{y})\n",
    "&= \\left \\Vert  \\qquad   \n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "&= \\left \\Vert  \\qquad  \n",
    "\\vec{y} \n",
    "\\quad - \\quad \n",
    "X \n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "&= \\left \\Vert  \\qquad  \n",
    "\\vec{y} \n",
    "\\quad - \\quad \n",
    "f_\\hat{\\theta}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication $\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1}\n",
    "\\end{bmatrix}$ is a linear combination of the columns of $X$: each $\\hat{\\theta_i}$ only ever multiplies with one column of $X$—this perspective shows us that $f_\\hat{\\theta}$ is a linear combination of the features of our data. In summary, simple linear regression fits $f_\\hat{\\theta}$ to $\\vec{1}$ and $\\vec{x}$.\n",
    "\n",
    "$X$ and $\\vec{y}$ are fixed, but $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$ can take on any value, so $f_\\hat{\\theta}$ can take on any of the infinite linear combinations of the columns of $X$. To have the smallest loss, we want to choose $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$ such that the resulting vector is as close to $\\vec{y}$ as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Intuition\n",
    "\n",
    "Now, let's develop an intuition for why it matters that $\\hat{y}$ is restricted to the linear combinations of the columns of $X$. Although the span of any set of vectors includes an infinite number of linear combinations, infinite does not mean any—the linear combinations are restricted by the basis vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here is our loss function and scatter plot:\n",
    "\n",
    "$$L(\\hat{\\theta}, \\vec{x}, \\vec{y}) \\quad = \\quad \\left \\Vert  \\quad  \n",
    "\\vec{y} \n",
    "\\quad - \\quad \n",
    "X \\hat{\\theta}\n",
    "\\quad \\right \\Vert ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HIDDEN\n",
    "sns.regplot(x='x', y='y', data=data, ci=None, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection, we see that no line can perfectly fit our points, so we cannot achieve 0 loss. Thus, we know that $\\vec{y}$ is not in the plane spanned by $\\vec{x}$ and $\\vec{1}$, represented as a parallelogram below.\n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection1.png\" width=\"500\" />\n",
    "\n",
    "Since our error is based on distance, we can see that to minimize $ L(\\hat{\\theta}, \\vec{x}, \\vec{y}) = \\left \\Vert  \\vec{y} - X \\hat{\\theta} \\right \\Vert ^2$, we want $X \\hat{\\theta}$ to be as close to $\\vec{y}$ as possible.\n",
    "\n",
    "Mathematically, we are looking for the projection of $\\vec{y}$ onto the vector space spanned by the columns of $X$, as the projection of any vector is the closest point in $Span(X)$ to that vector. Thus, choosing $\\hat{\\theta}$ such that $\\hat{y} = X \\hat{\\theta} = $ proj$_{Span(X)} \\quad $ $y$ is the best solution.\n",
    "<img src=\"../../notebooks-images/linear_projection2.png\" width=\"500\" />\n",
    "\n",
    "To see why, consider other points on the vector space, in purple.\n",
    "<img src=\"../../notebooks-images/linear_projection3.png\" width=\"500\" />\n",
    "\n",
    "By the Pythagorean Theorem, any other point on the plane is farther from $\\vec{y}$ than $\\hat{y}$ is. The length of the perpendicular corresponding to $\\hat{y}$ represents the least squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "Since we've snuck in a lot of linear algebra concepts already, all that's left is solving for the $\\hat{\\theta}$ that yields our desired $\\hat{y}$.\n",
    "\n",
    "A couple things to note: \n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection5.png\" width=\"500\" />\n",
    "\n",
    "- $\\hat{y} + e = \\vec{y}$\n",
    "- $\\vec{e}$ is perpendicular to $\\vec{x}$ and $\\vec{1}$\n",
    "- $\\hat{y} = X \\hat{\\theta^*}$ is the vector closest to $y$ in the vector space spanned by $\\vec{x}$ and $\\vec{1}$\n",
    "\n",
    "Thus, we arrive at the equation:\n",
    "\n",
    "$$X  \\hat{\\theta^*} + \\vec{e} = \\vec{y}$$\n",
    "\n",
    "Left-multiplying both sides by $X^T$:\n",
    "\n",
    "$$X^T X  \\hat{\\theta^*} + X^T \\vec{e} = X^T \\vec{y}$$\n",
    "\n",
    "Since $\\vec{e}$ is perpendicular to the columns of $X$, $X^T \\vec{e}$ is a column vector of $0$'s. Thus, we arrive at the Normal Equation:\n",
    "\n",
    "$$X^T X  \\hat{\\theta^*} = X^T \\vec{y}$$\n",
    "\n",
    "From here, we can easily solve for $\\hat{\\theta^*}$ by left-multiplying both sides by $(X^T X)^{-1}$:\n",
    "\n",
    "$$\\hat{\\theta^*} = (X^T X)^{-1} X^T \\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing up the Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our case study, apply what we've learned, and explain why our solution is sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{y} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -2  \\end{bmatrix} \\qquad X = \\begin{bmatrix} 1 & 3 \\\\ 1 & 0 \\\\ 1 & -1 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\theta^*} \n",
    "&= \n",
    "\\left(\n",
    "\\begin{bmatrix} 1 & 1 & 1 \\\\ 3 & 0 & -1 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 & 3 \\\\ 1 & 0 \\\\ 1 & -1 \\end{bmatrix}\n",
    "\\right)^{-1}\n",
    "\\begin{bmatrix} 1 & 1 & 1 \\\\ 3 & 0 & -1 \\end{bmatrix}\n",
    "\\begin{bmatrix} 2 \\\\ 1 \\\\ -2  \\end{bmatrix} \\\\\n",
    "&= \n",
    "\\left(\n",
    "\\begin{bmatrix} 3 & 2\\\\ 2 & 10 \\end{bmatrix}\n",
    "\\right)^{-1}\n",
    "\\begin{bmatrix} 1 \\\\ 8 \\end{bmatrix} \\\\\n",
    "&=\n",
    "\\frac{1}{30-4}\n",
    "\\begin{bmatrix} 10 & -2\\\\ -2 & 3 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 8 \\end{bmatrix} \\\\\n",
    "&=\n",
    "\\frac{1}{26}\n",
    "\\begin{bmatrix} -6 \\\\ 22 \\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix} - \\frac{3}{13} \\\\ \\frac{11}{13} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have analytically found that best model for least squares regression is $f_\\hat{\\theta} (x_i) = - \\frac{3}{13} + \\frac{11}{13} x_i$. We know that our choice of $\\hat{\\theta}$ is sound by the mathematical property that the projection of $\\vec{y}$ onto the span of the columns of $X$ yields the closest point in the vector space to $\\vec{y}$. Under linear constraints using the least squares metric, solving for $\\hat{\\theta}$ by taking the projection guarantees us the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Variables are Linearly Dependent\n",
    "\n",
    "For every additional variable, we add one column to $X$. The span of the columns of $X$ is the linear combinations of the column vectors, so adding columns only changes the span if it is linearly independent from all existing columns.\n",
    "\n",
    "When the added column is linearly dependent, it can be expressed as a linear combination of some other columns, and thus will not introduce new any vectors to the subspace.\n",
    "\n",
    "Recall that the span of $X$ is important because it is the subspace we want to project $\\vec{y}$ onto. If the subspace does not change, then the projection will not change.\n",
    "\n",
    "For example, when we introduced $\\vec{x}$ to the constant model to get the simple linear model, we introduced a independent variable. $\\vec{x} = \\begin{bmatrix} 3 \\\\ 0 \\\\ -1 \\end{bmatrix}$ cannot be expressed as a scalar of $\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$. Thus, we moved from finding the projection of $\\vec{y}$ onto a line:\n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection__1dprojection.png\" width=\"250\" />\n",
    "\n",
    "to finding the projection of $\\vec{y}$ onto a plane:\n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection1.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "Now, lets introduce another variable, $\\vec{z}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | z |  y |\n",
    "| - |:-| - |\n",
    "| 3 | 4 | 2 |\n",
    "| 0 | 1 | 1 |\n",
    "| -1 | 0 | -2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since $\\vec{z} = \\vec{1} + \\vec{x}$, $\\vec{z}$ is linearly dependent on the existing columns and therefore does not change $Span(X)$. In fact, $\\vec{z}$ lies in the original $Span(X)$:\n",
    "\n",
    "<img src=\"../../notebooks-images/linear_projection__dependent_variablesz.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\vec{z} = \\vec{1} + \\vec{x}$. Since $\\vec{z}$ is a linear combination of $\\vec{1}$ and $\\vec{x}$, it lies in the original $Span(X)$. Furthermore, we can say that the original $Span(X)$ is the same as the new $Span(X)$. Thus, the projection of $\\vec{y}$ onto the subspace spanned by $\\vec{1}$, $\\vec{x}$, and $\\vec{z}$ would be the same as the projection of $\\vec{y}$ onto the subspace spanned by $\\vec{1}$ and $\\vec{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe this from minimizing the loss function:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L(\\hat{\\theta}, \\vec{d}, \\vec{y})\n",
    "&= \\left \\Vert  \\qquad   \n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} 1 & x_1 & z_1 \\\\ 1 & x_2 & z_2\\\\ 1 & x_3 & z_3\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1} \\\\\n",
    "     \\hat{\\theta_2}\n",
    "\\end{bmatrix}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "&= \\left \\Vert  \\qquad   \n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3  \\end{bmatrix} \\quad - \\quad \n",
    "\\begin{bmatrix} 1 & 3 & 4\\\\ 1 & 0 & 1\\\\ 1 & -1 & 0\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "     \\hat{\\theta_0} \\\\\n",
    "     \\hat{\\theta_1} \\\\\n",
    "     \\hat{\\theta_2}\n",
    "\\end{bmatrix}\n",
    "\\qquad \\right \\Vert ^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Our possible solutions follow the form $\\theta_0 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + \\theta_1 \\begin{bmatrix} 3 \\\\ 0 \\\\ -1 \\end{bmatrix} + \\theta_2 \\begin{bmatrix} 4 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n",
    "\n",
    "\n",
    "Since $\\vec{z} = \\vec{1} + \\vec{x}$, regardless of $\\theta_0$, $\\theta_1$, and $\\theta_2$, the possible values can be rewritten as:\n",
    "$$ (\\theta_0 + \\theta_2) \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} + (\\theta_1 + \\theta_2) \\begin{bmatrix} 3 \\\\ 0 \\\\ -1 \\end{bmatrix}$$\n",
    "\n",
    "So adding $\\vec{z}$ does not change the problem at all. The only difference is, we can express this projection in multiple ways. Recall that we found the projection of $\\vec{y}$ onto the plane spanned by $\\vec{1}$ and $\\vec{x}$ to be:\n",
    "\n",
    "$$ \\begin{bmatrix} \\vec{1} & \\vec{x} \\end{bmatrix}  \\begin{bmatrix} - \\frac{3}{13} \\\\ \\frac{11}{13} \\end{bmatrix} = - \\frac{3}{13} \\vec{1} + \\frac{11}{13} \\vec{x}$$\n",
    "\n",
    "However, with the introduction of $\\vec{z}$, we have more ways to express this same projection vector. \n",
    "\n",
    "Since $\\vec{1} = \\vec{z} - \\vec{x}$, $\\hat{y}$ can also be expressed as:\n",
    "\n",
    "$$ - \\frac{3}{13} (\\vec{z} - \\vec{x}) + \\frac{11}{13} \\vec{x} = - \\frac{3}{13} \\vec{z} + \\frac{14}{13} \\vec{x} $$\n",
    "\n",
    "Since $\\vec{x} = \\vec{z} + \\vec{1}$, $\\hat{y}$ can also be expressed as:\n",
    "\n",
    "$$ - \\frac{3}{13} \\vec{1} + \\frac{11}{13} (\\vec{z} + \\vec{1}) = \\frac{8}{13} \\vec{1} + \\frac{11}{13} \\vec{z} $$\n",
    "\n",
    "But all three expressions represent the same projection.\n",
    "\n",
    "In conclusion, adding a linearly dependent column to $X$ does not change $Span(X)$, and thus will not change the projection and solution to the least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Review\n",
    "- Definition of a vector\n",
    "- Scaling vectors\n",
    "- Adding vectors\n",
    "- Vector notations\n",
    "- The $\\vec{1}$ vector\n",
    "\n",
    "- Span of a set of vectors\n",
    "- Vector space\n",
    "- Vector subspace\n",
    "\n",
    "\n",
    "- Angles between vectors\n",
    "- Vector length\n",
    "- Distance between of vectors\n",
    "- Orthogonal vectors\n",
    "- Projections of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Definition\n",
    "A vector is defined by a length and a direction.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__vectors.png\" width=\"500\" />\n",
    "\n",
    "Notice that $\\vec{x}$ and $\\vec{y}$ have the same length and direction. They are equal vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling and Adding Vectors\n",
    "To scale a vector is to change it's length.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__scaling.png\" width=\"500\" />\n",
    "\n",
    "Notice that $\\vec{2x}$ and $\\vec{y}$ have the direction but different lengths. They are not equal.\n",
    "\n",
    "To add two vectors $\\vec{y} + \\vec{z}$, take one step according to the length of $\\vec{y}$, then immediately take one step according to the length of $\\vec{z}$ (or vice versa). This is also known as triangle method, where you place the initial point of a vector on the terminal point of the other.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__adding.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Notations\n",
    "Vectors are usually represented as Cartesian Coordinates.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__notation.png\" width=\"500\" />\n",
    "\n",
    "$$ \\vec{x} = \\begin{bmatrix} 1 \\\\ 4  \\end{bmatrix} , \\quad \n",
    "   \\vec{y} = \\begin{bmatrix} 3 \\\\ 2  \\end{bmatrix} , \\quad \n",
    "   \\vec{z} = \\begin{bmatrix} 4 \\\\ 0  \\end{bmatrix}$$\n",
    "   \n",
    "In this notation, arithmetic operations we saw earlier become quite easy.\n",
    "\n",
    "$$ \\vec{2x} = \\begin{bmatrix} 2 \\\\ 8  \\end{bmatrix} , \\quad\n",
    "   \\vec{-0.5z} = \\begin{bmatrix} -2 \\\\ 0  \\end{bmatrix} , \\quad\n",
    "   \\vec{2x + -0.5z} = \\begin{bmatrix} 0 \\\\ 8  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__notation_arithmetic.png\" width=\"500\" />\n",
    "\n",
    "\n",
    "Vectors can be added and scaled element-wise:\n",
    "\n",
    "$$a \\vec{x} + b \\vec{y} = \\begin{bmatrix} a \\ x_1 & + & b \\  y_1 \\\\\n",
    "                                            & \\vdots & \\\\\n",
    "                                          a \\  x_n & + & b \\  y_n\n",
    "                          \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The $\\vec{1}$ Vector\n",
    "In any $d$ dimensional space, $\\vec{1}$ is the vector of all $1$'s: \n",
    "$ \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1  \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span\n",
    "The span of a set of vectors $\\{\\vec{v_1}, \\vec{v_2}, \\dots, \\vec{v_p}\\}$ is the set of all possible linear combinations. For these $p$ vectors:\n",
    "\n",
    "$$ \\{ c_1 \\ \\vec{v_1} + c_2 \\ \\vec{v_2} + \\dots + c_p \\ \\vec{v_p} \\ : \\ \\forall c_i \\in F\\}$$\n",
    "\n",
    "where $F$ is the field of the vector space (out of scope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Spaces\n",
    "A vector space $V$ is the span of a set of vectors $\\{\\vec{v_1}, \\vec{v_2}, \\dots, \\vec{v_p}\\}$, where each $\\vec{v_i}$ is a $n \\times 1$ dimensional column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Subspaces\n",
    "A subspace $U$ of $V$ is the span of a set of vectors $\\{\\vec{u_1}, \\dots, \\vec{u_q}\\}$ where each $\\vec{u_i} \\in V$. This means every vector in $U$ is also in $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Angles\n",
    "When you put any two vectors terminal end to terminal end without changing their direction, you can measure the angle between them.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__angle.png\" width=\"300\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Length\n",
    "\n",
    "Intuition in $\\mathbb{R}^2$ :\n",
    "\n",
    "Recall the triangle method of adding two vectors. If we add two perpendicular vectors $\\vec{a} + \\vec{b}$ in $\\mathbb{R}^2$, then we know that the resulting vector will be the hypotenuse. In this case, we also know that the length of $\\vec{a} + \\vec{b}$ will follow the Pythagorean Theorem: $\\sqrt{a^2 + b^2}$.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__length.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "General Formula for  Length of $\\vec{v} \\in \\mathbb{R}^n$ :\n",
    "$$\\begin{aligned} || \\vec{v} || \\quad\n",
    "&=  \\quad \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}  \\\\\n",
    "&= \\quad \\sqrt{\\vec{v} \\cdot \\vec{v}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the final operator is the dot product.\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\vec{x} \\cdot \\vec{y} \\quad \n",
    "&= \\quad x_1 \\ y_1 + x_2 \\ y_2 + \\dots + x_n \\ y_n \\\\\n",
    "&= \\quad||x|| \\ ||y|| \\ \\cos{\\theta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first expression is known as the algebraic definition of the dot product, and the second is the geometric definition. Note that the dot product is the inner product defined for vectors in $\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Between Two Vectors\n",
    "$$dist(\\vec{x},\\vec{y}) \\quad = \\quad || \\vec{x} - \\vec{y} ||$$\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__distance.png\" width=\"500\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal Vectors\n",
    "For two non-zero vectors to be orthogonal, they must satisfy the property that $\\vec{x} \\cdot \\vec{y} = 0$. Since they have non-zero length, the only way for the two vectors to be orthogonal is if $\\cos{\\theta} = 0$. One satisfying $\\theta$ is 90 degrees, our familiar right angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projections\n",
    "To project one vector $\\vec{x}$ onto another vector $\\vec{y}$, we want to find $k \\ \\vec{y}$ that is closest to $\\vec{x}$.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__projection.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "By the Pythagorean Theorem, we know that $k$ must be the scalar such that $\\vec{x} - k \\ \\vec{y}$ is perpendicular to $\\vec{y}$, so  $k \\ \\vec{y}$ is the (orthogonal) projection of $\\vec{x}$ onto $\\vec{y}$.\n",
    "\n",
    "Likewise, to project one vector $\\vec{x}$ onto any vector space spanned by some set of vectors $\\{\\vec{v_1}, \\vec{v_2}, \\dots, \\vec{v_p}\\}$, we still find the linear combination $k_1 \\ \\vec{v_1} + k_2 \\ \\vec{v_2} + \\dots + k_p \\ \\vec{v_p}\\}$ that is closest to $\\vec{x}$.\n",
    "\n",
    "<img src=\"../../notebooks-images/vector_space_review__proj2d.png\" width=\"500\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
