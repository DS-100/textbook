{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.850913Z",
     "start_time": "2018-06-26T05:27:27.561608Z"
    }
   },
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import nbinteract as nbi\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.options.display.max_rows = 7\n",
    "pd.options.display.max_columns = 8\n",
    "pd.set_option('precision', 2)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Logistic Regression\n",
    "\n",
    "Previously, we covered batch gradient descent, an algorithm that iteratively updates $\\hat\\theta$ to find the loss-minimizing parameters. We also discussed stochastic gradient descent and mini-batch gradient descent, methods that take advantage of statistical theory and parallelized hardware to decrease the time spent training the gradient descent algorithm. In this section, we will apply these concepts to logistic regression and walk through examples using scikit-learn functions.\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "The general update formula for batch gradient descent is given by:\n",
    "\n",
    "$$\n",
    "\\hat\\theta_{t+1} = \\hat\\theta_t - \\alpha \\cdot \\nabla_\\hat\\theta L(\\hat\\theta, X, y)\n",
    "$$\n",
    "\n",
    "In logistic regression, we use the cross entropy loss as our loss function:\n",
    "\n",
    "$$\n",
    "L(\\hat\\theta, X, y) = \\frac{1}{n} \\sum_{i} \\left(-y_i \\ln \\left(f_{\\hat\\theta} \\left(X_i \\right) \\right) - \\left(1 - y_i \\right) \\ln \\left(1 - f_{\\hat\\theta} \\left(X_i \\right) \\right) \\right)\n",
    "$$\n",
    "\n",
    "$\\nabla_{\\hat\\theta} L(\\hat\\theta, X, y) = -\\frac{1}{n}\\sum_{i=1}^n(y_i - \\sigma_i)X_i $ is then the gradient of the cross entropy loss; plugging this in allows us to find the gradient descent algorithm specific to logistic regression. Letting $ \\sigma_i = f_\\hat\\theta(X_i) = \\sigma(X_i \\cdot \\hat \\theta) $, this becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\theta_{t+1} &= \\hat\\theta_t - \\alpha \\cdot \\left(- \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) X_i \\right) \\\\\n",
    "&= \\hat\\theta_t + \\alpha \\cdot \\left(\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) X_i \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\hat\\theta_t$ is the current estimate of $\\theta$ at iteration $t$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $-\\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) X_i$ is the gradient of the cross entropy loss\n",
    "- $\\hat\\theta_{t+1}$ is the next estimate of $\\theta$ computed by subtracting the product of $\\alpha$ and the cross entropy loss computed at $\\hat\\theta_t$\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "The general update formula is below, where $l(\\hat\\theta, X_i, y_i)$ is the loss function for a single data point:\n",
    "\n",
    "$$\n",
    "\\hat\\theta_{t+1} = \\hat\\theta_t - \\alpha \\nabla_\\hat\\theta l(\\hat\\theta, X_i, y_i)\n",
    "$$\n",
    "\n",
    "Returning back to our example in logistic regression, we approximate the gradient of the cross entropy loss across all data points using the gradient of the cross entropy loss of one data point. This is shown below, with $ \\sigma_i = f_\\hat{\\theta}(X_i) = \\sigma(X_i \\cdot \\hat \\theta) $.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\hat\\theta L(\\hat\\theta, X, y) &\\approx \\nabla_\\hat\\theta l(\\hat\\theta, X_i, y_i)\\\\\n",
    "&= -(y_i - \\sigma_i)X_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When we plug this approximation into the general formula for stochastic gradient descent, we find the stochastic gradient descent update formula for logistic regression.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\theta_{t+1} &= \\hat\\theta_t - \\alpha \\nabla_\\hat\\theta l(\\hat\\theta, X_i, y_i) \\\\\n",
    "&= \\hat\\theta_t + \\alpha \\cdot (y_i - \\sigma_i)X_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "\n",
    "Similarly, we can approximate the gradient of the cross entropy loss using a random sample of data points, also known as a mini-batch.\n",
    "\n",
    "$$\n",
    "\\nabla_\\hat\\theta L(\\hat\\theta, X, y) \\approx \\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}\\nabla_{\\hat\\theta}l(\\hat\\theta, X_i, y_i)\n",
    "$$\n",
    "\n",
    "We substitute this for the gradient of the cross entropy loss, yielding a mini-batch gradient descent update formula specific to logistic regression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat\\theta_{t+1} &= \\hat\\theta_t - \\alpha \\cdot -\\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}(y_i - \\sigma_i)X_i \\\\\n",
    "&= \\hat\\theta_t + \\alpha \\cdot \\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}(y_i - \\sigma_i)X_i\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Scikit-Learn\n",
    "\n",
    "Scikit-learn has implementations for stochastic gradient descent and mini-batch gradient descent using the [`SGDClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class. To gain a better estimate of the speed benefits afforded by stochastic gradient descent and mini-batch gradient descent, we will first manually implement batch gradient descent; then, we will compare the results with stochastic gradient descent and mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.877265Z",
     "start_time": "2018-06-26T05:27:28.853504Z"
    }
   },
   "outputs": [],
   "source": [
    "lebron = pd.read_csv('lebron.csv')\n",
    "\n",
    "columns = ['shot_distance', 'minute', 'action_type', 'shot_type', 'opponent']\n",
    "rows = lebron[columns].to_dict(orient='row')\n",
    "\n",
    "onehot = DictVectorizer(sparse=False).fit(rows)\n",
    "X = onehot.transform(rows)\n",
    "y = lebron['shot_made'].as_matrix()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.883408Z",
     "start_time": "2018-06-26T05:27:28.879695Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# iris = load_iris()\n",
    "# X = iris['data']\n",
    "# y = iris['target'] == 2\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=20, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.896911Z",
     "start_time": "2018-06-26T05:27:28.885989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 15.6 ms, total: 15.6 ms\n",
      "Wall time: 1.77 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Stochastic GD\n",
    "sgd_clf = SGDClassifier(loss='log', random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.912098Z",
     "start_time": "2018-06-26T05:27:28.900069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.920103Z",
     "start_time": "2018-06-26T05:27:28.914719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mini-batch GD\n",
    "\n",
    "def iter_minibatches(X, y, minibatch_size):\n",
    "    # Provide chunks one by one\n",
    "    shuffled_indices = np.random.permutation(len(y))\n",
    "    X = X[shuffled_indices]\n",
    "    y = y[shuffled_indices]\n",
    "    for i in range(0, len(y), minibatch_size):\n",
    "        X_b, y_b = X[i:i+minibatch_size], y[i:i+minibatch_size]\n",
    "        yield X_b, y_b\n",
    "        \n",
    "minibatch_generator = iter_minibatches(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.947996Z",
     "start_time": "2018-06-26T05:27:28.926317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 0 ns, total: 15.6 ms\n",
      "Wall time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mbgd_clf = SGDClassifier(loss='log', random_state=42)\n",
    "for X_b, y_b in minibatch_generator:\n",
    "    mbgd_clf.partial_fit(X_b, y_b, classes=np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-26T05:27:28.962402Z",
     "start_time": "2018-06-26T05:27:28.951484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbgd_clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
