{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import nbinteract as nbi\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.options.display.max_rows = 7\n",
    "pd.options.display.max_columns = 8\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Gradient Descent\n",
    "\n",
    "In chapter 11, we covered the gradient descent algorithm, which is used to converge at the $\\theta$ parameters that minimize a loss function $L(\\theta, y)$.\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\nabla_\\theta L(\\theta, y)\n",
    "$$\n",
    "\n",
    "We can replace  $\\nabla_\\theta L(\\theta, y)$ with the gradient of the cross entropy cost that we previously derived to find the gradient descent algorithm specific to logistic regression. Letting $ \\sigma_i = f_\\hat{\\theta}(X_i) = \\sigma(X_i \\cdot \\hat \\theta) $, the gradient descent algorithm becomes:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\cdot \\left(- \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\sigma_i\\right) X_i \\right)\n",
    "$$\n",
    "\n",
    "By definition, the gradient of the cross entropy cost is the average of the gradient of the loss over all $n$ observations. This is then computed at each iteration $t$ of the gradient descent algorithm. For large $n$, this can become a computationally expensive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "An alternative to gradient descent is **stochastic gradient descent**, in which we approximate the gradient by taking a sample of observations. The gradient of the loss function in stochastic gradient descent is below, with $ \\sigma_i = f_\\hat{\\theta}(X_i) = \\sigma(X_i \\cdot \\hat \\theta) $.\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta, y) \\approx -\\frac{1}{|\\mathcal{B}|} \\sum_{i\\in\\mathcal{B}}(y_i - \\sigma_i)X_i\n",
    "$$\n",
    "\n",
    "$\\mathcal{B}$ is a batch of data points that we randomly sample from the $n$ observations. At each iteration of stochastic gradient descent, we will make an estimate of the true gradient of the loss function using a new randomly sampled batch. Because $\\mathcal{B}$ is a simple random sample, the expectation of the gradient of the loss function over the batch is equal to the true gradient over all $n$ observations. In other words, on average, we expect that stochastic gradient descent will converge to the $\\theta$ parameters that minimize the loss function.\n",
    "\n",
    "## Selecting the batch size\n",
    "\n",
    "Stochastic gradient descent generally refers to an algorithm with a batch size of 1, and mini-batch gradient descent is often used to describe algorithms with a randomly sampled batch of size smaller than $n$. In practice, stochastic gradient descent is used as an umbrella term that encompasses both concepts.\n",
    "\n",
    "Batch sizes are generally set to small numbers such as 1 or 2 to allow us to minimize computational cost. If we have access to a parallel machine (such as a GPU or a distributed computing system), we can increase the batch size to take advantage of the hardware. By running the algorithm in parallel, we could compute the gradient over a larger batch size in the same amount of time that the machine would take for a batch size of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Stochastic Gradient Descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
