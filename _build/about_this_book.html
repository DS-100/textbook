---
redirect_from:
  - "/about-this-book"
interact_link: content/about_this_book.md
kernel_name: 
has_widgets: false
title: |-
  About This Book
prev_page:
  url: /index.html
  title: |-
    Introduction
next_page:
  url: /ch/01/lifecycle_intro.html
  title: |-
    The Data Science Lifecycle
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About-This-Book">About This Book<a class="anchor-link" href="#About-This-Book"> </a></h2><p>In this book, we will proceed as though the reader is comfortable with the
knowledge presented in <a href="http://data8.org/">Data 8</a> or some equivalent. In particular, we
will assume that the reader is familiar with the following topics (links to
pages from the Data 8 textbook are given in parentheses).</p>
<ul>
<li>Tabular data manipulation: selection, filtering, grouping, joining <a href="https://www.inferentialthinking.com/chapters/08/2/classifying-by-one-variable.html">(link)</a></li>
<li>Basic probability concepts <a href="https://www.inferentialthinking.com/chapters/09/5/finding-probabilities.html">(link)</a></li>
<li>Sampling, empirical distributions of statistics <a href="https://www.inferentialthinking.com/chapters/10/3/empirical-distribution-of-a-statistic.html">(link)</a></li>
<li>Hypothesis testing using bootstrap resampling <a href="https://www.inferentialthinking.com/chapters/13/4/using-confidence-intervals.html">(link)</a></li>
<li>Least squares regression and regression inference <a href="https://www.inferentialthinking.com/chapters/16/2/inference-for-the-true-slope.html">(link)</a></li>
<li>Classification <a href="https://www.inferentialthinking.com/chapters/17/1/nearest-neighbors.html">(link)</a></li>
</ul>
<p>In addition, we assume that the reader has taken a course in computer
programming in Python, such as <a href="https://cs61a.org/">CS61A</a> or some equivalent. We will not
explain Python syntax except in special cases.</p>
<p>Finally, we assume that the reader has basic familiarity with partial
derivatives, gradients, vector algebra, and matrix algebra.</p>
<h3 id="Notation">Notation<a class="anchor-link" href="#Notation"> </a></h3><p>This book covers topics from multiple disciplines. Unfortunately, some of these
disciplines use the same notation to describe different concepts. In order to
prevent headaches, we have devised notation that may differ slightly
from the notation used in your discipline.</p>
<p>A population parameter is denoted by $ \theta^* $. The model parameter that
minimizes a specified loss function is denoted by $ \hat{\theta} $. Typically,
we desire $ \hat{\theta} \approx \theta^* $. We use the plain variable
$ \theta $ to denote a model parameter that does not minimize a particular loss
function. For example, we may arbitrarily set $ \theta = 16$ in order to
calculate a model's loss at that choice of $ \theta $. When using gradient
descent to minimize a loss function, we use $ \theta^{(t)} $ to represent the
intermediate values of $ \theta $.</p>
<p>We will always use bold lowercase letters for vectors. For example, we
represent a vector of population parameters using
$ \boldsymbol{\theta^\*} = [ \theta^\*_1, \theta^\*_2, \ldots, \theta^\*_n ] $
and a vector of fitted model parameters as
$ \boldsymbol{\hat{\theta}} = [\hat{\theta_1}, \hat{\theta_2}, \ldots, \hat{\theta_n} ] $.</p>
<p>We will always use bold uppercase letters for matrices. For example, we
commonly represent a data matrix using $ \boldsymbol X $.</p>
<p>We will always use non-bolded uppercase letters for random variables, such as
$ X $ or $ Y $.</p>
<p>When discussing the bootstrap, we use $ \theta^* $ to denote the population
parameter, $ \hat{\theta} $ to denote the sample test statistic, and
$ \tilde{\theta} $ to denote a bootstrapped test statistic.</p>

</div>
</div>
</div>
</div>

 

