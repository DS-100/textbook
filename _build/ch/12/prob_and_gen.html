---
redirect_from:
  - "/ch/12/prob-and-gen"
interact_link: content/ch/12/prob_and_gen.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Probability and Generalization
prev_page:
  url: /ch/11/gradient_stochastic.html
  title: |-
    Stochastic Gradient Descent
next_page:
  url: /ch/12/prob_random_vars.html
  title: |-
    Random Variables
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Probability-and-Generalization">Probability and Generalization<a class="anchor-link" href="#Probability-and-Generalization"> </a></h1><p>We have introduced a sequence of steps to create a model using a dataset:</p>
<ol>
<li>Select a model.</li>
<li>Select a loss function.</li>
<li>Fit the model by minimizing the loss on the dataset.</li>
</ol>
<p>Thus far, we have introduced the constant model (1), a set of loss functions (2), and gradient descent as a general method of minimizing the loss (3). Following these steps will often generate a model that makes accurate predictions on the dataset it was trained on.</p>
<p>Unfortunately, a model that only performs well on its training data has little real-world utility. We care about the model's ability to <strong>generalize</strong>. Our model should make accurate predictions about the population, not just the training data. This problem seems challenging to answerâ€”how might we reason about data we haven't seen yet?</p>
<p>Here we turn to the inferential power of statistics. We first introduce some mathematical tools: random variables, expectation, and variance. Using these tools, we draw conclusions about our model's long-term performance on data from our population, even data that we did not use to train the model!</p>

</div>
</div>
</div>
</div>

 

